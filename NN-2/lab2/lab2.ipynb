{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 2. Модели представления текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, os, string, re, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\artem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\artem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODING_DEFAULT = 'utf-8'\n",
    "TRAIN_FILE = 'train.json'\n",
    "TEST_FILE = 'test.json'\n",
    "PRECISION = 4 # the number of signs after the comma\n",
    "\n",
    "ID = 'id'\n",
    "TEXT = 'text'\n",
    "SENTIMENT = 'sentiment'\n",
    "SENTIMENTS = ['negative', 'positive', 'neutral']\n",
    "POSITIVE = 0\n",
    "NEGATIVE = 1\n",
    "NEUTRAL = 2\n",
    "\n",
    "ORIGIN_WORD = 'OriginWord'\n",
    "NORMAL_FORM = 'NormalForm'\n",
    "POS = 'POS'\n",
    "CASE = 'Case'\n",
    "GENDER = 'Gender'\n",
    "DICT_EXIST = 'FromDict'\n",
    "WORDS = \"Words\"\n",
    "FREQ = \"Freq\"\n",
    "UNIQ = \"uniquies\"\n",
    "COUNT = \"count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = {\n",
    "    \"NOUN\":\"имя существительное\",\n",
    "    \"ADJF\": \"имя прилагательное (полное)\",\n",
    "    \"NOUN\":\"имя существительное\",\n",
    "    \"ADJF\":\"имя прилагательное (полное)\",\n",
    "    \"ADJS\":\"имя прилагательное (краткое)\",\n",
    "    \"COMP\":\"компаратив\",\n",
    "    \"VERB\":\"глагол (личная форма)\",\n",
    "    \"INFN\":\"глагол (инфинитив)\",\n",
    "    \"PRTF\":\"причастие (полное)\",\n",
    "    \"PRTS\":\"причастие (краткое)\",\n",
    "    \"GRND\":\"деепричастие\",\n",
    "    \"NUMR\":\"числительное\",\n",
    "    \"ADVB\":\"наречие\",\n",
    "    \"NPRO\":\"местоимение-существительное\",\n",
    "    \"PRED\":\"предикатив\",\n",
    "    \"PREP\":\"предлог\",\n",
    "    \"CONJ\":\"союз\",\n",
    "    \"PRCL\":\"частица\",\n",
    "    \"INTJ\":\"междометие\",\n",
    "    \"nomn\":\"именительный\",\n",
    "    \"gent\":\"родительный\",\n",
    "    \"datv\":\"дательный\",\n",
    "    \"accs\":\"винительный\",\n",
    "    \"ablt\":\"творительный\",\n",
    "    \"loct\":\"предложный\",\n",
    "    \"voct\":\"звательный\",\n",
    "    \"gen2\":\"второй родительный (частичный)\",\n",
    "    \"acc2\":\"второй винительный\",\n",
    "    \"loc2\":\"второй предложный (местный)\",\n",
    "    \"masc\":\"мужской род\",\n",
    "    \"femn\":\"женский род\",\n",
    "    \"neut\":\"средний род\",\n",
    "    \"LATN\":\"Токен состоит из латинских букв (например, “foo-bar” или “Maßstab”)\",\n",
    "    \"PNCT\":\"Пунктуация (например, , или !? или …)\",\n",
    "    \"NUMB\":\"Число (например, “204” или “3.14”)\",\n",
    "    \"intg\":\"целое число (например, “204”)\",\n",
    "    \"real\":\"вещественное число (например, “3.14”)\",\n",
    "    \"ROMN\":\"Римское число (например, XI)\",\n",
    "    \"UNKN\":\"Токен не удалось разобрать\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Подготовка текстового корпуса\n",
    "- Прочитайте описание задачи Sentiment analysis: https://www.kaggle.com/c/sentiment-analysis-in-russian.\n",
    "- Скачайте файлы `train.json` и `test.json`: https://www.kaggle.com/c/sentiment-analysis-in-russian/data.\n",
    "- Загрузите в ноутбук скачанные файлы, выведите информацию по ним."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    start = time.perf_counter()\n",
    "    with open(filename, 'r', encoding=ENCODING_DEFAULT) as file:\n",
    "        data = json.load(file)\n",
    "    print(\"[INFO]: read time: %0.4f sec\" % (time.perf_counter() - start))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_file_description(data, train=False):\n",
    "    '''размер файла, количество текстов, средняя/макс./мин. длина текста, распределение тональности для обучающей выборки'''\n",
    "    text_lens = []\n",
    "    if train:\n",
    "        sentiments = dict.fromkeys(SENTIMENTS, 0)\n",
    "    for news in data:\n",
    "        if train:\n",
    "            sentiments[news[SENTIMENT]] += 1\n",
    "        text_lens.append(len(news[TEXT]))\n",
    "    print(\"Размер(байты):\", os.path.getsize(TRAIN_FILE if train else TEST_FILE))\n",
    "    print(\"Количество примеров:\", len(data))\n",
    "    print(\"Средняя длина текста:\", np.mean(text_lens))\n",
    "    print(\"Максимальная длина текста:\", np.max(text_lens))\n",
    "    print(\"Минимальная длина текста:\", np.min(text_lens))\n",
    "    if train:\n",
    "        print(\"Распределение тональностей:\")\n",
    "        print(\"\\tПозитивных:\", sentiments[SENTIMENTS[POSITIVE]])\n",
    "        print(\"\\tНегативных:\", sentiments[SENTIMENTS[NEGATIVE]])\n",
    "        print(\"\\tНейтральных:\", sentiments[SENTIMENTS[NEUTRAL]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: read time: 0.4423 sec\n",
      "Размер(байты): 59298269\n",
      "Количество примеров: 8263\n",
      "Средняя длина текста: 3911.85017548106\n",
      "Максимальная длина текста: 381498\n",
      "Минимальная длина текста: 28\n",
      "Распределение тональностей:\n",
      "\tПозитивных: 1434\n",
      "\tНегативных: 2795\n",
      "\tНейтральных: 4034\n"
     ]
    }
   ],
   "source": [
    "train_json = read_json(TRAIN_FILE)\n",
    "print_file_description(train_json, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: read time: 0.1541 sec\n",
      "Размер(байты): 15417058\n",
      "Количество примеров: 2056\n",
      "Средняя длина текста: 4102.4143968871595\n",
      "Максимальная длина текста: 320754\n",
      "Минимальная длина текста: 35\n"
     ]
    }
   ],
   "source": [
    "test_json = read_json(TEST_FILE)\n",
    "print_file_description(test_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Булевская модель\n",
    "Напишите функцию `get_bool_model()` на входе которой текстовый корпус, на выходе – булевская матрица термин-документ.\n",
    "\n",
    "В функции должны осуществляться следующие действия:\n",
    "- все слова преобразуются в нормальную форму;\n",
    "- удаляются стоп-слова;\n",
    "- удаляются (или оставляются) слова заданных частей речи (список частей речи должен передаваться в виде параметра);\n",
    "- удаляются слова, частотность которых во всем корпусе ниже заданного порога (параметр);\n",
    "- создается словарь корпуса.\n",
    "\n",
    "Нельзя использовать библиотечные функции `scikit-learn`.  \n",
    "Можно использовать функции, разработанные в этой и предыдущей лабораторных работах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "nlp = word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        return super().default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "stopwords_dict = Counter(stopwords.words('russian')) # {\"sw1\" : 1, \"sw2\" : 1, ..., \"swN\" : 1}\n",
    "re_numbers_pattern = re.compile(r\"\\d+([,\\.]\\d+){0,}\") # https://regex101.com/r/1V0HDV/1\n",
    "re_spaces_pattern = re.compile(r\"\\s{2,}\") # two or more spaces\n",
    "re_newline_pattern = re.compile(r\"\\n\") # newline\n",
    "\n",
    "def save_data(data, filename='tmp'):\n",
    "    with open(f\"{filename}.json\", \"w+\", encoding='utf-8') as f:\n",
    "        tojs = json.dumps(data, indent=4, ensure_ascii=False, cls=SetEncoder)\n",
    "        f.write(tojs)\n",
    "\n",
    "def get_data(t):\n",
    "    return 'None' if t is None else t\n",
    "\n",
    "def check_word(word):\n",
    "    # token filter\n",
    "    if word not in string.punctuation and \\\n",
    "        len(re.findall(re_numbers_pattern, word)) == 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def text_prepare(text):\n",
    "    # replace '\\n' on ' ', '<spaces>' on '<space>'\n",
    "    text = re.sub(re_newline_pattern, ' ', text)\n",
    "    text = re.sub(re_spaces_pattern, ' ', text)\n",
    "    return text\n",
    "\n",
    "def parse_text(text):\n",
    "    ret = []\n",
    "    tokens = nlp(text_prepare(text))\n",
    "    for word in tokens:\n",
    "        if check_word(word):\n",
    "            parsed = morph.parse(word)[0]\n",
    "            data = {\n",
    "                ORIGIN_WORD : word,\n",
    "                NORMAL_FORM : parsed.normal_form,\n",
    "                POS         : parsed.tag.POS if parsed.tag.POS is not None else 'NOUN',\n",
    "                CASE        : parsed.tag.case,\n",
    "                GENDER      : parsed.tag.gender,\n",
    "                DICT_EXIST  : parsed.is_known,\n",
    "            }\n",
    "            ret.append(data)              \n",
    "    return ret\n",
    "\n",
    "def get_dictionary(parsed):\n",
    "    tmp = {}\n",
    "    for value in parsed:\n",
    "        if tmp.get(value[NORMAL_FORM]) is None:\n",
    "            tmp.update({\n",
    "            value[NORMAL_FORM] : { \n",
    "                POS   : value[POS], \n",
    "                WORDS : set(([value[ORIGIN_WORD]])),\n",
    "                FREQ : 1,\n",
    "            }})\n",
    "        else:\n",
    "            tmp[value[NORMAL_FORM]][WORDS].add(value[ORIGIN_WORD])\n",
    "            tmp[value[NORMAL_FORM]][FREQ] += 1  \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaa ff v ffg sdfsf fff fsfsfsf fsd g 123 fsdf'"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'aaa ff v    ffg\\nsdfsf fff \\n   fsfsfsf fsd g 123 fsdf'\n",
    "text_prepare(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearing(data, freq_rm, pos_rm):\n",
    "    tmp = {}\n",
    "    print(\"[INFO]: lenght of corpus dictionary befor clearing:\", len(data))\n",
    "    for key, val in data.items():\n",
    "        if not (val[POS] in pos_rm or val[FREQ] < freq_rm or key in stopwords_dict):\n",
    "            tmp.update({key : val})\n",
    "    print(\"[INFO]: lenght of corpus dictionary after clearing:\", len(tmp))\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def get_bool_model(corpus, freq_remove=0, pos_remove=[]):\n",
    "    parsed_texts = [] # to store the parsed texts\n",
    "    documents_dicts = [] # list of dicts of texts\n",
    "\n",
    "    # loop by corpus elements\n",
    "    for text in corpus:\n",
    "        parsed = parse_text(text[TEXT]) # text morph analysis\n",
    "        parsed_texts.extend(parsed) # adding to list of all parsed texts\n",
    "        documents_dicts.append(get_dictionary(parsed)) # forming dictionary of text and adding to list\n",
    "    corpus_dict = get_dictionary(parsed_texts)\n",
    "    save_data(corpus_dict) #todo delete\n",
    "\n",
    "    # clearing the text from stopwords, low-frequency and exclude some pos\n",
    "    corpus_dict_cleared = clearing(corpus_dict, freq_remove, pos_remove)\n",
    "\n",
    "    # forming term matrix\n",
    "    term_matrix = []\n",
    "    for word in corpus_dict_cleared:\n",
    "        word_exist = []\n",
    "        for doc_dict in documents_dicts:\n",
    "            word_exist.append(True if word in doc_dict else False)\n",
    "        term_matrix.append(word_exist)\n",
    "        \n",
    "    return term_matrix, corpus_dict_cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: lenght of corpus dictionary befor clearing: 662\n",
      "[INFO]: lenght of corpus dictionary after clearing: 294\n",
      "\n",
      "Матрица термин-документ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>досудебный</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>начать</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>национальный</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>сообщить</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>финансовый</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>формировать</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>сохраниться</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>модельный</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>зависеть</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>признать</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>294 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0      1      2\n",
       "досудебный     True   True  False\n",
       "начать         True   True  False\n",
       "национальный   True  False  False\n",
       "сообщить       True   True  False\n",
       "финансовый     True  False  False\n",
       "...             ...    ...    ...\n",
       "формировать   False  False   True\n",
       "сохраниться   False  False   True\n",
       "модельный     False  False   True\n",
       "зависеть      False  False   True\n",
       "признать      False  False   True\n",
       "\n",
       "[294 rows x 3 columns]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_model, dict_cleared = get_bool_model(train_json[:3], freq_remove=1, pos_remove=['NOUN'])\n",
    "df = pd.DataFrame(bool_model, index=dict_cleared.keys())\n",
    "df.to_csv(\"bool_model.csv\")\n",
    "print(\"\\nМатрица термин-документ\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3. Модель TF-IDF\n",
    "Напишите функцию `get_tfidf_model()` на входе которой текстовый корпус, на выходе – матрица термин-документ c TF-IDF-весами.\n",
    "\n",
    "В функции должны осуществляться следующие действия:\n",
    "- все слова преобразуются в нормальную форму;\n",
    "- удаляются стоп-слова;\n",
    "- удаляются (или оставляются) слова заданных частей речи (список частей речи должен передаваться в виде параметра);\n",
    "- удаляются слова, частотность которых во всем корпусе ниже заданного порога (параметр);\n",
    "- создается словарь корпуса;\n",
    "- вычисляются глобальные IDF-веса и сохраняются в словарь;\n",
    "- слова для документов взвешиваются в соответствии со схемой TF-IDF.\n",
    "\n",
    "Нельзя использовать библиотечные функции `scikit-learn`.  \n",
    "Можно использовать функции, разработанные в предыдущей лабораторной работе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_model():\n",
    "    # Ваш код здесь\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте матрицу термин-документ для текстовых корпусов из первого задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните результаты работы вашей функции (полученные веса) с результатами работы класса [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) из `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4. Модель word2vec\n",
    "Напишите функцию `get_word2vec_model()` на входе которой текстовый корпус и модель `word2vec`, на выходе – матрица термин-документ c word2vec-весами.\n",
    "\n",
    "В функции должны осуществляться следующие действия:\n",
    "- все слова преобразуются в нормальную форму (при необходимости – в зависимости от используемой модели);\n",
    "- удаляются стоп-слова;\n",
    "- удаляются (или оставляются) слова заданных частей речи (список частей речи должен передаваться в виде параметра);\n",
    "- удаляются слова, частотность которых во всем корпусе ниже заданного порога (параметр);\n",
    "- создается словарь корпуса;\n",
    "- вычисляются веса word2vec для заданной модели.\n",
    "\n",
    "\n",
    "Можно использовать библиотечные функции и функции, разработанные в лабораторных работах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_model():\n",
    "    # Ваш код здесь\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте матрицу термин-документ для текстовых корпусов из первого задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
