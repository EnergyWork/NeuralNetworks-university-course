{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции потерь для задач классификации в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной функцией потерь для задач классификации является *перекрестная энтропия* (cross-entropy):  \n",
    "\n",
    "$$H(p,q)=-\\sum_{y}p(y)\\log q(y),$$\n",
    "где $p$ – истинное вероятностное распределение, $q$ – вероятностное распределение, предсказанное моделью.  \n",
    "\n",
    "В PyTorch существует несколько вариантов [функций потерь](https://pytorch.org/docs/stable/nn.html#loss-functions), связанных с перекрестной энтропией, основными из которых являются следующие:\n",
    "- [BCELoss](#Binary-Cross-Entropy-(BCELoss)) – перекрестная энтропия для бинарной классификации;\n",
    "- [CrossEntropyLoss](#Cross-Entropy-(CrossEntropyLoss)) – перекрестная энтропия для многоклассовой классификации;\n",
    "- [NLLLoss](#Negative-Log-Likelihood-(NLLLoss)) – отрицательное логарифмическое правдоподобие (negative log likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "precision = 8 # Точность после запятой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy (BCELoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Бинарная перекрестная энтропия* (Binary Cross-Entropy):  \n",
    "\n",
    "$$L = -\\frac{1}{l}\\sum_{i=1}^{l}y_i\\log(p(y_i))+(1-y_i)\\log(1-p(y_i)),$$\n",
    "\n",
    "где $y_i$ – истинный ответ, $y_i\\in\\{0,1\\}$;  \n",
    "$p(y_i)$ – предсказанная вероятность ответа $y_i=1$.  \n",
    "\n",
    "Применяется для задач бинарной классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в NumPy\n",
    "Обратите внимание на размерности массивов данных – это одномерные массивы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Два класса: [0, 1]\n",
    "# Три примера\n",
    "# Числа обозначают вероятности отнесения примера к классу 1\n",
    "y_pred = np.array([0.1, 0.9, 0.2])\n",
    "y_true = np.array([0.0, 1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy\n",
    "def BCE(y_pred, y_true):\n",
    "    total_bce_loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    num_of_samples = y_pred.shape[0]\n",
    "    mean_bce_loss = total_bce_loss / num_of_samples\n",
    "    return mean_bce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss_np = BCE(y_pred, y_true)\n",
    "print(f'BCE loss (NumPy) = {bce_loss_np:.{precision}f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tensor = torch.from_numpy(y_pred)\n",
    "y_true_tensor = torch.from_numpy(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss_fn = nn.BCELoss()\n",
    "bce_loss_torch = bce_loss_fn(y_pred_tensor, y_true_tensor)\n",
    "print(f'BCE loss (PyTorch) = {bce_loss_torch:.{precision}f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy (CrossEntropyLoss)\n",
    "Перекрестная энтропия в случае многоклассовой классификации (Categorical Cross-Entropy, Multiclass Cross-Entropy):  \n",
    "\n",
    "$$L = -\\frac{1}{l}\\sum_{i=1}^{l}y_{i}\\log(p(y_{i}))$$\n",
    "\n",
    "В PyTorch в функции `CrossEntropyLoss()` предсказанные значения считаются ненормализованными, поэтому перед вычислением перекрестной энтропии к ним применяется функция`Softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в NumPy\n",
    "На входе двумерные массивы: количество строк совпадает с количеством примеров, количество столбцов – с количеством классов.  \n",
    "В каждой строке находятся вероятности принадлежности данного примера к классу, номер которого соответствует индексу столбца (от 0 до K–1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Три класса: [0, 1, 2]\n",
    "# Четыре примера\n",
    "y_pred = np.array([[0.8, 0.1, 0.1], [0.15, 0.8, 0.05], [0.05, 0.05, 0.9], [0.85, 0.1, 0.05]])\n",
    "y_true = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем Softmax, как в PyTorch\n",
    "y_pred_softmax = scipy.special.softmax(y_pred, axis=1)\n",
    "y_pred_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropy(y_pred, y_true):\n",
    "    total_loss = -np.sum(y_true * np.log(y_pred))\n",
    "    num_of_samples = y_pred.shape[0]\n",
    "    mean_loss = total_loss / num_of_samples\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss_np = CrossEntropy(y_pred_softmax, y_true)\n",
    "print(f'Cross Entropy loss (NumPy) = {cross_entropy_loss_np:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Бинарная классификация в NumPy (аналог BCELoss)\n",
    "Для примера покажем вычисление бинарной перекрестной энтропии при помощи функции `CrossEntropy()`.  \n",
    "Возьмем пример, который использовался выше в бинарной классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Два класса: [0, 1]\n",
    "# Три примера\n",
    "# Числа обозначают вероятности отнесения примера к классу 1\n",
    "# y_pred = np.array([0.1, 0.9, 0.2])\n",
    "# y_true = np.array([0.0, 1.0, 0.0])\n",
    "\n",
    "# Преобразуем к двумерным массивам\n",
    "y_pred = np.array([[0.9, 0.1], [0.1, 0.9], [0.8, 0.2]])\n",
    "y_true = np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss_np = CrossEntropy(y_pred, y_true)\n",
    "print(f'Cross Entropy loss (NumPy) = {cross_entropy_loss_np:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в PyTorch\n",
    "Вычисление многоклассовой перекрестной энтропии в PyTorch реализуется при помощи функции `CrossEntropyLoss()`.  \n",
    "На вход функции поступает двумерный массив предсказанных вероятностей (строки – примеры, столбцы – классы) и одномерный массив истинных классов: значение в этом массиве обозначает класс (от 0 до K–1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Три класса: [0, 1, 2]\n",
    "# Четыре примера\n",
    "y_pred = np.array([[0.8, 0.1, 0.1], [0.15, 0.8, 0.05], [0.05, 0.05, 0.9], [0.85, 0.1, 0.05]])\n",
    "y_true = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]])\n",
    "\n",
    "# Преобразуем двумерный массив y_true в одномерный\n",
    "y_true = np.array([0, 1, 2, 0]).astype(np.int64) # номера классов из диапазона [0, K-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tensor = torch.from_numpy(y_pred)\n",
    "y_true_tensor = torch.from_numpy(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss_fn = nn.CrossEntropyLoss()\n",
    "cross_entropy_loss_torch = cross_entropy_loss_fn(y_pred_tensor, y_true_tensor)\n",
    "print(f'Cross Entropy loss (PyTorch) = {cross_entropy_loss_torch:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Бинарная классификация в PyTorch (аналог BCELoss)\n",
    "Также для примера продемонстрируем вычисление бинарной перекрестной энтропии при помощи функции PyTorch `CrossEntropyLoss()`.\n",
    "Возьмем пример, который использовался выше в бинарной классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Два класса: [0, 1]\n",
    "# Три примера\n",
    "# Числа обозначают вероятности отнесения примера к классу 1\n",
    "# y_pred = np.array([0.1, 0.9, 0.2])\n",
    "# y_true = np.array([0.0, 1.0, 0.0])\n",
    "\n",
    "# Преобразуем к двумерным массивам\n",
    "y_pred = np.array([[0.9, 0.1], [0.1, 0.9], [0.8, 0.2]])\n",
    "y_true = np.array([0, 1, 0]).astype(np.int64) # номера классов из диапазона [0, K-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tensor = torch.from_numpy(y_pred)\n",
    "y_true_tensor = torch.from_numpy(y_true) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Логарифмируем, чтобы компенсировать Softmax, встроенный в CrossEntropyLoss()\n",
    "y_pred_log = torch.log(y_pred_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss_fn = nn.CrossEntropyLoss()\n",
    "cross_entropy_loss_torch = cross_entropy_loss_fn(y_pred_log, y_true_tensor)\n",
    "print(f'Cross Entropy loss (PyTorch) = {cross_entropy_loss_torch:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Log Likelihood (NLLLoss)\n",
    "*Отрицательное логарифмическое правдоподобие* (Negative Log Likelihood) – это другое название перекрестной энтропии.  \n",
    "В PyTorch отрицательное логарифмическое правдоподобие реализуется при помощи функции `NLLLoss()`, но предполагается, что предсказанные ненормализованные значения $x_i$ преобразуются с помощью функции `LogSoftmax()`:\n",
    "\n",
    "$$LogSoftmax(x_i)=\\log\\left(\\frac{e^{x_i}}{\\sum_{j}x_j}\\right)$$\n",
    "\n",
    "Поэтому в PyTorch перед использованием `NLLLoss()` ненормализованные выходы следует преобразовать с использованием функции `LogSoftmax()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Три класса: [0, 1, 2]\n",
    "# Четыре примера\n",
    "y_pred = np.array([[0.8, 0.1, 0.1], [0.15, 0.8, 0.05], [0.05, 0.05, 0.9], [0.85, 0.1, 0.05]])\n",
    "y_true = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем Softmax\n",
    "y_pred_softmax = scipy.special.softmax(y_pred, axis=1)\n",
    "y_pred_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем логаримф - получается LogSoftmax\n",
    "y_pred_log_softmax = np.log(y_pred_softmax)\n",
    "y_pred_log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLLLoss(y_pred, y_true):\n",
    "    # То же, что CrossEntropy, но без логарифма перед y_pred\n",
    "    total_loss = -np.sum(y_true * y_pred)\n",
    "    num_of_samples = y_pred.shape[0]\n",
    "    mean_loss = total_loss / num_of_samples\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_loss_np = NLLLoss(y_pred_log_softmax, y_true)\n",
    "print(f'Negative Log Likelihood loss (NumPy) = {nll_loss_np:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Три класса: [0, 1, 2]\n",
    "# Четыре примера\n",
    "y_pred = np.array([[0.8, 0.1, 0.1], [0.15, 0.8, 0.05], [0.05, 0.05, 0.9], [0.85, 0.1, 0.05]])\n",
    "y_true = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]])\n",
    "\n",
    "# Преобразуем двумерный массив y_true в одномерный\n",
    "y_true = np.array([0, 1, 2, 0]).astype(np.int64) # номера классов из диапазона [0, K-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tensor = torch.from_numpy(y_pred)\n",
    "y_true_tensor = torch.from_numpy(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.LogSoftmax(dim=1)\n",
    "y_pred_tensor = softmax(y_pred_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_loss_fn = nn.NLLLoss()\n",
    "nll_loss_torch = nll_loss_fn(y_pred_tensor, y_true_tensor)\n",
    "print(f'Negative Log Likelihood loss (PyTorch) = {nll_loss_torch:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Резюме\n",
    "Обозначим `y` – ненормированный выход последнего слоя нейронной сети (например, выход модуля `Linear`).\n",
    "1. Бинарная классификация:\n",
    "    - `y` 🡒 `Sigmoid` 🡒 `BCELoss`\n",
    "1. Многоклассовая классификация:\n",
    "    - `y` 🡒 `LogSoftmax` 🡒 `NLLLoss`\n",
    "    - `y` 🡒 `CrossEntropyLoss` (аналог: `y` 🡒 `LogSoftmax` 🡒 `NLLLoss`)\n",
    "\n",
    "Если требуется на выходе сети явно иметь результаты `Softmax`, можно использовать следующую схему:  \n",
    "- `y` 🡒 `Softmax` 🡒 `Log` 🡒 `NLLLoss`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
