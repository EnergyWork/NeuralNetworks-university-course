{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 1. Введение в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Содержание\n",
    "1. [Установка](#Установка)\n",
    "1. [Тензоры](#Тензоры)\n",
    "1. [Автоматическое дифференцирование](#Автоматическое-дифференцирование)\n",
    "1. [Линейная регрессия в PyTorch](#Линейная-регрессия-в-PyTorch)\n",
    "1. [Линейная классификация в PyTorch](#Линейная-классификация-в-PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка\n",
    "Чтобы установить PyTorch, нужно на [официальном сайте](https://pytorch.org/get-started/locally/) сгенерировать команду для установки в зависимости от версии операционной системы и других параметров.  \n",
    "Рекомендуется также установить [CUDA](https://developer.nvidia.com/cuda-zone) (Compute Unified Device Architecture) – вычислительную платформу от Nvidia для поддержки вычислений на видеокартах и [CuDNN](https://developer.nvidia.com/cudnn) (CUDA Deep Neural Network) – библиотеку на основе CUDA для поддержки глубоких нейронных сетей.  \n",
    "Проверить правильность установки и узнать версии используемого ПО можно следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  8 20:54:56 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 466.27       Driver Version: 466.27       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   59C    P0    43W / 230W |   1214MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1044    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      2192    C+G   ...zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A      2720    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      4648    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A      6624    C+G   ...r\\Application\\browser.exe    N/A      |\n",
      "|    0   N/A  N/A      7832    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      8792    C+G   ...ub.ThreadedWaitDialog.exe    N/A      |\n",
      "|    0   N/A  N/A      9016    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     11096    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11384    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12580    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     14600    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     15060    C+G   ...8wekyb3d8bbwe\\XboxApp.exe    N/A      |\n",
      "|    0   N/A  N/A     15800    C+G   ...Microsoft.Msn.Weather.exe    N/A      |\n",
      "|    0   N/A  N/A     17540    C+G   ...ty\\Common7\\IDE\\devenv.exe    N/A      |\n",
      "|    0   N/A  N/A     18480    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     18520    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     18712    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     21132    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     21312    C+G   ...t\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     21396    C+G   ...t\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     22988    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     24888    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     28048    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Утилита от Nvidia для вывода информации о видеокарте\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python VERSION: 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "\n",
      "pyTorch VERSION: 1.8.1+cu111\n",
      "\n",
      "CUDA VERSION:\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Sun_Mar_21_19:24:09_Pacific_Daylight_Time_2021\n",
      "Cuda compilation tools, release 11.3, V11.3.58\n",
      "Build cuda_11.3.r11.3/compiler.29745058_0\n",
      "\n",
      "CUDNN VERSION: 8005\n",
      "\n",
      "Number CUDA Devices: 1\n",
      "Current cuda device: 0\n",
      "Cuda device name:  NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "print('Python VERSION:', sys.version)\n",
    "print('\\npyTorch VERSION:', torch.__version__)\n",
    "print('\\nCUDA VERSION:')\n",
    "! nvcc --version\n",
    "print('\\nCUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('\\nNumber CUDA Devices:', torch.cuda.device_count())\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Cuda device name: ', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Проверить доступность CUDA можно также следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тензоры\n",
    "*Тензор* – многомерный массив данных в библиотеках глубокого обучения. Тензор может быть 0-мерным (скаляр), одномерным (вектор), двумерным (матрица) или размерностью больше двух.  \n",
    "Тензоры используются для представления входов, выходов и параметров моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Инициализация тензоров\n",
    "Тензоры могут быть инициализированы несколькими способами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. На основе данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Из массивов NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Заполненные случайным образом или константными значениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.8666, 0.7311, 0.6503],\n",
      "        [0.8814, 0.8344, 0.3880]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f'Random Tensor: \\n {rand_tensor} \\n')\n",
    "print(f'Ones Tensor: \\n {ones_tensor} \\n')\n",
    "print(f'Zeros Tensor: \\n {zeros_tensor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Атрибуты тензоров\n",
    "Тензоры имеют несколько атрибутов, в том числе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on the following device: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "print(f'Shape of tensor: {tensor.shape}')\n",
    "print(f'Datatype of tensor: {tensor.dtype}')\n",
    "print(f'Device tensor is stored on the following device: {tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Операции над тензорами\n",
    "По умолчанию тензоры создаются на CPU.  \n",
    "Чтобы переместить их на GPU нужно вызвать метод `to`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor is stored on: cpu\n",
      "Tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f'Tensor is stored on: {tensor.device}')\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "print(f'Tensor is stored on: {tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Индексирование и срезы (slicing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial tensor:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "\n",
      "First row: tensor([1, 2, 3, 4])\n",
      "\n",
      "First column: tensor([1, 5, 9])\n",
      "\n",
      "Last column: tensor([ 4,  8, 12])\n",
      "\n",
      "tensor([[ 1,  0,  3,  4],\n",
      "        [ 5,  0,  7,  8],\n",
      "        [ 9,  0, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "print(f'Initial tensor:\\n{tensor}\\n')\n",
    "print(f'First row: {tensor[0]}\\n')\n",
    "print(f'First column: {tensor[:, 0]}\\n')\n",
    "print(f'Last column: {tensor[..., -1]}\\n')\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Конкатенация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  0,  3,  4,  1,  0,  3,  4,  1,  0,  3,  4],\n",
      "        [ 5,  0,  7,  8,  5,  0,  7,  8,  5,  0,  7,  8],\n",
      "        [ 9,  0, 11, 12,  9,  0, 11, 12,  9,  0, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "concat_tensor = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(concat_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Арифметические операции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1 + tensor2 = \n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "\n",
      "tensor1 @ tensor2 = \n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "\n",
      "torch.matmul(tensor1, tensor2) = \n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "\n",
      "tensor1 * tensor2 = \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape = (3, 3)\n",
    "tensor1 = torch.ones(shape)\n",
    "tensor2 = torch.ones(shape)\n",
    "tensor_sum = tensor1 + tensor2\n",
    "print(f'tensor1 + tensor2 = \\n{tensor_sum}\\n')\n",
    "tensor_mul1 = tensor1 @ tensor2\n",
    "print(f'tensor1 @ tensor2 = \\n{tensor_mul1}\\n')\n",
    "tensor_mul2 = torch.matmul(tensor1, tensor2)\n",
    "print(f'torch.matmul(tensor1, tensor2) = \\n{tensor_mul2}\\n')\n",
    "tensor_mul3 = tensor1 * tensor2\n",
    "print(f'tensor1 * tensor2 = \\n{tensor_mul3}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Тензор может состоять из одного элемента (скаляр). В этом случае для доступа к значению тензора можно воспользоваться методом `item`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summa = 60, type(summa): <class 'torch.Tensor'>\n",
      "\n",
      "summa.item() = 60, type(summa.item()): <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "summa = tensor.sum()\n",
    "print(f'summa = {summa}, type(summa): {type(summa)}\\n')\n",
    "print(f'summa.item() = {summa.item()}, type(summa.item()): {type(summa.item())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Операции `in-place` – это операции, результат которых сохраняется в самом операнде. Обозначаются суфиксом `_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  0,  3,  4],\n",
      "        [ 5,  0,  7,  8],\n",
      "        [ 9,  0, 11, 12]])\n",
      "\n",
      "tensor([[ 6,  5,  8,  9],\n",
      "        [10,  5, 12, 13],\n",
      "        [14,  5, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "print(f'{tensor}\\n')\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Автоматическое дифференцирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для настройки весов в нейронных сетях используется *алгоритм обратного распространения ошибки* (back propagation). В этом алгоритме веса изменяются в зависимости от градиента функции ошибки.  \n",
    "В PyTorch модуль `torch.autograd` отвечает за автоматическое дифференцирование на вычислительном графе.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим сигмоидальную функцию и её производную:\n",
    "$$\\sigma(u)=\\frac{1}{1+e^{-u}}$$\n",
    "  \n",
    "$$\\sigma'(u)=\\frac{e^{-u}}{(1+e^{-u})^2}=\\frac{1}{(1+e^{-u})}\\frac{e^{-u}}{(1+e^{-u})}=\\frac{1}{(1+e^{-u})}\\frac{(1+e^{-u})-1}{(1+e^{-u})}=\\sigma(u)(1-\\sigma(u))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим эти функции в PyTorch и нарисуем их графики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(u):\n",
    "    return 1 / (1 + torch.exp(-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(u):\n",
    "    return sigmoid(u) * (1 - sigmoid(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = torch.tensor(np.linspace(-5, 5, 100))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['bottom'].set_position(('data', 0))\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, sigmoid_deriv(x), label='sigmoid_deriv')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим тензор, для которого мы хотели бы найти градиент (параметр `requires_grad=True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor(0.0, requires_grad=True)\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим тензор, которому присвоим значение функции "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sigmoid(u)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В атрибуте `grad_fn` хранится ссылка на функцию, которая была использована для вычисления тензора.  \n",
    "В случае задания тензора пользователем – `grad_fn = None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u.grad_fn: None\n",
      "\n",
      "s.grad_fn: <MulBackward0 object at 0x000002BE89311AC0>\n",
      "\n",
      "(u+u).grad_fn: <AddBackward0 object at 0x000002BE89311610>\n",
      "\n",
      "(u-u).grad_fn: <SubBackward0 object at 0x000002BED196E6A0>\n",
      "\n",
      "(u*u).grad_fn: <MulBackward0 object at 0x000002BED196E0D0>\n",
      "\n",
      "(u/u).grad_fn: <DivBackward0 object at 0x000002BED196E730>\n",
      "\n",
      "(1/u).grad_fn: <MulBackward0 object at 0x000002BED196E0D0>\n",
      "\n",
      "mean(u).grad_fn: <MeanBackward0 object at 0x000002BED196EB80>\n"
     ]
    }
   ],
   "source": [
    "print(f'u.grad_fn: {u.grad_fn}\\n')\n",
    "print(f's.grad_fn: {s.grad_fn}\\n')\n",
    "a = u + u\n",
    "print(f'(u+u).grad_fn: {a.grad_fn}\\n')\n",
    "a = u - u\n",
    "print(f'(u-u).grad_fn: {a.grad_fn}\\n')\n",
    "a = u * u\n",
    "print(f'(u*u).grad_fn: {a.grad_fn}\\n')\n",
    "a = u / u\n",
    "print(f'(u/u).grad_fn: {a.grad_fn}\\n')\n",
    "a = 1 / u\n",
    "print(f'(1/u).grad_fn: {a.grad_fn}\\n')\n",
    "a = torch.mean(u)\n",
    "print(f'mean(u).grad_fn: {a.grad_fn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Для вычисления градиента необходимо вызвать метод `backward`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение градиента после вызова `backward` хранится в атрибуте `grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2500)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что значение градиента (производной) совпадает со значением, вычисленным аналитически:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2500, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_deriv(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что повторный вызов метода `backward` невозможен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Раскомментируйте эту строку\n",
    "#s.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы повторно вызывать метод `backward`, следует указать при его вызове параметр `retain_graph=True`. Но имейте в виду, что в этом случае значение градиента будет накапливаться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad = 12.0\n",
      "a.grad = 24.0\n",
      "a.grad = 36.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = a * a * a\n",
    "b.backward(retain_graph=True)\n",
    "print(f'a.grad = {a.grad}')\n",
    "b.backward(retain_graph=True)\n",
    "print(f'a.grad = {a.grad}')\n",
    "b.backward()\n",
    "print(f'a.grad = {a.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При необходимости градиенты можно обнулить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.grad.data.zero_()\n",
    "u.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Проверьте значения производной сигмоидальной функции, вычисленные в разных точках. Сравните их со значениями на графике функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10499356687068939\n",
      "0.10499362647533417\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(2.0, requires_grad=True) # создаем тензор\n",
    "f = sigmoid(t) # задаем тензору функцию\n",
    "f.backward(retain_graph=True) # вычисляем градиент\n",
    "print(f'{t.grad}')\n",
    "print(f'{sigmoid_deriv(t)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Отметим, что в PyTorch, конечно, имеется [своя реализация](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) сигмоидальной функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Посчитайте значения градиента при помощи сигмоидальной функции из PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10499358177185059\n",
      "0.10499358177185059\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(-2.0, requires_grad=True) # создаем тензор\n",
    "sig = torch.nn.Sigmoid()\n",
    "f = sig(t)\n",
    "f.backward(retain_graph=True) # вычисляем градиент\n",
    "print(f'{t.grad}')\n",
    "print(f'{sigmoid_deriv(t)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Замечание*. Мы можем найти градиенты только для листьев в графе вычислений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Приостановка отслеживания градиентов\n",
    "По умолчанию, PyTorch отслеживает историю вычислений и поддерживает вычисление градиентов для тензоров, у которых установлено `requires_grad=True`.  \n",
    "Однако иногда бывает необходимо приостановить отслеживание, например, когда сеть уже обучена и требуется выполнять только прямой проход или когда в процессе обучения сети нужно зафиксировать веса определенных слоев. В этом случае можно воспользоваться блоком `no_grad` или методом `detach()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s.requires_grad = True\n",
      "s.requires_grad = False\n"
     ]
    }
   ],
   "source": [
    "s = sigmoid(u)\n",
    "print(f's.requires_grad = {s.requires_grad}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    s = sigmoid(u)\n",
    "print(f's.requires_grad = {s.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s.requires_grad = True\n",
      "s.requires_grad = False\n"
     ]
    }
   ],
   "source": [
    "s = sigmoid(u)\n",
    "print(f's.requires_grad = {s.requires_grad}')\n",
    "\n",
    "s = s.detach()\n",
    "print(f's.requires_grad = {s.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная регрессия в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание модели\n",
    "Построим однослойную сеть, состоящую из одного нейрона, которая аналогична обычной линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пространство имен `nn` в PyTorch предоставляет все необходимые блоки для построения нейронных сетей.  \n",
    "Каждый модуль наследует классу [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). Нейронная сеть представляет собой также модуль, который содержит другие модули (слои)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим устройство, на котором будем обучать сеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using \"cuda\" device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using \"{}\" device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем класс нашей нейронной сети (который должен являться наследником `nn.Module`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.regr = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.regr(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `__init__` является конструктором класса.  \n",
    "В первой строке конструктора вызывается конструктор базового класса `nn.Module`.  \n",
    "Во второй строке создается единственный слой, содержащий один нейрон, с помощью модуля [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).  \n",
    "Данный модуль осуществляет линейное преобразование входа $\\vec{x}$ в выход $\\vec{y}$ с учетом матрицы весов $W$:  \n",
    "\n",
    "$$\\vec{y}=\\vec{x}W^T+\\vec{b}.$$\n",
    "\n",
    "Первый аргумент для `Linear` – `in_features` – количество входов $x$, второй `out_features` – количество выходов $y$ (фактически, количество нейронов в данном слое). Существует также третий аргумент – `bias`, определяющий наличие вектора свободных коэффициентов $\\vec{b}$ (по умолчанию `bias=True`).  \n",
    "Матрица весов $W$ будет иметь размерность (`out_features`, `in_features`), вектор свободных коэффициентов $\\vec{b}$ – (`out_features`).  \n",
    "Значения $W$ и $\\vec{b}$ инициализируются случайным образом из равномерного распределения $U(-\\sqrt{k},\\sqrt{k})$, где $k=\\frac{1}{in\\_features}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что атрибут `requires_grad` для тензоров-весов сети автоматически устанавливается в `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `forward` определяет поток данных через нашу сеть (*прямой проход*). Аргумент `x` – это входные данные для сети. В нашем примере к `x` применяется линейное преобразование, результат которого является выходом сети (линейная регрессия)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Инициализируем генератор случайных чисел PyTorch для воспроизводимости результатов (для одинаковых начальных значений весов):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2bebeed1e70>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем экземпляр класса `NeuralNetwork` и выводим структуру сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (regr): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Перемещаем сеть на GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (regr): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Вывод значений весов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(model):\n",
    "    for parameter in model.named_parameters():\n",
    "        print(f'{parameter[0]} = {parameter[1].data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regr.weight = tensor([[0.7645]], device='cuda:0')\n",
      "regr.bias = tensor([0.8300], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем генератор случайных чисел `numpy` для воспроизводимости результатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируем случайные данные:  \n",
    "- $X$ – из равномерного распределения от 1 до 10,\n",
    "- $y=2x+1$ с учетом нормально распределенного шума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "data = np.random.uniform(1, 10, (n_samples, 1)).astype(np.float32)\n",
    "targets = 2 * data + 1 + np.random.normal(0, 2, (n_samples, 1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data, targets)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем тензоры PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.from_numpy(data)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перемещаем тензоры на GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device)\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем предсказание модели со случайно инициализированными весами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model(data)\n",
    "plt.scatter(data.cpu().numpy(), targets.cpu().numpy())\n",
    "plt.plot(data.cpu().detach().numpy(), predictions.cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение сети\n",
    "Сначала задаем функцию потерь ([MSE](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем определим оптимизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.01\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим функцию, определяющую действия в процессе одной эпохи обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data, targets, model, loss_fn, optimizer, log=True):\n",
    "    for i in range(len(data)):\n",
    "        # Вычисляем предсказание модели на одном объекте\n",
    "        pred = model(data[i])\n",
    "        # Вычисляем функцию потерь\n",
    "        loss = loss_fn(pred, targets[i])\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if log:\n",
    "            if i % 10 == 0:\n",
    "                print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем цикл обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regr.weight = tensor([[-0.2191]], device='cuda:0')\n",
      "regr.bias = tensor([0.2018], device='cuda:0')\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "loss: 113.8851089477539\n",
      "loss: 12.973309516906738\n",
      "loss: 21.2176513671875\n",
      "loss: 40.88903045654297\n",
      "loss: 0.3920753300189972\n",
      "loss: 2.107555389404297\n",
      "loss: 1.1625468730926514\n",
      "loss: 6.798628807067871\n",
      "loss: 14.56350040435791\n",
      "loss: 3.177443265914917\n",
      "\n",
      "regr.weight = tensor([[2.0937]], device='cuda:0')\n",
      "regr.bias = tensor([0.5875], device='cuda:0')\n",
      "-------------------------------\n",
      "Epoch 2\n",
      "loss: 0.03137291967868805\n",
      "loss: 5.0901689529418945\n",
      "loss: 1.0823955535888672\n",
      "loss: 18.802562713623047\n",
      "loss: 0.0942116230726242\n",
      "loss: 4.609859943389893\n",
      "loss: 1.4583176374435425\n",
      "loss: 7.273194313049316\n",
      "loss: 14.274089813232422\n",
      "loss: 3.0636165142059326\n",
      "\n",
      "regr.weight = tensor([[2.0889]], device='cuda:0')\n",
      "regr.bias = tensor([0.6262], device='cuda:0')\n",
      "-------------------------------\n",
      "Epoch 3\n",
      "loss: 0.025440897792577744\n",
      "loss: 4.945288181304932\n",
      "loss: 1.0816454887390137\n",
      "loss: 18.823997497558594\n",
      "loss: 0.07899833470582962\n",
      "loss: 4.517879009246826\n",
      "loss: 1.490458369255066\n",
      "loss: 7.243123531341553\n",
      "loss: 14.360464096069336\n",
      "loss: 2.974385976791382\n",
      "\n",
      "regr.weight = tensor([[2.0830]], device='cuda:0')\n",
      "regr.bias = tensor([0.6632], device='cuda:0')\n",
      "-------------------------------\n",
      "Epoch 4\n",
      "loss: 0.02191239967942238\n",
      "loss: 4.8111348152160645\n",
      "loss: 1.0847820043563843\n",
      "loss: 18.85391616821289\n",
      "loss: 0.06572854518890381\n",
      "loss: 4.428871154785156\n",
      "loss: 1.5215046405792236\n",
      "loss: 7.214067459106445\n",
      "loss: 14.44382095336914\n",
      "loss: 2.889960527420044\n",
      "\n",
      "regr.weight = tensor([[2.0774]], device='cuda:0')\n",
      "regr.bias = tensor([0.6987], device='cuda:0')\n",
      "-------------------------------\n",
      "Epoch 5\n",
      "loss: 0.018772682175040245\n",
      "loss: 4.684057235717773\n",
      "loss: 1.0878016948699951\n",
      "loss: 18.882658004760742\n",
      "loss: 0.05413414165377617\n",
      "loss: 4.344215393066406\n",
      "loss: 1.5516138076782227\n",
      "loss: 7.186215400695801\n",
      "loss: 14.524090766906738\n",
      "loss: 2.810037851333618\n",
      "\n",
      "regr.weight = tensor([[2.0720]], device='cuda:0')\n",
      "regr.bias = tensor([0.7329], device='cuda:0')\n",
      "-------------------------------\n",
      "Epoch 6\n",
      "loss: 0.015986602753400803\n",
      "loss: 4.563640594482422\n",
      "loss: 1.0907140970230103\n",
      "loss: 18.910308837890625\n",
      "loss: 0.04405936971306801\n",
      "loss: 4.263699531555176\n",
      "loss: 1.5808042287826538\n",
      "loss: 7.159519672393799\n",
      "loss: 14.601375579833984\n",
      "loss: 2.7343480587005615\n",
      "\n",
      "regr.weight = tensor([[2.0669]], device='cuda:0')\n",
      "regr.bias = tensor([0.7656], device='cuda:0')\n",
      "-------------------------------\n",
      "Epoch 7\n",
      "loss: 0.013521389104425907\n",
      "loss: 4.449488162994385\n",
      "loss: 1.0934985876083374\n",
      "loss: 18.93683433532715\n",
      "loss: 0.03536057099699974\n",
      "loss: 4.1871209144592285\n",
      "loss: 1.6090857982635498\n",
      "loss: 7.133932590484619\n",
      "loss: 14.675752639770508\n",
      "loss: 2.66264271736145\n",
      "\n",
      "regr.weight = tensor([[2.0619]], device='cuda:0')\n",
      "regr.bias = tensor([0.7971], device='cuda:0')\n",
      "-------------------------------\n",
      "Epoch 8\n",
      "loss: 0.011348478496074677\n",
      "loss: 4.34124231338501\n",
      "loss: 1.0961889028549194\n",
      "loss: 18.96234130859375\n",
      "loss: 0.02790757641196251\n",
      "loss: 4.114222526550293\n",
      "loss: 1.6364809274673462\n",
      "loss: 7.109408855438232\n",
      "loss: 14.747359275817871\n",
      "loss: 2.5946898460388184\n",
      "\n",
      "regr.weight = tensor([[2.0571]], device='cuda:0')\n",
      "regr.bias = tensor([0.8273], device='cuda:0')\n",
      "-------------------------------\n",
      "Epoch 9\n",
      "loss: 0.009441058151423931\n",
      "loss: 4.238558292388916\n",
      "loss: 1.0987704992294312\n",
      "loss: 18.98685073852539\n",
      "loss: 0.02158036269247532\n",
      "loss: 4.044833183288574\n",
      "loss: 1.6630027294158936\n",
      "loss: 7.085902214050293\n",
      "loss: 14.816290855407715\n",
      "loss: 2.530261754989624\n",
      "\n",
      "regr.weight = tensor([[2.0525]], device='cuda:0')\n",
      "regr.bias = tensor([0.8563], device='cuda:0')\n",
      "-------------------------------\n",
      "Epoch 10\n",
      "loss: 0.007774462457746267\n",
      "loss: 4.141113758087158\n",
      "loss: 1.1012510061264038\n",
      "loss: 19.010412216186523\n",
      "loss: 0.016269277781248093\n",
      "loss: 3.9787652492523193\n",
      "loss: 1.688669204711914\n",
      "loss: 7.063356876373291\n",
      "loss: 14.8826322555542\n",
      "loss: 2.4691710472106934\n",
      "\n",
      "regr.weight = tensor([[2.0481]], device='cuda:0')\n",
      "regr.bias = tensor([0.8842], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "print_weights(model)\n",
    "for t in range(epochs):\n",
    "    print(f'-------------------------------\\nEpoch {t+1}')\n",
    "    train_loop(data, targets, model, loss_fn, optimizer)\n",
    "    print()\n",
    "    print_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Нарисуем получившуюся модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2be9bbc4490>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model(data)\n",
    "plt.scatter(data.cpu().numpy(), targets.cpu().numpy())\n",
    "plt.plot(data.cpu().detach().numpy(), predictions.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Вычислите MSE для итоговой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4057605266571045\n"
     ]
    }
   ],
   "source": [
    "lossfn = nn.MSELoss(reduction='mean')\n",
    "l = lossfn(predictions, targets)\n",
    "#l.backward(retain_graph=True)\n",
    "print(f'{l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Выполните перебор гиперпараметров – скорости обучения и количества эпох (не менее пяти значений на каждый гиперпараметр) и найдите комбинацию, при которой достигается минимум MSE на обучающих данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.005, 'epoches': 15}\n",
      "tensor(3.2452, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.15]\n",
    "epochs = [1, 5, 10, 15, 20, 25]\n",
    "best = {'learning_rate' : learning_rates[0], 'epoches' : epochs[0]}\n",
    "b_loss = 100000\n",
    "for le in learning_rates:\n",
    "    for ep in epochs:\n",
    "        model = NeuralNetwork()\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=le)\n",
    "        for t in range(ep):\n",
    "            train_loop(data, targets, model, loss_fn, optimizer, log=False)\n",
    "        cur_loss = lossfn(model(data), targets)\n",
    "        if cur_loss <= b_loss:\n",
    "            b_loss = cur_loss\n",
    "            best['learning_rate'] = le\n",
    "            best['epoches'] = ep\n",
    "\n",
    "print(best)\n",
    "print(b_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная классификация в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация набора данных\n",
    "Сгенерируем набор данных для классификации: объекты с меткой \"0\" будут нормально распределены вокруг точки (-1, -1),  объекты с меткой \"1\" – вокруг точки (1, 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "half = int(n_samples / 2)\n",
    "     \n",
    "x_class0 = np.random.normal(size=[half, 2]) + [-1, -1]\n",
    "y_class0 = np.zeros((half, 1))\n",
    "\n",
    "x_class1 = np.random.normal(size=[half, 2]) + [1, 1]\n",
    "y_class1 = np.ones((half, 1))\n",
    "\n",
    "data = np.vstack([x_class0, x_class1])\n",
    "targets = np.vstack([y_class0, y_class1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(np.float32)\n",
    "targets = targets.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2be873c6370>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_class0[:, 0], x_class0[:, 1], c='red')\n",
    "plt.scatter(x_class1[:, 0], x_class1[:, 1], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.from_numpy(data)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9870,  0.4535],\n",
       "        [-1.2647,  1.7202],\n",
       "        [-0.3743, -1.8572],\n",
       "        [-2.0709, -0.5175],\n",
       "        [-1.2235, -0.2860]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device)\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание.\n",
    "Постройте нейронную сеть – линейный классификатор и обучите её распознавать приведенный набор данных.  \n",
    "*Подсказка*: в качестве функции потерь можно использовать Binary Cross Entropy ([BCELOSS](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork2, self).__init__()\n",
    "        self.regr = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.regr(x)\n",
    "        return torch.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork2(\n",
       "  (regr): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork2()\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1684, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "for t in range(ep):\n",
    "    train_loop(data, targets, model, loss_fn, optimizer, log=False)\n",
    "cur_loss = loss_fn(model(data), targets)\n",
    "print(cur_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8893279433250427\n",
      "loss: 0.8467200994491577\n",
      "loss: 0.7301824688911438\n",
      "loss: 1.171910047531128\n",
      "loss: 0.29076576232910156\n",
      "loss: 1.099567174911499\n",
      "loss: 1.0550345182418823\n",
      "loss: 1.134468674659729\n",
      "loss: 0.7383397817611694\n",
      "loss: 0.9158022999763489\n",
      "loss: 0.6466873288154602\n",
      "loss: 0.5625187754631042\n",
      "loss: 0.6055960059165955\n",
      "loss: 0.3642590641975403\n",
      "loss: 1.1437526941299438\n",
      "loss: 0.3526914417743683\n",
      "loss: 0.29772669076919556\n",
      "loss: 0.29771777987480164\n",
      "loss: 0.5219368934631348\n",
      "loss: 0.3860265612602234\n",
      "loss: 0.6330639719963074\n",
      "loss: 0.514829695224762\n",
      "loss: 0.5505269765853882\n",
      "loss: 0.3199041485786438\n",
      "loss: 1.097702980041504\n",
      "loss: 0.33836206793785095\n",
      "loss: 0.2706756591796875\n",
      "loss: 0.27857765555381775\n",
      "loss: 0.5128700137138367\n",
      "loss: 0.36974436044692993\n",
      "loss: 0.6204198002815247\n",
      "loss: 0.4729051887989044\n",
      "loss: 0.502264142036438\n",
      "loss: 0.28256314992904663\n",
      "loss: 1.0549696683883667\n",
      "loss: 0.32551056146621704\n",
      "loss: 0.2473682463169098\n",
      "loss: 0.2617124915122986\n",
      "loss: 0.5045807957649231\n",
      "loss: 0.35509514808654785\n",
      "loss: 0.6086483001708984\n",
      "loss: 0.4359239637851715\n",
      "loss: 0.4598429203033447\n",
      "loss: 0.2509673237800598\n",
      "loss: 1.0152568817138672\n",
      "loss: 0.3139304518699646\n",
      "loss: 0.22716853022575378\n",
      "loss: 0.24677127599716187\n",
      "loss: 0.4969792664051056\n",
      "loss: 0.34185805916786194\n",
      "loss: 0.5976550579071045\n",
      "loss: 0.40318503975868225\n",
      "loss: 0.422435462474823\n",
      "loss: 0.22408819198608398\n",
      "loss: 0.9782932996749878\n",
      "loss: 0.30344849824905396\n",
      "loss: 0.20956124365329742\n",
      "loss: 0.2334655374288559\n",
      "loss: 0.4899877905845642\n",
      "loss: 0.3298468589782715\n",
      "loss: 1.034904956817627\n",
      "loss: 1.7210276126861572\n",
      "loss: 1.7653101682662964\n",
      "loss: 2.183708906173706\n",
      "loss: 1.047224760055542\n",
      "loss: 1.009437084197998\n",
      "loss: 1.441921591758728\n",
      "loss: 1.195972204208374\n",
      "loss: 0.7374351024627686\n",
      "loss: 0.9537131786346436\n",
      "loss: 0.9977394342422485\n",
      "loss: 1.5387535095214844\n",
      "loss: 1.5681617259979248\n",
      "loss: 1.9035522937774658\n",
      "loss: 0.9862657785415649\n",
      "loss: 0.929305911064148\n",
      "loss: 1.2571107149124146\n",
      "loss: 1.0691566467285156\n",
      "loss: 0.7079574465751648\n",
      "loss: 0.8775519728660583\n",
      "loss: 0.9626888632774353\n",
      "loss: 1.373443603515625\n",
      "loss: 1.3899043798446655\n",
      "loss: 1.6497516632080078\n",
      "loss: 0.930363655090332\n",
      "loss: 0.8575573563575745\n",
      "loss: 1.0945663452148438\n",
      "loss: 0.9570580720901489\n",
      "loss: 0.6812615394592285\n",
      "loss: 0.8096872568130493\n",
      "loss: 0.9298315644264221\n",
      "loss: 1.2252486944198608\n",
      "loss: 1.2307220697402954\n",
      "loss: 1.4235154390335083\n",
      "loss: 0.8793869614601135\n",
      "loss: 0.7937454581260681\n",
      "loss: 0.9534991383552551\n",
      "loss: 0.8588570356369019\n",
      "loss: 0.657196581363678\n",
      "loss: 0.7495611906051636\n",
      "loss: 0.8992051482200623\n",
      "loss: 1.0937705039978027\n",
      "loss: 1.0901551246643066\n",
      "loss: 1.2249631881713867\n",
      "loss: 0.8331143260002136\n",
      "loss: 0.737281858921051\n",
      "loss: 0.8324061036109924\n",
      "loss: 0.7734229564666748\n",
      "loss: 0.6355750560760498\n",
      "loss: 0.6965084671974182\n",
      "loss: 0.8707994222640991\n",
      "loss: 0.9781259298324585\n",
      "loss: 0.9671742916107178\n",
      "loss: 1.0531202554702759\n",
      "loss: 0.7912488579750061\n",
      "loss: 0.6874834299087524\n",
      "loss: 0.729293704032898\n",
      "loss: 0.6994349956512451\n",
      "loss: 0.6161845326423645\n",
      "loss: 0.6498031616210938\n",
      "loss: 0.8445549011230469\n",
      "loss: 0.8770655393600464\n",
      "loss: 0.8603298664093018\n",
      "loss: 0.9060971736907959\n",
      "loss: 0.7534409761428833\n",
      "loss: 0.6436227560043335\n",
      "loss: 0.6419264674186707\n",
      "loss: 0.6355001926422119\n",
      "loss: 0.5988016128540039\n",
      "loss: 0.6087065935134888\n",
      "loss: 0.8203661441802979\n",
      "loss: 0.789116382598877\n",
      "loss: 0.7679235339164734\n",
      "loss: 0.78138267993927\n",
      "loss: 0.7193123698234558\n",
      "loss: 0.6049751043319702\n",
      "loss: 0.5680492520332336\n",
      "loss: 0.5802527666091919\n",
      "loss: 0.5832043290138245\n",
      "loss: 0.5725046396255493\n",
      "loss: 0.7980925440788269\n",
      "loss: 0.7127225399017334\n",
      "loss: 0.6881747245788574\n",
      "loss: 0.6761714816093445\n",
      "loss: 0.6884776949882507\n",
      "loss: 0.5708540081977844\n",
      "loss: 0.5055474042892456\n",
      "loss: 0.5324253439903259\n",
      "loss: 0.5691815614700317\n",
      "loss: 0.5405362248420715\n",
      "loss: 0.7775731086730957\n",
      "loss: 0.6463608741760254\n",
      "loss: 0.6193533539772034\n",
      "loss: 0.5876469612121582\n",
      "loss: 0.6605655550956726\n",
      "loss: 0.5406363010406494\n",
      "loss: 0.4525378346443176\n",
      "loss: 0.4908881187438965\n",
      "loss: 0.5565394163131714\n",
      "loss: 0.5122089385986328\n",
      "loss: 0.7938446998596191\n",
      "loss: 0.837256908416748\n",
      "loss: 0.8764567971229553\n",
      "loss: 0.7120084762573242\n",
      "loss: 1.1150357723236084\n",
      "loss: 0.457599014043808\n",
      "loss: 0.4564135670661926\n",
      "loss: 0.43184319138526917\n",
      "loss: 0.5366871953010559\n",
      "loss: 0.47349876165390015\n",
      "loss: 0.772452175617218\n",
      "loss: 0.7569024562835693\n",
      "loss: 0.7871056795120239\n",
      "loss: 0.6152156591415405\n",
      "loss: 1.0642626285552979\n",
      "loss: 0.4333672523498535\n",
      "loss: 0.4064481556415558\n",
      "loss: 0.3971260190010071\n",
      "loss: 0.524831235408783\n",
      "loss: 0.44824182987213135\n",
      "loss: 0.7527137994766235\n",
      "loss: 0.6867017149925232\n",
      "loss: 0.7092812061309814\n",
      "loss: 0.533967137336731\n",
      "loss: 1.017627239227295\n",
      "loss: 0.4119822084903717\n",
      "loss: 0.3642144501209259\n",
      "loss: 0.3670906126499176\n",
      "loss: 0.5141493082046509\n",
      "loss: 0.4258822798728943\n",
      "loss: 0.7344790101051331\n",
      "loss: 0.6253261566162109\n",
      "loss: 0.6414806842803955\n",
      "loss: 0.4657681882381439\n",
      "loss: 0.9747627973556519\n",
      "loss: 0.3930254280567169\n",
      "loss: 0.3283468186855316\n",
      "loss: 0.340976357460022\n",
      "loss: 0.5044946670532227\n",
      "loss: 0.4060038924217224\n",
      "loss: 0.717599093914032\n",
      "loss: 0.571565568447113\n",
      "loss: 0.5823268294334412\n",
      "loss: 0.4084189832210541\n",
      "loss: 0.9353131055831909\n",
      "loss: 0.3761407136917114\n",
      "loss: 0.29771947860717773\n",
      "loss: 0.318149209022522\n",
      "loss: 0.4957391917705536\n",
      "loss: 0.38825109601020813\n",
      "loss: 0.7019340991973877\n",
      "loss: 0.5243481397628784\n",
      "loss: 0.5305964946746826\n",
      "loss: 0.3600417375564575\n",
      "loss: 0.8989434242248535\n",
      "loss: 0.36102789640426636\n",
      "loss: 0.2714133560657501\n",
      "loss: 0.29808518290519714\n",
      "loss: 0.48777157068252563\n",
      "loss: 0.37232327461242676\n",
      "loss: 0.6873553991317749\n",
      "loss: 0.4827437102794647\n",
      "loss: 0.4852242171764374\n",
      "loss: 0.3190670907497406\n",
      "loss: 0.8653463125228882\n",
      "loss: 0.3474353849887848\n",
      "loss: 0.24868348240852356\n",
      "loss: 0.28035250306129456\n",
      "loss: 0.48049598932266235\n",
      "loss: 0.3579675257205963\n",
      "loss: 0.6737480759620667\n",
      "loss: 0.4459550678730011\n",
      "loss: 0.445296049118042\n",
      "loss: 0.28420183062553406\n",
      "loss: 0.834243893623352\n",
      "loss: 0.3351525366306305\n",
      "loss: 0.2289266735315323\n",
      "loss: 0.26459625363349915\n",
      "loss: 0.47382983565330505\n",
      "loss: 0.3449709415435791\n",
      "loss: 0.661009669303894\n",
      "loss: 0.41330409049987793\n",
      "loss: 0.4100337326526642\n",
      "loss: 0.2543877959251404\n",
      "loss: 0.8053869009017944\n",
      "loss: 0.3240031599998474\n",
      "loss: 0.21165424585342407\n",
      "loss: 0.2505239248275757\n",
      "loss: 0.46770256757736206\n",
      "loss: 0.3331547975540161\n",
      "loss: 0.649050235748291\n",
      "loss: 0.3842160999774933\n",
      "loss: 0.37877777218818665\n",
      "loss: 0.22876332700252533\n",
      "loss: 0.7785534262657166\n",
      "loss: 0.313839316368103\n",
      "loss: 0.19646932184696198\n",
      "loss: 0.2378939688205719\n",
      "loss: 0.4620535373687744\n",
      "loss: 0.3223686218261719\n",
      "loss: 0.6377906203269958\n",
      "loss: 0.3582051694393158\n",
      "loss: 0.35097092390060425\n",
      "loss: 0.2066269963979721\n",
      "loss: 0.7535465359687805\n",
      "loss: 0.30453622341156006\n",
      "loss: 0.1830485463142395\n",
      "loss: 0.22650589048862457\n",
      "loss: 0.4568304121494293\n",
      "loss: 0.312484472990036\n",
      "loss: 0.6271619200706482\n",
      "loss: 0.3348606526851654\n",
      "loss: 0.32614266872406006\n",
      "loss: 0.18740780651569366\n",
      "loss: 0.7301918864250183\n",
      "loss: 0.2959890365600586\n",
      "loss: 0.17112691700458527\n",
      "loss: 0.2161928117275238\n",
      "loss: 0.45198795199394226\n",
      "loss: 0.3033949136734009\n",
      "loss: 0.6171033382415771\n",
      "loss: 0.3138345181941986\n",
      "loss: 0.30389508605003357\n",
      "loss: 0.17063964903354645\n",
      "loss: 0.7083343863487244\n",
      "loss: 0.2881085276603699\n",
      "loss: 0.16048641502857208\n",
      "loss: 0.20681506395339966\n",
      "loss: 0.44748711585998535\n",
      "loss: 0.2950077950954437\n",
      "loss: 0.6075616478919983\n",
      "loss: 0.29483142495155334\n",
      "loss: 0.2838907837867737\n",
      "loss: 0.15594059228897095\n",
      "loss: 0.6878369450569153\n",
      "loss: 0.28081849217414856\n",
      "loss: 0.15094658732414246\n",
      "loss: 0.19825507700443268\n",
      "loss: 0.44329386949539185\n",
      "loss: 0.28724420070648193\n",
      "loss: 0.5984905958175659\n",
      "loss: 0.2776002883911133\n",
      "loss: 0.26584360003471375\n",
      "loss: 0.14299684762954712\n",
      "loss: 0.6685779690742493\n",
      "loss: 0.2740536630153656\n",
      "loss: 0.14235731959342957\n",
      "loss: 0.19041316211223602\n",
      "loss: 0.43937864899635315\n",
      "loss: 0.2800365388393402\n",
      "loss: 0.29122623801231384\n",
      "loss: 0.3517214059829712\n",
      "loss: 0.42928066849708557\n",
      "loss: 0.20800139009952545\n",
      "loss: 1.0282877683639526\n",
      "loss: 0.6929935216903687\n",
      "loss: 0.7311810851097107\n",
      "loss: 0.6260567307472229\n",
      "loss: 1.0444010496139526\n",
      "loss: 0.7967427968978882\n",
      "loss: 0.28787997364997864\n",
      "loss: 0.3169044554233551\n",
      "loss: 0.3833707571029663\n",
      "loss: 0.17819058895111084\n",
      "loss: 0.983817458152771\n",
      "loss: 0.6531631946563721\n",
      "loss: 0.6521836519241333\n",
      "loss: 0.5740877985954285\n",
      "loss: 1.0127315521240234\n",
      "loss: 0.7510612607002258\n",
      "loss: 0.2849511504173279\n",
      "loss: 0.2871382236480713\n",
      "loss: 0.3442005217075348\n",
      "loss: 0.15400958061218262\n",
      "loss: 0.9427487850189209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6174541711807251\n",
      "loss: 0.5839044451713562\n",
      "loss: 0.5284854173660278\n",
      "loss: 0.9834186434745789\n",
      "loss: 0.7099145650863647\n",
      "loss: 0.28237104415893555\n",
      "loss: 0.26156219840049744\n",
      "loss: 0.31064677238464355\n",
      "loss: 0.1342376321554184\n",
      "loss: 0.9047870635986328\n",
      "loss: 0.5853758454322815\n",
      "loss: 0.5248542428016663\n",
      "loss: 0.48837608098983765\n",
      "loss: 0.9562734961509705\n",
      "loss: 0.6727878451347351\n",
      "loss: 0.28008195757865906\n",
      "loss: 0.2394685596227646\n",
      "loss: 0.2817738950252533\n",
      "loss: 0.11793599277734756\n",
      "loss: 0.8696542978286743\n",
      "loss: 0.5564895868301392\n",
      "loss: 0.47370606660842896\n",
      "loss: 0.4529968500137329\n",
      "loss: 0.9311127662658691\n",
      "loss: 0.6392164826393127\n",
      "loss: 0.27803584933280945\n",
      "loss: 0.22027820348739624\n",
      "loss: 0.2568083107471466\n",
      "loss: 0.10438303649425507\n",
      "loss: 0.8370926380157471\n",
      "loss: 0.5304075479507446\n",
      "loss: 0.4292997121810913\n",
      "loss: 0.421689510345459\n",
      "loss: 0.9077645540237427\n",
      "loss: 0.6087870001792908\n",
      "loss: 0.27619242668151855\n",
      "loss: 0.2035171389579773\n",
      "loss: 0.23511263728141785\n",
      "loss: 0.09302245080471039\n",
      "loss: 0.8068654537200928\n",
      "loss: 0.5067894458770752\n",
      "loss: 0.3906346261501312\n",
      "loss: 0.3938884437084198\n",
      "loss: 0.8860675096511841\n",
      "loss: 0.5811338424682617\n",
      "loss: 0.2745187282562256\n",
      "loss: 0.18879848718643188\n",
      "loss: 0.21616336703300476\n",
      "loss: 0.08342377096414566\n",
      "loss: 0.7787576913833618\n",
      "loss: 0.4853384792804718\n",
      "loss: 0.35685789585113525\n",
      "loss: 0.3691118657588959\n",
      "loss: 0.8658739924430847\n",
      "loss: 0.5559361577033997\n",
      "loss: 0.27298709750175476\n",
      "loss: 0.17580506205558777\n",
      "loss: 0.19953015446662903\n",
      "loss: 0.07525213062763214\n",
      "loss: 0.7525742650032043\n",
      "loss: 0.46579694747924805\n",
      "loss: 0.3272460699081421\n",
      "loss: 0.3469497263431549\n",
      "loss: 0.8470489978790283\n",
      "loss: 0.5329128503799438\n",
      "loss: 0.2715749144554138\n",
      "loss: 0.16427618265151978\n",
      "loss: 0.18485844135284424\n",
      "loss: 0.06824474781751633\n",
      "loss: 0.7281395792961121\n",
      "loss: 0.44794076681137085\n",
      "loss: 0.30118924379348755\n",
      "loss: 0.3270537853240967\n",
      "loss: 0.8294698596000671\n",
      "loss: 0.5118190050125122\n",
      "loss: 0.2702634632587433\n",
      "loss: 0.1539972722530365\n",
      "loss: 0.17185591161251068\n",
      "loss: 0.06219472363591194\n",
      "loss: 0.705295979976654\n",
      "loss: 0.4315761923789978\n",
      "loss: 0.2781735956668854\n",
      "loss: 0.30912765860557556\n",
      "loss: 0.8130256533622742\n",
      "loss: 0.4924401044845581\n",
      "loss: 0.26903700828552246\n",
      "loss: 0.14479026198387146\n",
      "loss: 0.1602802723646164\n",
      "loss: 0.05693770572543144\n",
      "loss: 0.6839017271995544\n",
      "loss: 0.4165347218513489\n",
      "loss: 0.25776681303977966\n",
      "loss: 0.29291966557502747\n",
      "loss: 0.7976168394088745\n",
      "loss: 0.47459015250205994\n",
      "loss: 0.26788249611854553\n",
      "loss: 0.13650739192962646\n",
      "loss: 0.14993023872375488\n",
      "loss: 0.05234208330512047\n",
      "loss: 0.6638298630714417\n",
      "loss: 0.4026707410812378\n",
      "loss: 0.23960496485233307\n",
      "loss: 0.27821484208106995\n",
      "loss: 0.7831535339355469\n",
      "loss: 0.4581056237220764\n",
      "loss: 0.26678916811943054\n",
      "loss: 0.12902481853961945\n",
      "loss: 0.14063768088817596\n",
      "loss: 0.04830186814069748\n",
      "loss: 0.6449667811393738\n",
      "loss: 0.3898567259311676\n",
      "loss: 0.2233813852071762\n",
      "loss: 0.26482993364334106\n",
      "loss: 0.7695549726486206\n",
      "loss: 0.44284436106681824\n",
      "loss: 0.26574796438217163\n",
      "loss: 0.12223897129297256\n",
      "loss: 0.13226190209388733\n",
      "loss: 0.044731080532073975\n",
      "loss: 0.6272104978561401\n",
      "loss: 0.3779825270175934\n",
      "loss: 0.20883731544017792\n",
      "loss: 0.2526080012321472\n",
      "loss: 0.7567486763000488\n",
      "loss: 0.4286818504333496\n",
      "loss: 0.26475125551223755\n",
      "loss: 0.11606219410896301\n",
      "loss: 0.12468430399894714\n",
      "loss: 0.04155946150422096\n",
      "loss: 0.6104686260223389\n",
      "loss: 0.3669518232345581\n",
      "loss: 0.19575341045856476\n",
      "loss: 0.2414141148328781\n",
      "loss: 0.7446698546409607\n",
      "loss: 0.41550853848457336\n",
      "loss: 0.2637927234172821\n",
      "loss: 0.11042030900716782\n",
      "loss: 0.117804616689682\n",
      "loss: 0.03872935473918915\n",
      "loss: 0.5946594476699829\n",
      "loss: 0.35668015480041504\n",
      "loss: 0.18394388258457184\n",
      "loss: 0.23113250732421875\n",
      "loss: 0.7332594990730286\n",
      "loss: 0.40322840213775635\n",
      "loss: 0.2628670334815979\n",
      "loss: 0.1052502766251564\n",
      "loss: 0.11153793334960938\n",
      "loss: 0.036192961037158966\n",
      "loss: 0.5797086358070374\n",
      "loss: 0.3470933437347412\n",
      "loss: 0.1732501983642578\n",
      "loss: 0.2216627150774002\n",
      "loss: 0.7224648594856262\n",
      "loss: 0.39175674319267273\n",
      "loss: 0.26196983456611633\n",
      "loss: 0.10049805045127869\n",
      "loss: 0.10581152141094208\n",
      "loss: 0.03391058370471001\n",
      "loss: 0.5655492544174194\n",
      "loss: 0.33812665939331055\n",
      "loss: 0.16353698074817657\n",
      "loss: 0.21291762590408325\n",
      "loss: 0.7122384905815125\n",
      "loss: 0.3810190260410309\n",
      "loss: 0.2610970735549927\n",
      "loss: 0.09611712396144867\n",
      "loss: 0.10056322813034058\n",
      "loss: 0.031848806887865067\n",
      "loss: 0.552121102809906\n",
      "loss: 0.3297222852706909\n",
      "loss: 0.1546884924173355\n",
      "loss: 0.2048218548297882\n",
      "loss: 0.7025371789932251\n",
      "loss: 0.37094846367836\n",
      "loss: 0.31381428241729736\n",
      "loss: 0.2452852427959442\n",
      "loss: 0.2811569273471832\n",
      "loss: 0.12065590918064117\n",
      "loss: 0.8529180884361267\n",
      "loss: 0.5003345608711243\n",
      "loss: 0.4001818001270294\n",
      "loss: 0.39826926589012146\n",
      "loss: 0.8424630165100098\n",
      "loss: 0.5690720081329346\n",
      "loss: 0.3111909329891205\n",
      "loss: 0.22664517164230347\n",
      "loss: 0.2575308382511139\n",
      "loss: 0.10754591226577759\n",
      "loss: 0.8220850825309753\n",
      "loss: 0.47869032621383667\n",
      "loss: 0.36471182107925415\n",
      "loss: 0.3725864887237549\n",
      "loss: 0.8231433033943176\n",
      "loss: 0.5438559055328369\n",
      "loss: 0.3087805211544037\n",
      "loss: 0.21025916934013367\n",
      "loss: 0.23686940968036652\n",
      "loss: 0.09645701944828033\n",
      "loss: 0.7934248447418213\n",
      "loss: 0.4590408205986023\n",
      "loss: 0.33373740315437317\n",
      "loss: 0.34969809651374817\n",
      "loss: 0.8051878809928894\n",
      "loss: 0.5208945274353027\n",
      "loss: 0.3065509796142578\n",
      "loss: 0.19577950239181519\n",
      "loss: 0.2187114804983139\n",
      "loss: 0.08700712770223618\n",
      "loss: 0.7667349576950073\n",
      "loss: 0.441143274307251\n",
      "loss: 0.3065812885761261\n",
      "loss: 0.32921937108039856\n",
      "loss: 0.7884693145751953\n",
      "loss: 0.499923437833786\n",
      "loss: 0.3044756054878235\n",
      "loss: 0.18292014300823212\n",
      "loss: 0.2026768922805786\n",
      "loss: 0.0788964331150055\n",
      "loss: 0.7418329119682312\n",
      "loss: 0.42478859424591064\n",
      "loss: 0.28267616033554077\n",
      "loss: 0.3108256757259369\n",
      "loss: 0.7728731036186218\n",
      "loss: 0.48071369528770447\n",
      "loss: 0.3025321066379547\n",
      "loss: 0.17144504189491272\n",
      "loss: 0.18845140933990479\n",
      "loss: 0.07188822329044342\n",
      "loss: 0.7185556888580322\n",
      "loss: 0.40979665517807007\n",
      "loss: 0.261547327041626\n",
      "loss: 0.29424208402633667\n",
      "loss: 0.7582961320877075\n",
      "loss: 0.4630665183067322\n",
      "loss: 0.30070194602012634\n",
      "loss: 0.16115838289260864\n",
      "loss: 0.17577451467514038\n",
      "loss: 0.06579415500164032\n",
      "loss: 0.6967572569847107\n",
      "loss: 0.39601194858551025\n",
      "loss: 0.24279722571372986\n",
      "loss: 0.27923548221588135\n",
      "loss: 0.7446464896202087\n",
      "loss: 0.4468090832233429\n",
      "loss: 0.2989698052406311\n",
      "loss: 0.1518973708152771\n",
      "loss: 0.1644294112920761\n",
      "loss: 0.060463327914476395\n",
      "loss: 0.676307201385498\n",
      "loss: 0.3832997679710388\n",
      "loss: 0.22609257698059082\n",
      "loss: 0.2656080424785614\n",
      "loss: 0.7318419814109802\n",
      "loss: 0.4317916929721832\n",
      "loss: 0.29732269048690796\n",
      "loss: 0.14352580904960632\n",
      "loss: 0.1542348861694336\n",
      "loss: 0.05577424541115761\n",
      "loss: 0.6570884585380554\n",
      "loss: 0.3715437650680542\n",
      "loss: 0.21115358173847198\n",
      "loss: 0.2531915605068207\n",
      "loss: 0.7198087573051453\n",
      "loss: 0.4178837835788727\n",
      "loss: 0.2957494854927063\n",
      "loss: 0.13592921197414398\n",
      "loss: 0.1450391262769699\n",
      "loss: 0.05162796378135681\n",
      "loss: 0.6389963626861572\n",
      "loss: 0.3606431186199188\n",
      "loss: 0.1977444440126419\n",
      "loss: 0.24184194207191467\n",
      "loss: 0.70848149061203\n",
      "loss: 0.4049715995788574\n",
      "loss: 0.29424113035202026\n",
      "loss: 0.12901072204113007\n",
      "loss: 0.1367136836051941\n",
      "loss: 0.04794374480843544\n",
      "loss: 0.6219371557235718\n",
      "loss: 0.3505093455314636\n",
      "loss: 0.1856655478477478\n",
      "loss: 0.23143544793128967\n",
      "loss: 0.6978009939193726\n",
      "loss: 0.3929554224014282\n",
      "loss: 0.2927896976470947\n",
      "loss: 0.12268846482038498\n",
      "loss: 0.1291504204273224\n",
      "loss: 0.04465516656637192\n",
      "loss: 0.6058266758918762\n",
      "loss: 0.3410661220550537\n",
      "loss: 0.17474830150604248\n",
      "loss: 0.22186627984046936\n",
      "loss: 0.6877142190933228\n",
      "loss: 0.3817478120326996\n",
      "loss: 0.29138872027397156\n",
      "loss: 0.11689259856939316\n",
      "loss: 0.12225688993930817\n",
      "loss: 0.041707102209329605\n",
      "loss: 0.590589165687561\n",
      "loss: 0.3322455883026123\n",
      "loss: 0.1648487150669098\n",
      "loss: 0.21304234862327576\n",
      "loss: 0.6781738996505737\n",
      "loss: 0.3712719976902008\n",
      "loss: 0.29003238677978516\n",
      "loss: 0.11156319081783295\n",
      "loss: 0.11595456302165985\n",
      "loss: 0.039053697139024734\n",
      "loss: 0.5761564373970032\n",
      "loss: 0.3239890933036804\n",
      "loss: 0.15584425628185272\n",
      "loss: 0.20488440990447998\n",
      "loss: 0.6691376566886902\n",
      "loss: 0.36146003007888794\n",
      "loss: 0.2887159287929535\n",
      "loss: 0.10664869844913483\n",
      "loss: 0.11017565429210663\n",
      "loss: 0.036656454205513\n",
      "loss: 0.5624673366546631\n",
      "loss: 0.3162441551685333\n",
      "loss: 0.14762970805168152\n",
      "loss: 0.19732345640659332\n",
      "loss: 0.6605666875839233\n",
      "loss: 0.35225221514701843\n",
      "loss: 0.28743526339530945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.10210481286048889\n",
      "loss: 0.10486223548650742\n",
      "loss: 0.03448314219713211\n",
      "loss: 0.5494660139083862\n",
      "loss: 0.30896511673927307\n",
      "loss: 0.14011463522911072\n",
      "loss: 0.19029909372329712\n",
      "loss: 0.6524264216423035\n",
      "loss: 0.3435952365398407\n",
      "loss: 0.2861866354942322\n",
      "loss: 0.09789281338453293\n",
      "loss: 0.09996402263641357\n",
      "loss: 0.032506201416254044\n",
      "loss: 0.537102222442627\n",
      "loss: 0.3021109700202942\n",
      "loss: 0.1332210898399353\n",
      "loss: 0.18375855684280396\n",
      "loss: 0.6446852087974548\n",
      "loss: 0.33544185757637024\n",
      "loss: 0.28496700525283813\n",
      "loss: 0.09397927671670914\n",
      "loss: 0.09543728828430176\n",
      "loss: 0.03070223517715931\n",
      "loss: 0.5253303647041321\n",
      "loss: 0.2956453263759613\n",
      "loss: 0.12688130140304565\n",
      "loss: 0.177655428647995\n",
      "loss: 0.6373149752616882\n",
      "loss: 0.327749639749527\n",
      "loss: 0.28377363085746765\n",
      "loss: 0.0903346836566925\n",
      "loss: 0.09124401211738586\n",
      "loss: 0.029051348567008972\n",
      "loss: 0.5141090154647827\n",
      "loss: 0.28953590989112854\n",
      "loss: 0.12103667110204697\n",
      "loss: 0.17194892466068268\n",
      "loss: 0.6302897334098816\n",
      "loss: 0.3204813003540039\n",
      "loss: 0.28260427713394165\n",
      "loss: 0.08693352341651917\n",
      "loss: 0.08735115826129913\n",
      "loss: 0.02753637358546257\n",
      "loss: 0.5034007430076599\n",
      "loss: 0.2837541401386261\n",
      "loss: 0.11563601344823837\n",
      "loss: 0.16660328209400177\n",
      "loss: 0.6235859394073486\n",
      "loss: 0.31360283493995667\n",
      "loss: 0.28145673871040344\n",
      "loss: 0.08375315368175507\n",
      "loss: 0.08372949808835983\n",
      "loss: 0.026142440736293793\n",
      "loss: 0.4931707978248596\n",
      "loss: 0.2782741189002991\n",
      "loss: 0.11063499748706818\n",
      "loss: 0.16158641874790192\n",
      "loss: 0.6171823143959045\n",
      "loss: 0.3070845305919647\n",
      "loss: 0.28032970428466797\n",
      "loss: 0.08077353239059448\n",
      "loss: 0.08035353571176529\n",
      "loss: 0.024856824427843094\n",
      "loss: 0.4833875894546509\n",
      "loss: 0.2730724513530731\n",
      "loss: 0.10599368065595627\n",
      "loss: 0.15686984360218048\n",
      "loss: 0.6110590100288391\n",
      "loss: 0.3008982539176941\n",
      "loss: 0.27922120690345764\n",
      "loss: 0.07797686755657196\n",
      "loss: 0.07720048725605011\n",
      "loss: 0.02366824820637703\n",
      "loss: 0.47402286529541016\n",
      "loss: 0.26812803745269775\n",
      "loss: 0.10167787224054337\n",
      "loss: 0.15242815017700195\n",
      "loss: 0.6051986217498779\n",
      "loss: 0.2950194776058197\n",
      "loss: 0.27812981605529785\n",
      "loss: 0.0753474310040474\n",
      "loss: 0.07425031810998917\n",
      "loss: 0.022567041218280792\n",
      "loss: 0.4650501310825348\n",
      "loss: 0.26342225074768066\n",
      "loss: 0.09765704721212387\n",
      "loss: 0.1482388824224472\n",
      "loss: 0.5995844602584839\n",
      "loss: 0.28942644596099854\n",
      "loss: 0.27705448865890503\n",
      "loss: 0.07287125289440155\n",
      "loss: 0.07148512452840805\n",
      "loss: 0.02154451049864292\n",
      "loss: 0.45644497871398926\n",
      "loss: 0.258937805891037\n",
      "loss: 0.09390398114919662\n",
      "loss: 0.14428143203258514\n",
      "loss: 0.5942014455795288\n",
      "loss: 0.28409814834594727\n",
      "loss: 1.082103967666626\n",
      "loss: 1.8354169130325317\n",
      "loss: 1.7999780178070068\n",
      "loss: 2.1732709407806396\n",
      "loss: 0.896454930305481\n",
      "loss: 1.0742863416671753\n",
      "loss: 1.344610571861267\n",
      "loss: 1.0808168649673462\n",
      "loss: 0.7168121933937073\n",
      "loss: 0.8260748386383057\n",
      "loss: 0.6330289244651794\n",
      "loss: 0.9788609147071838\n",
      "loss: 1.033361792564392\n",
      "loss: 0.8554987907409668\n",
      "loss: 1.0305925607681274\n",
      "loss: 0.7995278835296631\n",
      "loss: 0.8900138139724731\n",
      "loss: 0.7179452180862427\n",
      "loss: 0.7957203984260559\n",
      "loss: 0.6972950100898743\n",
      "loss: 0.5489428043365479\n",
      "loss: 0.545172929763794\n",
      "loss: 0.5601103901863098\n",
      "loss: 0.3727551996707916\n",
      "loss: 0.8129299879074097\n",
      "loss: 0.5837974548339844\n",
      "loss: 0.49105173349380493\n",
      "loss: 0.46631360054016113\n",
      "loss: 0.694550633430481\n",
      "loss: 0.5274806618690491\n",
      "loss: 0.4976460337638855\n",
      "loss: 0.342907577753067\n",
      "loss: 0.3420281410217285\n",
      "loss: 0.19290342926979065\n",
      "loss: 0.6740561127662659\n",
      "loss: 0.4622572660446167\n",
      "loss: 0.3060244023799896\n",
      "loss: 0.3355306386947632\n",
      "loss: 0.6289961338043213\n",
      "loss: 0.42789509892463684\n",
      "loss: 0.4632154107093811\n",
      "loss: 0.2390032261610031\n",
      "loss: 0.2318277657032013\n",
      "loss: 0.1158505529165268\n",
      "loss: 0.5797560214996338\n",
      "loss: 0.3872131407260895\n",
      "loss: 0.21093429625034332\n",
      "loss: 0.2596908509731293\n",
      "loss: 0.583667516708374\n",
      "loss: 0.36371883749961853\n",
      "loss: 0.437554270029068\n",
      "loss: 0.17873792350292206\n",
      "loss: 0.16914507746696472\n",
      "loss: 0.07702169567346573\n",
      "loss: 0.5115126371383667\n",
      "loss: 0.3367202579975128\n",
      "loss: 0.15627893805503845\n",
      "loss: 0.21128486096858978\n",
      "loss: 0.550498366355896\n",
      "loss: 0.3190957009792328\n",
      "loss: 0.6050443649291992\n",
      "loss: 1.2437989711761475\n",
      "loss: 1.294424295425415\n",
      "loss: 1.3996381759643555\n",
      "loss: 0.809404730796814\n",
      "loss: 1.3708374500274658\n",
      "loss: 1.7151062488555908\n",
      "loss: 1.3377587795257568\n",
      "loss: 1.0802757740020752\n",
      "loss: 1.1133537292480469\n",
      "loss: 0.5118716955184937\n",
      "loss: 0.6419581770896912\n",
      "loss: 0.6486058235168457\n",
      "loss: 0.5720371007919312\n",
      "loss: 0.6226535439491272\n",
      "loss: 0.9540386199951172\n",
      "loss: 0.9292876720428467\n",
      "loss: 0.8214515447616577\n",
      "loss: 0.9030754566192627\n",
      "loss: 0.7914978265762329\n",
      "loss: 0.4571876525878906\n",
      "loss: 0.3683134913444519\n",
      "loss: 0.36040884256362915\n",
      "loss: 0.2591230869293213\n",
      "loss: 0.5114954710006714\n",
      "loss: 0.7077006101608276\n",
      "loss: 0.5339893102645874\n",
      "loss: 0.5479968190193176\n",
      "loss: 0.7880610227584839\n",
      "loss: 0.6063000559806824\n",
      "loss: 0.42414799332618713\n",
      "loss: 0.2420518547296524\n",
      "loss: 0.22989842295646667\n",
      "loss: 0.14094460010528564\n",
      "loss: 0.4413903057575226\n",
      "loss: 0.5604820847511292\n",
      "loss: 0.33921417593955994\n",
      "loss: 0.3973505198955536\n",
      "loss: 0.7108990550041199\n",
      "loss: 0.49279627203941345\n",
      "loss: 0.40117818117141724\n",
      "loss: 0.17490313947200775\n",
      "loss: 0.1619238704442978\n",
      "loss: 0.08781139552593231\n",
      "loss: 0.3930244743824005\n",
      "loss: 0.46628570556640625\n",
      "loss: 0.23513782024383545\n",
      "loss: 0.30688926577568054\n",
      "loss: 0.6561270952224731\n",
      "loss: 0.4175458252429962\n",
      "loss: 0.38342392444610596\n",
      "loss: 0.13454490900039673\n",
      "loss: 0.12190514802932739\n",
      "loss: 0.059946395456790924\n",
      "loss: 0.35723021626472473\n",
      "loss: 0.40169405937194824\n",
      "loss: 0.17405842244625092\n",
      "loss: 0.24809275567531586\n",
      "loss: 0.6152849197387695\n",
      "loss: 0.3643394410610199\n",
      "loss: 0.36875998973846436\n",
      "loss: 0.1080377846956253\n",
      "loss: 0.09611869603395462\n",
      "loss: 0.043593183159828186\n",
      "loss: 0.32936784625053406\n",
      "loss: 0.3548520505428314\n",
      "loss: 0.13526511192321777\n",
      "loss: 0.2073996514081955\n",
      "loss: 0.5836451053619385\n",
      "loss: 0.32482194900512695\n",
      "loss: 0.3561314344406128\n",
      "loss: 0.089472196996212\n",
      "loss: 0.07836718112230301\n",
      "loss: 0.033178817480802536\n",
      "loss: 0.30686768889427185\n",
      "loss: 0.31937623023986816\n",
      "loss: 0.10904093831777573\n",
      "loss: 0.1778288632631302\n",
      "loss: 0.5584078431129456\n",
      "loss: 0.294342577457428\n",
      "loss: 0.34495270252227783\n",
      "loss: 0.07583175599575043\n",
      "loss: 0.06552599370479584\n",
      "loss: 0.02613038755953312\n",
      "loss: 0.28819146752357483\n",
      "loss: 0.29158639907836914\n",
      "loss: 0.09042360633611679\n",
      "loss: 0.15550272166728973\n",
      "loss: 0.5378173589706421\n",
      "loss: 0.27013030648231506\n",
      "loss: 0.3348686695098877\n",
      "loss: 0.06543563306331635\n",
      "loss: 0.055875085294246674\n",
      "loss: 0.02113203890621662\n",
      "loss: 0.2723575532436371\n",
      "loss: 0.26922813057899475\n",
      "loss: 0.07668165117502213\n",
      "loss: 0.13812433183193207\n",
      "loss: 0.5207155346870422\n",
      "loss: 0.2504383325576782\n",
      "loss: 1.3974449634552002\n",
      "loss: 2.2105116844177246\n",
      "loss: 2.1858675479888916\n",
      "loss: 2.466493606567383\n",
      "loss: 1.287436842918396\n",
      "loss: 0.7261976599693298\n",
      "loss: 0.9444351196289062\n",
      "loss: 0.7441872358322144\n",
      "loss: 0.5044726729393005\n",
      "loss: 0.5788971781730652\n",
      "loss: 1.1679608821868896\n",
      "loss: 1.3348830938339233\n",
      "loss: 1.2728263139724731\n",
      "loss: 1.2705090045928955\n",
      "loss: 0.9829106330871582\n",
      "loss: 0.5034714341163635\n",
      "loss: 0.4732338786125183\n",
      "loss: 0.45014500617980957\n",
      "loss: 0.4449652433395386\n",
      "loss: 0.42077741026878357\n",
      "loss: 0.9947208166122437\n",
      "loss: 0.8113223314285278\n",
      "loss: 0.7479444146156311\n",
      "loss: 0.6374367475509644\n",
      "loss: 0.7858383655548096\n",
      "loss: 0.38863396644592285\n",
      "loss: 0.2755780816078186\n",
      "loss: 0.3093837797641754\n",
      "loss: 0.411319375038147\n",
      "loss: 0.33522698283195496\n",
      "loss: 0.8735466599464417\n",
      "loss: 0.5285180807113647\n",
      "loss: 0.47402170300483704\n",
      "loss: 0.3485521674156189\n",
      "loss: 0.6576764583587646\n",
      "loss: 0.32344022393226624\n",
      "loss: 0.18379494547843933\n",
      "loss: 0.23431888222694397\n",
      "loss: 0.39075592160224915\n",
      "loss: 0.28416648507118225\n",
      "loss: 0.7866179943084717\n",
      "loss: 0.3705878257751465\n",
      "loss: 0.32494303584098816\n",
      "loss: 0.21131600439548492\n",
      "loss: 0.5691355466842651\n",
      "loss: 0.28192606568336487\n",
      "loss: 0.1343950480222702\n",
      "loss: 0.18906164169311523\n",
      "loss: 0.37716156244277954\n",
      "loss: 0.2505057156085968\n",
      "loss: 0.7207149863243103\n",
      "loss: 0.2753470242023468\n",
      "loss: 0.23692697286605835\n",
      "loss: 0.13922759890556335\n",
      "loss: 0.5042254328727722\n",
      "loss: 0.25311279296875\n",
      "loss: 0.10453959554433823\n",
      "loss: 0.15911000967025757\n",
      "loss: 0.36765897274017334\n",
      "loss: 0.22660091519355774\n",
      "loss: 0.6683499217033386\n",
      "loss: 0.21369102597236633\n",
      "loss: 0.18098358809947968\n",
      "loss: 0.09767144173383713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.45441973209381104\n",
      "loss: 0.23184999823570251\n",
      "loss: 0.08490928262472153\n",
      "loss: 0.13790392875671387\n",
      "loss: 0.36076828837394714\n",
      "loss: 0.2086859494447708\n",
      "loss: 0.6252442002296448\n",
      "loss: 0.17142240703105927\n",
      "loss: 0.14324238896369934\n",
      "loss: 0.07185465842485428\n",
      "loss: 0.4148623049259186\n",
      "loss: 0.215443953871727\n",
      "loss: 0.0711762011051178\n",
      "loss: 0.12212386727333069\n",
      "loss: 0.35565489530563354\n",
      "loss: 0.19471591711044312\n",
      "loss: 0.5888087153434753\n",
      "loss: 0.1410977840423584\n",
      "loss: 0.11654595285654068\n",
      "loss: 0.05484148859977722\n",
      "loss: 0.38259604573249817\n",
      "loss: 0.202354297041893\n",
      "loss: 0.06110692769289017\n",
      "loss: 0.10992919653654099\n",
      "loss: 0.3518107235431671\n",
      "loss: 0.1834876984357834\n",
      "loss: 0.5573839545249939\n",
      "loss: 0.11853775382041931\n",
      "loss: 0.09693236649036407\n",
      "loss: 0.04309204965829849\n",
      "loss: 0.355716735124588\n",
      "loss: 0.19163639843463898\n",
      "loss: 0.05344942957162857\n",
      "loss: 0.10022305697202682\n",
      "loss: 0.3489077389240265\n",
      "loss: 0.1742471307516098\n",
      "loss: 0.5298511385917664\n",
      "loss: 0.1012529581785202\n",
      "loss: 0.08207198977470398\n",
      "loss: 0.034665532410144806\n",
      "loss: 0.33293992280960083\n",
      "loss: 0.18267755210399628\n",
      "loss: 0.04745399206876755\n",
      "loss: 0.0923130065202713\n",
      "loss: 0.3467233180999756\n",
      "loss: 0.16649667918682098\n",
      "loss: 0.5054243206977844\n",
      "loss: 0.08768521994352341\n",
      "loss: 0.07052359730005264\n",
      "loss: 0.028432095423340797\n",
      "loss: 0.31336668133735657\n",
      "loss: 0.175062894821167\n",
      "loss: 0.04264708608388901\n",
      "loss: 0.08574115484952927\n",
      "loss: 0.3451004922389984\n",
      "loss: 0.1598944216966629\n",
      "loss: 0.48353123664855957\n",
      "loss: 0.07681738585233688\n",
      "loss: 0.06135660037398338\n",
      "loss: 0.023700717836618423\n",
      "loss: 0.29634740948677063\n",
      "loss: 0.16849994659423828\n",
      "loss: 0.03871677443385124\n",
      "loss: 0.08019284158945084\n",
      "loss: 0.3439249098300934\n",
      "loss: 0.1541968435049057\n",
      "loss: 0.4637436270713806\n",
      "loss: 0.06796196103096008\n",
      "loss: 0.053948041051626205\n",
      "loss: 0.020030518993735313\n",
      "loss: 0.28139981627464294\n",
      "loss: 0.16277720034122467\n",
      "loss: 0.035449519753456116\n",
      "loss: 0.07544474303722382\n",
      "loss: 0.3431112766265869\n",
      "loss: 0.1492258459329605\n",
      "loss: 0.44573259353637695\n",
      "loss: 0.06063956394791603\n",
      "loss: 0.04786777123808861\n",
      "loss: 0.017130171880126\n",
      "loss: 0.26815804839134216\n",
      "loss: 0.1577371507883072\n",
      "loss: 0.032695066183805466\n",
      "loss: 0.0713343620300293\n",
      "loss: 0.34259435534477234\n",
      "loss: 0.14484813809394836\n",
      "loss: 0.4056810140609741\n",
      "loss: 0.2764998972415924\n",
      "loss: 0.25204041600227356\n",
      "loss: 0.21930551528930664\n",
      "loss: 0.2907247841358185\n",
      "loss: 0.9077204465866089\n",
      "loss: 0.6422420144081116\n",
      "loss: 0.6995124220848083\n",
      "loss: 0.895902156829834\n",
      "loss: 0.7267831563949585\n",
      "loss: 0.38048362731933594\n",
      "loss: 0.18455567955970764\n",
      "loss: 0.16363054513931274\n",
      "loss: 0.11950547993183136\n",
      "loss: 0.26240214705467224\n",
      "loss: 0.711007833480835\n",
      "loss: 0.4047052264213562\n",
      "loss: 0.5020235776901245\n",
      "loss: 0.8013484477996826\n",
      "loss: 0.5848147869110107\n",
      "loss: 0.36256957054138184\n",
      "loss: 0.13510386645793915\n",
      "loss: 0.11718286573886871\n",
      "loss: 0.07441320270299911\n",
      "loss: 0.2429000437259674\n",
      "loss: 0.5826100707054138\n",
      "loss: 0.2762663662433624\n",
      "loss: 0.38179102540016174\n",
      "loss: 0.73360276222229\n",
      "loss: 0.48988115787506104\n",
      "loss: 0.34851062297821045\n",
      "loss: 0.10514479875564575\n",
      "loss: 0.08960459381341934\n",
      "loss: 0.05079852417111397\n",
      "loss: 0.2283148467540741\n",
      "loss: 0.49423298239707947\n",
      "loss: 0.20125636458396912\n",
      "loss: 0.30370956659317017\n",
      "loss: 0.6828675866127014\n",
      "loss: 0.4227396845817566\n",
      "loss: 0.3367689251899719\n",
      "loss: 0.08533574640750885\n",
      "loss: 0.07167963683605194\n",
      "loss: 0.036975547671318054\n",
      "loss: 0.2167586237192154\n",
      "loss: 0.4303475618362427\n",
      "loss: 0.15411680936813354\n",
      "loss: 0.2500452995300293\n",
      "loss: 0.6434729099273682\n",
      "loss: 0.37303534150123596\n",
      "loss: 0.3265634775161743\n",
      "loss: 0.07137232273817062\n",
      "loss: 0.05922897905111313\n",
      "loss: 0.028190569952130318\n",
      "loss: 0.20722350478172302\n",
      "loss: 0.38225889205932617\n",
      "loss: 0.122628353536129\n",
      "loss: 0.2114119529724121\n",
      "loss: 0.6120009422302246\n",
      "loss: 0.3348805010318756\n",
      "loss: 0.31745466589927673\n",
      "loss: 0.06104743480682373\n",
      "loss: 0.0501399002969265\n",
      "loss: 0.022251902148127556\n",
      "loss: 0.19912375509738922\n",
      "loss: 0.3448590338230133\n",
      "loss: 0.10053297877311707\n",
      "loss: 0.1825362592935562\n",
      "loss: 0.5862883925437927\n",
      "loss: 0.30473029613494873\n",
      "loss: 0.309174507856369\n",
      "loss: 0.05312761291861534\n",
      "loss: 0.04324651509523392\n",
      "loss: 0.018041983246803284\n",
      "loss: 0.19209249317646027\n",
      "loss: 0.3149931728839874\n",
      "loss: 0.08440037816762924\n",
      "loss: 0.16028475761413574\n",
      "loss: 0.5649017095565796\n",
      "loss: 0.2803398072719574\n",
      "loss: 0.30154889822006226\n",
      "loss: 0.04687495902180672\n",
      "loss: 0.037858784198760986\n",
      "loss: 0.014943779446184635\n",
      "loss: 0.18588696420192719\n",
      "loss: 0.2906220257282257\n",
      "loss: 0.07223330438137054\n",
      "loss: 0.14270126819610596\n",
      "loss: 0.5468536615371704\n",
      "loss: 0.2602248191833496\n",
      "loss: 0.29445818066596985\n",
      "loss: 0.04182282090187073\n",
      "loss: 0.033544816076755524\n",
      "loss: 0.012593920342624187\n",
      "loss: 0.18033893406391144\n",
      "loss: 0.27037522196769714\n",
      "loss: 0.06280814856290817\n",
      "loss: 0.12851214408874512\n",
      "loss: 0.5314403772354126\n",
      "loss: 0.24336650967597961\n",
      "loss: 0.2878168821334839\n",
      "loss: 0.03766237944364548\n",
      "loss: 0.030021334066987038\n",
      "loss: 0.010767042636871338\n",
      "loss: 0.17532694339752197\n",
      "loss: 0.25329914689064026\n",
      "loss: 0.05534180626273155\n",
      "loss: 0.11685796082019806\n",
      "loss: 0.5181465148925781\n",
      "loss: 0.2290443331003189\n",
      "loss: 0.28156155347824097\n",
      "loss: 0.03418168053030968\n",
      "loss: 0.027095520868897438\n",
      "loss: 0.009317096322774887\n",
      "loss: 0.17076067626476288\n",
      "loss: 0.23871161043643951\n",
      "loss: 0.04931382089853287\n",
      "loss: 0.10713986307382584\n",
      "loss: 0.5065847635269165\n",
      "loss: 0.2167341262102127\n",
      "loss: 0.27564316987991333\n",
      "loss: 0.031230464577674866\n",
      "loss: 0.0246318019926548\n",
      "loss: 0.008146055974066257\n",
      "loss: 0.16657109558582306\n",
      "loss: 0.22611193358898163\n",
      "loss: 0.04436732083559036\n",
      "loss: 0.09893018007278442\n",
      "loss: 0.49645906686782837\n",
      "loss: 0.20604638755321503\n",
      "loss: 0.2700239419937134\n",
      "loss: 0.028699440881609917\n",
      "loss: 0.022532230243086815\n",
      "loss: 0.007186003960669041\n",
      "loss: 0.16270418465137482\n",
      "loss: 0.21512483060359955\n",
      "loss: 0.04025109112262726\n",
      "loss: 0.09191577881574631\n",
      "loss: 0.48753756284713745\n",
      "loss: 0.19668559730052948\n",
      "loss: 0.26467275619506836\n",
      "loss: 0.02650715969502926\n",
      "loss: 0.020724304020404816\n",
      "loss: 0.006388703361153603\n",
      "loss: 0.15911713242530823\n",
      "loss: 0.205463245511055\n",
      "loss: 0.03678333759307861\n",
      "loss: 0.0858624055981636\n",
      "loss: 0.47963640093803406\n",
      "loss: 0.1884232610464096\n",
      "loss: 0.2595643401145935\n",
      "loss: 0.024591848254203796\n",
      "loss: 0.019153345376253128\n",
      "loss: 0.005718936678022146\n",
      "loss: 0.15577495098114014\n",
      "loss: 0.1969042420387268\n",
      "loss: 0.0338301807641983\n",
      "loss: 0.08059235662221909\n",
      "loss: 0.472608357667923\n",
      "loss: 0.1810806691646576\n",
      "loss: 0.25467756390571594\n",
      "loss: 0.02290569804608822\n",
      "loss: 0.01777741126716137\n",
      "loss: 0.005150745622813702\n",
      "loss: 0.15264926850795746\n",
      "loss: 0.18927215039730072\n",
      "loss: 0.03129122406244278\n",
      "loss: 0.07596827298402786\n",
      "loss: 0.466333270072937\n",
      "loss: 0.17451557517051697\n",
      "loss: 0.24999448657035828\n",
      "loss: 0.02141120284795761\n",
      "loss: 0.01656370237469673\n",
      "loss: 0.004664374515414238\n",
      "loss: 0.14971601963043213\n",
      "loss: 0.18242637813091278\n",
      "loss: 0.02908957749605179\n",
      "loss: 0.07188238948583603\n",
      "loss: 0.460712194442749\n",
      "loss: 0.1686132550239563\n",
      "loss: 0.24549946188926697\n",
      "loss: 0.020078619942069054\n",
      "loss: 0.015486206859350204\n",
      "loss: 0.004244740586727858\n",
      "loss: 0.14695529639720917\n",
      "loss: 0.17625334858894348\n",
      "loss: 0.02716582827270031\n",
      "loss: 0.06824908405542374\n",
      "loss: 0.45566269755363464\n",
      "loss: 0.16328065097332\n",
      "loss: 0.2411796748638153\n",
      "loss: 0.01888386160135269\n",
      "loss: 0.014524225145578384\n",
      "loss: 0.003880031406879425\n",
      "loss: 0.14435029029846191\n",
      "loss: 0.1706603616476059\n",
      "loss: 0.025473257526755333\n",
      "loss: 0.06499969959259033\n",
      "loss: 0.451116144657135\n",
      "loss: 0.15844100713729858\n",
      "loss: 0.950714111328125\n",
      "loss: 1.3526771068572998\n",
      "loss: 1.3017147779464722\n",
      "loss: 1.4973429441452026\n",
      "loss: 0.7521244883537292\n",
      "loss: 0.9219249486923218\n",
      "loss: 1.0055519342422485\n",
      "loss: 0.8684118986129761\n",
      "loss: 0.6860248446464539\n",
      "loss: 0.7188451886177063\n",
      "loss: 0.8018603324890137\n",
      "loss: 0.7602722644805908\n",
      "loss: 0.7045583724975586\n",
      "loss: 0.6910821199417114\n",
      "loss: 0.5930578112602234\n",
      "loss: 0.6593966484069824\n",
      "loss: 0.5408761501312256\n",
      "loss: 0.5514020323753357\n",
      "loss: 0.5996445417404175\n",
      "loss: 0.5343215465545654\n",
      "loss: 0.7038509845733643\n",
      "loss: 0.4682852625846863\n",
      "loss: 0.4211878478527069\n",
      "loss: 0.34894904494285583\n",
      "loss: 0.4978666305541992\n",
      "loss: 0.5135741233825684\n",
      "loss: 0.3304179310798645\n",
      "loss: 0.38922423124313354\n",
      "loss: 0.5461198091506958\n",
      "loss: 0.4291800558567047\n",
      "loss: 0.6376785039901733\n",
      "loss: 0.31927573680877686\n",
      "loss: 0.28038257360458374\n",
      "loss: 0.20129477977752686\n",
      "loss: 0.4356841742992401\n",
      "loss: 0.425062894821167\n",
      "loss: 0.22485901415348053\n",
      "loss: 0.2968968451023102\n",
      "loss: 0.5102174878120422\n",
      "loss: 0.3628426790237427\n",
      "loss: 0.589127779006958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.23431549966335297\n",
      "loss: 0.2018054723739624\n",
      "loss: 0.12880946695804596\n",
      "loss: 0.391317218542099\n",
      "loss: 0.3662649691104889\n",
      "loss: 0.16519318521022797\n",
      "loss: 0.2388055920600891\n",
      "loss: 0.4844856858253479\n",
      "loss: 0.3173353672027588\n",
      "loss: 0.5510944128036499\n",
      "loss: 0.181053027510643\n",
      "loss: 0.15344253182411194\n",
      "loss: 0.08880109339952469\n",
      "loss: 0.35762491822242737\n",
      "loss: 0.3244261145591736\n",
      "loss: 0.1281094253063202\n",
      "loss: 0.19938427209854126\n",
      "loss: 0.4651423692703247\n",
      "loss: 0.2841583490371704\n",
      "loss: 0.5199185609817505\n",
      "loss: 0.14519762992858887\n",
      "loss: 0.12140116095542908\n",
      "loss: 0.0646371990442276\n",
      "loss: 0.3309001922607422\n",
      "loss: 0.2931038439273834\n",
      "loss: 0.10335256159305573\n",
      "loss: 0.17108257114887238\n",
      "loss: 0.4500948190689087\n",
      "loss: 0.2588655352592468\n",
      "loss: 0.4935378432273865\n",
      "loss: 0.11973349004983902\n",
      "loss: 0.09896144270896912\n",
      "loss: 0.04901004582643509\n",
      "loss: 0.3090217113494873\n",
      "loss: 0.2687421143054962\n",
      "loss: 0.08590196073055267\n",
      "loss: 0.1498747020959854\n",
      "loss: 0.43809089064598083\n",
      "loss: 0.23892094194889069\n",
      "loss: 0.4706929922103882\n",
      "loss: 0.10088973492383957\n",
      "loss: 0.08255892992019653\n",
      "loss: 0.038355752825737\n",
      "loss: 0.29067856073379517\n",
      "loss: 0.24922707676887512\n",
      "loss: 0.0730707198381424\n",
      "loss: 0.13344290852546692\n",
      "loss: 0.4283323585987091\n",
      "loss: 0.22277480363845825\n",
      "loss: 0.45056402683258057\n",
      "loss: 0.08648571372032166\n",
      "loss: 0.07015762478113174\n",
      "loss: 0.030782753601670265\n",
      "loss: 0.275011271238327\n",
      "loss: 0.23322580754756927\n",
      "loss: 0.06331437081098557\n",
      "loss: 0.12036725133657455\n",
      "loss: 0.42028412222862244\n",
      "loss: 0.20942631363868713\n",
      "loss: 0.4325888752937317\n",
      "loss: 0.07518413662910461\n",
      "loss: 0.06052234396338463\n",
      "loss: 0.025216002017259598\n",
      "loss: 0.2614295184612274\n",
      "loss: 0.21985533833503723\n",
      "loss: 0.05569130927324295\n",
      "loss: 0.10973328351974487\n",
      "loss: 0.41357237100601196\n",
      "loss: 0.19820049405097961\n",
      "loss: 0.4163655936717987\n",
      "loss: 0.06612446904182434\n",
      "loss: 0.05286653712391853\n",
      "loss: 0.02100943960249424\n",
      "loss: 0.24951183795928955\n",
      "loss: 0.2085079550743103\n",
      "loss: 0.04959983751177788\n",
      "loss: 0.10092778503894806\n",
      "loss: 0.40792760252952576\n",
      "loss: 0.18862442672252655\n",
      "loss: 0.4015970528125763\n",
      "loss: 0.05873050540685654\n",
      "loss: 0.04666834697127342\n",
      "loss: 0.017756599932909012\n",
      "loss: 0.23894819617271423\n",
      "loss: 0.19875068962574005\n",
      "loss: 0.04463939741253853\n",
      "loss: 0.09352423995733261\n",
      "loss: 0.4031490087509155\n",
      "loss: 0.1803572028875351\n",
      "loss: 0.38805750012397766\n",
      "loss: 0.05260390043258667\n",
      "loss: 0.0415700227022171\n",
      "loss: 0.015191687270998955\n",
      "loss: 0.22950416803359985\n",
      "loss: 0.1902674436569214\n",
      "loss: 0.04053502529859543\n",
      "loss: 0.08721853792667389\n",
      "loss: 0.39908456802368164\n",
      "loss: 0.173146590590477\n",
      "loss: 0.37557104229927063\n",
      "loss: 0.04746074229478836\n",
      "loss: 0.03731881082057953\n",
      "loss: 0.013135134242475033\n",
      "loss: 0.22099903225898743\n",
      "loss: 0.18282096087932587\n",
      "loss: 0.03709189593791962\n",
      "loss: 0.08178720623254776\n",
      "loss: 0.3956160247325897\n",
      "loss: 0.16680149734020233\n",
      "loss: 0.36399784684181213\n",
      "loss: 0.04309416562318802\n",
      "loss: 0.033731721341609955\n",
      "loss: 0.011462085880339146\n",
      "loss: 0.21329064667224884\n",
      "loss: 0.17623017728328705\n",
      "loss: 0.034168604761362076\n",
      "loss: 0.07706309109926224\n",
      "loss: 0.392650306224823\n",
      "loss: 0.16117477416992188\n",
      "loss: 0.3532247841358185\n",
      "loss: 0.039350058883428574\n",
      "loss: 0.030673593282699585\n",
      "loss: 0.010083560831844807\n",
      "loss: 0.20626525580883026\n",
      "loss: 0.17035414278507233\n",
      "loss: 0.03166065737605095\n",
      "loss: 0.07291869819164276\n",
      "loss: 0.3901125490665436\n",
      "loss: 0.15615107119083405\n",
      "loss: 0.3431593179702759\n",
      "loss: 0.036111507564783096\n",
      "loss: 0.028042398393154144\n",
      "loss: 0.008935016579926014\n",
      "loss: 0.199831023812294\n",
      "loss: 0.16508139669895172\n",
      "loss: 0.029488952830433846\n",
      "loss: 0.0692550539970398\n",
      "loss: 0.38794222474098206\n",
      "loss: 0.15163826942443848\n",
      "loss: 0.33372360467910767\n",
      "loss: 0.03328837454319\n",
      "loss: 0.025759994983673096\n",
      "loss: 0.007968499325215816\n",
      "loss: 0.19391204416751862\n",
      "loss: 0.16032274067401886\n",
      "loss: 0.02759280428290367\n",
      "loss: 0.06599431484937668\n",
      "loss: 0.38608962297439575\n",
      "loss: 0.14756283164024353\n",
      "loss: 0.3248531222343445\n",
      "loss: 0.030810292810201645\n",
      "loss: 0.02376578375697136\n",
      "loss: 0.00714782252907753\n",
      "loss: 0.18844594061374664\n",
      "loss: 0.15600600838661194\n",
      "loss: 0.025925079360604286\n",
      "loss: 0.06307455897331238\n",
      "loss: 0.38451364636421204\n",
      "loss: 0.143864244222641\n",
      "loss: 0.31649234890937805\n",
      "loss: 0.02862129732966423\n",
      "loss: 0.022011803463101387\n",
      "loss: 0.0064453925006091595\n",
      "loss: 0.1833798587322235\n",
      "loss: 0.15207181870937347\n",
      "loss: 0.024448299780488014\n",
      "loss: 0.06044553592801094\n",
      "loss: 0.38317933678627014\n",
      "loss: 0.14049287140369415\n",
      "loss: 0.30859386920928955\n",
      "loss: 0.026676714420318604\n",
      "loss: 0.020459933206439018\n",
      "loss: 0.005839856341481209\n",
      "loss: 0.17866958677768707\n",
      "loss: 0.14847084879875183\n",
      "loss: 0.023133017122745514\n",
      "loss: 0.05806664004921913\n",
      "loss: 0.3820578157901764\n",
      "loss: 0.13740761578083038\n",
      "loss: 0.30111685395240784\n",
      "loss: 0.02494029700756073\n",
      "loss: 0.01907946728169918\n",
      "loss: 0.00531425978988409\n",
      "loss: 0.17427735030651093\n",
      "loss: 0.14516276121139526\n",
      "loss: 0.021954957395792007\n",
      "loss: 0.055904142558574677\n",
      "loss: 0.3811239004135132\n",
      "loss: 0.13457375764846802\n",
      "loss: 0.29402533173561096\n",
      "loss: 0.02338247559964657\n",
      "loss: 0.01784536801278591\n",
      "loss: 0.004855420906096697\n",
      "loss: 0.17017033696174622\n",
      "loss: 0.14211268723011017\n",
      "loss: 0.020894644781947136\n",
      "loss: 0.05393011495471001\n",
      "loss: 0.3803565204143524\n",
      "loss: 0.1319621205329895\n",
      "loss: 0.28728798031806946\n",
      "loss: 0.021978840231895447\n",
      "loss: 0.016737094148993492\n",
      "loss: 0.004452589899301529\n",
      "loss: 0.16632090508937836\n",
      "loss: 0.13929139077663422\n",
      "loss: 0.019935719668865204\n",
      "loss: 0.05212140083312988\n",
      "loss: 0.3797372281551361\n",
      "loss: 0.12954793870449066\n",
      "loss: 0.7197962403297424\n",
      "loss: 1.1505390405654907\n",
      "loss: 1.05308198928833\n",
      "loss: 1.2345243692398071\n",
      "loss: 0.4601794183254242\n",
      "loss: 1.3561701774597168\n",
      "loss: 1.30247962474823\n",
      "loss: 1.0604711771011353\n",
      "loss: 0.9046239852905273\n",
      "loss: 0.8323295712471008\n",
      "loss: 0.6116335391998291\n",
      "loss: 0.6783829927444458\n",
      "loss: 0.6500447392463684\n",
      "loss: 0.4736303687095642\n",
      "loss: 0.6815823316574097\n",
      "loss: 0.7100155353546143\n",
      "loss: 0.5549976825714111\n",
      "loss: 0.5040604472160339\n",
      "loss: 0.7008476853370667\n",
      "loss: 0.5152897834777832\n",
      "loss: 0.49121353030204773\n",
      "loss: 0.26594865322113037\n",
      "loss: 0.24531160295009613\n",
      "loss: 0.13185752928256989\n",
      "loss: 0.49340105056762695\n",
      "loss: 0.45367759466171265\n",
      "loss: 0.23408754169940948\n",
      "loss: 0.28230392932891846\n",
      "loss: 0.5931007862091064\n",
      "loss: 0.3635132312774658\n",
      "loss: 0.4351098835468292\n",
      "loss: 0.14916682243347168\n",
      "loss: 0.13275431096553802\n",
      "loss: 0.05955301597714424\n",
      "loss: 0.39845579862594604\n",
      "loss: 0.34299197793006897\n",
      "loss: 0.13310818374156952\n",
      "loss: 0.19262422621250153\n",
      "loss: 0.5344725847244263\n",
      "loss: 0.28827783465385437\n",
      "loss: 0.39855557680130005\n",
      "loss: 0.0990438312292099\n",
      "loss: 0.08566141128540039\n",
      "loss: 0.033853981643915176\n",
      "loss: 0.3395966589450836\n",
      "loss: 0.28211209177970886\n",
      "loss: 0.08886734396219254\n",
      "loss: 0.14590933918952942\n",
      "loss: 0.49756819009780884\n",
      "loss: 0.24333587288856506\n",
      "loss: 0.3708896338939667\n",
      "loss: 0.07211029529571533\n",
      "loss: 0.06097425892949104\n",
      "loss: 0.021846510469913483\n",
      "loss: 0.29878583550453186\n",
      "loss: 0.24350634217262268\n",
      "loss: 0.06520809978246689\n",
      "loss: 0.11767178773880005\n",
      "loss: 0.4723179340362549\n",
      "loss: 0.21340271830558777\n",
      "loss: 0.6164700388908386\n",
      "loss: 1.0795117616653442\n",
      "loss: 1.0373252630233765\n",
      "loss: 1.0504491329193115\n",
      "loss: 0.5922062397003174\n",
      "loss: 1.2843294143676758\n",
      "loss: 1.275566816329956\n",
      "loss: 0.9938696622848511\n",
      "loss: 0.9490854740142822\n",
      "loss: 0.8215089440345764\n",
      "loss: 0.4577710032463074\n",
      "loss: 0.3191625475883484\n",
      "loss: 0.2935941815376282\n",
      "loss: 0.20155198872089386\n",
      "loss: 0.40676993131637573\n",
      "loss: 0.7033179402351379\n",
      "loss: 0.43442195653915405\n",
      "loss: 0.4621064066886902\n",
      "loss: 0.7392733097076416\n",
      "loss: 0.5085229277610779\n",
      "loss: 0.4007081687450409\n",
      "loss: 0.15663287043571472\n",
      "loss: 0.1385614424943924\n",
      "loss: 0.0735340490937233\n",
      "loss: 0.3286297023296356\n",
      "loss: 0.4763924479484558\n",
      "loss: 0.20748268067836761\n",
      "loss: 0.27916887402534485\n",
      "loss: 0.6354038715362549\n",
      "loss: 0.3738217353820801\n",
      "loss: 0.368099182844162\n",
      "loss: 0.0985972136259079\n",
      "loss: 0.08460533618927002\n",
      "loss: 0.0378846600651741\n",
      "loss: 0.28375083208084106\n",
      "loss: 0.36607301235198975\n",
      "loss: 0.1245650053024292\n",
      "loss: 0.19577674567699432\n",
      "loss: 0.5738608241081238\n",
      "loss: 0.30052539706230164\n",
      "loss: 0.3443336486816406\n",
      "loss: 0.07004082202911377\n",
      "loss: 0.058725133538246155\n",
      "loss: 0.02319999784231186\n",
      "loss: 0.25339698791503906\n",
      "loss: 0.30181002616882324\n",
      "loss: 0.08529789745807648\n",
      "loss: 0.14975513517856598\n",
      "loss: 0.5331590175628662\n",
      "loss: 0.2546473443508148\n",
      "loss: 0.3251408636569977\n",
      "loss: 0.05335775762796402\n",
      "loss: 0.04393337294459343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.015717172995209694\n",
      "loss: 0.2309563308954239\n",
      "loss: 0.259861022233963\n",
      "loss: 0.06339800357818604\n",
      "loss: 0.12109580636024475\n",
      "loss: 0.5043621063232422\n",
      "loss: 0.22328808903694153\n",
      "loss: 0.3088271915912628\n",
      "loss: 0.04253973439335823\n",
      "loss: 0.034518372267484665\n",
      "loss: 0.011373762041330338\n",
      "loss: 0.21343141794204712\n",
      "loss: 0.2303461879491806\n",
      "loss: 0.049797095358371735\n",
      "loss: 0.10173745453357697\n",
      "loss: 0.4830661714076996\n",
      "loss: 0.2005300372838974\n",
      "loss: 0.2945501208305359\n",
      "loss: 0.0350186824798584\n",
      "loss: 0.0280759297311306\n",
      "loss: 0.008622280322015285\n",
      "loss: 0.19923068583011627\n",
      "loss: 0.20846125483512878\n",
      "loss: 0.04069114476442337\n",
      "loss: 0.08788128942251205\n",
      "loss: 0.4668295085430145\n",
      "loss: 0.18328464031219482\n",
      "loss: 0.2818235158920288\n",
      "loss: 0.029523085802793503\n",
      "loss: 0.023432692512869835\n",
      "loss: 0.006766510661691427\n",
      "loss: 0.18741463124752045\n",
      "loss: 0.1915939599275589\n",
      "loss: 0.0342479832470417\n",
      "loss: 0.07752441614866257\n",
      "loss: 0.4541797637939453\n",
      "loss: 0.16978298127651215\n",
      "loss: 0.2703355550765991\n",
      "loss: 0.02535470761358738\n",
      "loss: 0.019952865317463875\n",
      "loss: 0.005454428028315306\n",
      "loss: 0.17738428711891174\n",
      "loss: 0.17820340394973755\n",
      "loss: 0.0294925756752491\n",
      "loss: 0.06951916217803955\n",
      "loss: 0.44417107105255127\n",
      "loss: 0.15893957018852234\n",
      "loss: 1.0237629413604736\n",
      "loss: 1.4869135618209839\n",
      "loss: 1.4446182250976562\n",
      "loss: 1.1588187217712402\n",
      "loss: 1.2014597654342651\n",
      "loss: 0.5537075996398926\n",
      "loss: 0.5225651264190674\n",
      "loss: 0.4298955202102661\n",
      "loss: 0.5293928384780884\n",
      "loss: 0.4190243184566498\n",
      "loss: 0.7428975701332092\n",
      "loss: 0.5302536487579346\n",
      "loss: 0.4929561913013458\n",
      "loss: 0.27544477581977844\n",
      "loss: 0.7881909608840942\n",
      "loss: 0.34622156620025635\n",
      "loss: 0.20014530420303345\n",
      "loss: 0.22895005345344543\n",
      "loss: 0.457415908575058\n",
      "loss: 0.29087913036346436\n",
      "loss: 0.6116528511047363\n",
      "loss: 0.26022830605506897\n",
      "loss: 0.23394066095352173\n",
      "loss: 0.10704466700553894\n",
      "loss: 0.5911911129951477\n",
      "loss: 0.26750999689102173\n",
      "loss: 0.11247509717941284\n",
      "loss: 0.15700241923332214\n",
      "loss: 0.4230908751487732\n",
      "loss: 0.23350626230239868\n",
      "loss: 0.5353407859802246\n",
      "loss: 0.15828248858451843\n",
      "loss: 0.1381129026412964\n",
      "loss: 0.055597856640815735\n",
      "loss: 0.47713422775268555\n",
      "loss: 0.2260250300168991\n",
      "loss: 0.07586532086133957\n",
      "loss: 0.12106707692146301\n",
      "loss: 0.4033491015434265\n",
      "loss: 0.20071806013584137\n",
      "loss: 0.4824504852294922\n",
      "loss: 0.10849987715482712\n",
      "loss: 0.09238818287849426\n",
      "loss: 0.03379460424184799\n",
      "loss: 0.40249189734458923\n",
      "loss: 0.20003601908683777\n",
      "loss: 0.056538570672273636\n",
      "loss: 0.0995897427201271\n",
      "loss: 0.3908654451370239\n",
      "loss: 0.17929348349571228\n",
      "loss: 0.4421440660953522\n",
      "loss: 0.08005045354366302\n",
      "loss: 0.06682480126619339\n",
      "loss: 0.022618191316723824\n",
      "loss: 0.3497123122215271\n",
      "loss: 0.182038813829422\n",
      "loss: 0.04482395946979523\n",
      "loss: 0.0853007584810257\n",
      "loss: 0.3825802505016327\n",
      "loss: 0.16410093009471893\n",
      "loss: 0.4096835255622864\n",
      "loss: 0.06205550581216812\n",
      "loss: 0.05096760392189026\n",
      "loss: 0.016150616109371185\n",
      "loss: 0.3103601634502411\n",
      "loss: 0.16874322295188904\n",
      "loss: 0.03705447167158127\n",
      "loss: 0.07509803771972656\n",
      "loss: 0.37696385383605957\n",
      "loss: 0.1527210772037506\n",
      "loss: 0.3826042711734772\n",
      "loss: 0.04984571039676666\n",
      "loss: 0.04039028659462929\n",
      "loss: 0.012081896886229515\n",
      "loss: 0.2798653244972229\n",
      "loss: 0.15846797823905945\n",
      "loss: 0.031566161662340164\n",
      "loss: 0.06744030863046646\n",
      "loss: 0.37315574288368225\n",
      "loss: 0.1438562572002411\n",
      "loss: 0.35946333408355713\n",
      "loss: 0.04112523794174194\n",
      "loss: 0.032947786152362823\n",
      "loss: 0.009361197240650654\n",
      "loss: 0.25552991032600403\n",
      "loss: 0.15025892853736877\n",
      "loss: 0.027504514902830124\n",
      "loss: 0.06147518381476402\n",
      "loss: 0.37062978744506836\n",
      "loss: 0.13674409687519073\n",
      "loss: 0.3393391966819763\n",
      "loss: 0.03464880958199501\n",
      "loss: 0.027492383494973183\n",
      "loss: 0.007455355022102594\n",
      "loss: 0.23565569519996643\n",
      "loss: 0.14353103935718536\n",
      "loss: 0.02438911609351635\n",
      "loss: 0.05669336020946503\n",
      "loss: 0.36904364824295044\n",
      "loss: 0.1309053599834442\n",
      "loss: 0.32160595059394836\n",
      "loss: 0.029688730835914612\n",
      "loss: 0.023362280800938606\n",
      "loss: 0.006070404779165983\n",
      "loss: 0.21911828219890594\n",
      "loss: 0.13790427148342133\n",
      "loss: 0.021930890157818794\n",
      "loss: 0.05277140066027641\n",
      "loss: 0.36816421151161194\n",
      "loss: 0.1260225623846054\n",
      "loss: 0.30581679940223694\n",
      "loss: 0.025794245302677155\n",
      "loss: 0.02015269361436367\n",
      "loss: 0.005033743102103472\n",
      "loss: 0.20514363050460815\n",
      "loss: 0.13312003016471863\n",
      "loss: 0.01994599588215351\n",
      "loss: 0.04949411377310753\n",
      "loss: 0.36782610416412354\n",
      "loss: 0.12187673151493073\n",
      "loss: 0.2916408181190491\n",
      "loss: 0.022673185914754868\n",
      "loss: 0.017603959888219833\n",
      "loss: 0.004238515626639128\n",
      "loss: 0.193180114030838\n",
      "loss: 0.1289960741996765\n",
      "loss: 0.018312696367502213\n",
      "loss: 0.04671306163072586\n",
      "loss: 0.3679090142250061\n",
      "loss: 0.1183115616440773\n",
      "loss: 0.2788255214691162\n",
      "loss: 0.02012842893600464\n",
      "loss: 0.01554286852478981\n",
      "loss: 0.0036157690919935703\n",
      "loss: 0.18282438814640045\n",
      "loss: 0.12539979815483093\n",
      "loss: 0.016946768388152122\n",
      "loss: 0.04432171210646629\n",
      "loss: 0.36832278966903687\n",
      "loss: 0.11521221697330475\n",
      "loss: 0.2671731114387512\n",
      "loss: 0.018022866919636726\n",
      "loss: 0.01385007705539465\n",
      "loss: 0.0031194414477795362\n",
      "loss: 0.1737741380929947\n",
      "loss: 0.12223230302333832\n",
      "loss: 0.015788806602358818\n",
      "loss: 0.04224254563450813\n",
      "loss: 0.36899858713150024\n",
      "loss: 0.11249217391014099\n",
      "loss: 1.060068130493164\n",
      "loss: 0.6009341478347778\n",
      "loss: 0.45393383502960205\n",
      "loss: 0.5137285590171814\n",
      "loss: 0.28935399651527405\n",
      "loss: 0.5222219228744507\n",
      "loss: 0.2575169503688812\n",
      "loss: 0.3534505367279053\n",
      "loss: 0.4173640310764313\n",
      "loss: 0.342836856842041\n",
      "loss: 0.8467385768890381\n",
      "loss: 0.29371026158332825\n",
      "loss: 0.21605098247528076\n",
      "loss: 0.1887950301170349\n",
      "loss: 0.24652299284934998\n",
      "loss: 0.37813588976860046\n",
      "loss: 0.13801518082618713\n",
      "loss: 0.22790856659412384\n",
      "loss: 0.3871912658214569\n",
      "loss: 0.2668512165546417\n",
      "loss: 0.7209237217903137\n",
      "loss: 0.17753490805625916\n",
      "loss: 0.12901423871517181\n",
      "loss: 0.0918005183339119\n",
      "loss: 0.2209623008966446\n",
      "loss: 0.3036176562309265\n",
      "loss: 0.08980905264616013\n",
      "loss: 0.1670912653207779\n",
      "loss: 0.3704330325126648\n",
      "loss: 0.22415584325790405\n",
      "loss: 0.6344955563545227\n",
      "loss: 0.12093744426965714\n",
      "loss: 0.08729495853185654\n",
      "loss: 0.05284027010202408\n",
      "loss: 0.20298637449741364\n",
      "loss: 0.2582937479019165\n",
      "loss: 0.06515898555517197\n",
      "loss: 0.13200640678405762\n",
      "loss: 0.3603115677833557\n",
      "loss: 0.19673863053321838\n",
      "loss: 0.5697627067565918\n",
      "loss: 0.08871902525424957\n",
      "loss: 0.0637793019413948\n",
      "loss: 0.03384096920490265\n",
      "loss: 0.18926307559013367\n",
      "loss: 0.22779935598373413\n",
      "loss: 0.05060802400112152\n",
      "loss: 0.10942528396844864\n",
      "loss: 0.354001522064209\n",
      "loss: 0.1776060163974762\n",
      "loss: 0.5186370015144348\n",
      "loss: 0.06842535734176636\n",
      "loss: 0.04906601086258888\n",
      "loss: 0.02330535650253296\n",
      "loss: 0.17826102674007416\n",
      "loss: 0.2058507651090622\n",
      "loss: 0.04116691276431084\n",
      "loss: 0.09377415478229523\n",
      "loss: 0.3500954508781433\n",
      "loss: 0.16348060965538025\n",
      "loss: 0.47680848836898804\n",
      "loss: 0.054711539298295975\n",
      "loss: 0.03917090222239494\n",
      "loss: 0.016913123428821564\n",
      "loss: 0.16914691030979156\n",
      "loss: 0.18927782773971558\n",
      "loss: 0.03461931273341179\n",
      "loss: 0.08233322948217392\n",
      "loss: 0.34780353307724\n",
      "loss: 0.15261955559253693\n",
      "loss: 0.44172215461730957\n",
      "loss: 0.04495406523346901\n",
      "loss: 0.0321555994451046\n",
      "loss: 0.012769523076713085\n",
      "loss: 0.16141647100448608\n",
      "loss: 0.17630966007709503\n",
      "loss: 0.029848631471395493\n",
      "loss: 0.07362829893827438\n",
      "loss: 0.34664449095726013\n",
      "loss: 0.14400775730609894\n",
      "loss: 0.41174036264419556\n",
      "loss: 0.037732817232608795\n",
      "loss: 0.026977943256497383\n",
      "loss: 0.009944187477231026\n",
      "loss: 0.15474151074886322\n",
      "loss: 0.1658785194158554\n",
      "loss: 0.026238318532705307\n",
      "loss: 0.06679555028676987\n",
      "loss: 0.3463062644004822\n",
      "loss: 0.13701331615447998\n",
      "loss: 0.3857510983943939\n",
      "loss: 0.03222023323178291\n",
      "loss: 0.02303365431725979\n",
      "loss: 0.007939119823276997\n",
      "loss: 0.1488959640264511\n",
      "loss: 0.15730197727680206\n",
      "loss: 0.023422563448548317\n",
      "loss: 0.06129702925682068\n",
      "loss: 0.3465762138366699\n",
      "loss: 0.1312214732170105\n",
      "loss: 0.3629657030105591\n",
      "loss: 0.027905095368623734\n",
      "loss: 0.019951162859797478\n",
      "loss: 0.006469568703323603\n",
      "loss: 0.1437176614999771\n",
      "loss: 0.15012268722057343\n",
      "loss: 0.02117221988737583\n",
      "loss: 0.056780796498060226\n",
      "loss: 0.3473042845726013\n",
      "loss: 0.12634815275669098\n",
      "loss: 0.3428022563457489\n",
      "loss: 0.02445654571056366\n",
      "loss: 0.017490830272436142\n",
      "loss: 0.005363337229937315\n",
      "loss: 0.13908682763576508\n",
      "loss: 0.14402318000793457\n",
      "loss: 0.019336970522999763\n",
      "loss: 0.053007811307907104\n",
      "loss: 0.348381906747818\n",
      "loss: 0.12219249457120895\n",
      "loss: 0.32482045888900757\n",
      "loss: 0.021652191877365112\n",
      "loss: 0.015491897240281105\n",
      "loss: 0.004511683713644743\n",
      "loss: 0.1349121779203415\n",
      "loss: 0.13877533376216888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.017814483493566513\n",
      "loss: 0.0498102530837059\n",
      "loss: 0.34972721338272095\n",
      "loss: 0.11860794574022293\n",
      "loss: 0.3086785078048706\n",
      "loss: 0.019337639212608337\n",
      "loss: 0.013843188062310219\n",
      "loss: 0.0038432925939559937\n",
      "loss: 0.13112306594848633\n",
      "loss: 0.13421091437339783\n",
      "loss: 0.01653316058218479\n",
      "loss: 0.04706643149256706\n",
      "loss: 0.35127872228622437\n",
      "loss: 0.11548520624637604\n",
      "loss: 0.29410579800605774\n",
      "loss: 0.017402639612555504\n",
      "loss: 0.012465543113648891\n",
      "loss: 0.003310132771730423\n",
      "loss: 0.1276632398366928\n",
      "loss: 0.13020357489585876\n",
      "loss: 0.01544104889035225\n",
      "loss: 0.044686950743198395\n",
      "loss: 0.35298848152160645\n",
      "loss: 0.11274106800556183\n",
      "loss: 0.2808839976787567\n",
      "loss: 0.015766825526952744\n",
      "loss: 0.011301240883767605\n",
      "loss: 0.0028785730246454477\n",
      "loss: 0.12448777258396149\n",
      "loss: 0.1266563981771469\n",
      "loss: 0.014500034973025322\n",
      "loss: 0.04260367155075073\n",
      "loss: 0.35481905937194824\n",
      "loss: 0.11031121760606766\n",
      "loss: 0.2688359320163727\n",
      "loss: 0.014370324090123177\n",
      "loss: 0.010307441465556622\n",
      "loss: 0.0025248187594115734\n",
      "loss: 0.1215595230460167\n",
      "loss: 0.12349343299865723\n",
      "loss: 0.013681354001164436\n",
      "loss: 0.040764957666397095\n",
      "loss: 0.35674044489860535\n",
      "loss: 0.10814470797777176\n",
      "loss: 0.2578141689300537\n",
      "loss: 0.013167626224458218\n",
      "loss: 0.009451629593968391\n",
      "loss: 0.0022315229289233685\n",
      "loss: 0.11884850263595581\n",
      "loss: 0.12065456062555313\n",
      "loss: 0.01296309009194374\n",
      "loss: 0.039129871875047684\n",
      "loss: 0.35872891545295715\n",
      "loss: 0.10620079189538956\n",
      "loss: 0.24769584834575653\n",
      "loss: 0.012123645283281803\n",
      "loss: 0.00870873685926199\n",
      "loss: 0.0019859708845615387\n",
      "loss: 0.1163291186094284\n",
      "loss: 0.11809179931879044\n",
      "loss: 0.01232788898050785\n",
      "loss: 0.037666402757167816\n",
      "loss: 0.3607652187347412\n",
      "loss: 0.10444722324609756\n",
      "loss: 0.238377183675766\n",
      "loss: 0.011211182922124863\n",
      "loss: 0.008059286512434483\n",
      "loss: 0.0017784546362236142\n",
      "loss: 0.11398018896579742\n",
      "loss: 0.1157657653093338\n",
      "loss: 0.011762685142457485\n",
      "loss: 0.036348775029182434\n",
      "loss: 0.3628336787223816\n",
      "loss: 0.10285697132349014\n",
      "loss: 1.0493855476379395\n",
      "loss: 0.8734424114227295\n",
      "loss: 0.6654027104377747\n",
      "loss: 1.0195626020431519\n",
      "loss: 0.2080690562725067\n",
      "loss: 1.0084842443466187\n",
      "loss: 0.6744369864463806\n",
      "loss: 0.7454200983047485\n",
      "loss: 0.5936882495880127\n",
      "loss: 0.5806765556335449\n",
      "loss: 0.7932332754135132\n",
      "loss: 0.3356969654560089\n",
      "loss: 0.24366503953933716\n",
      "loss: 0.27975696325302124\n",
      "loss: 0.1762879639863968\n",
      "loss: 0.6221076846122742\n",
      "loss: 0.27292975783348083\n",
      "loss: 0.3989401161670685\n",
      "loss: 0.506152868270874\n",
      "loss: 0.3967788517475128\n",
      "loss: 0.6651690006256104\n",
      "loss: 0.18176569044589996\n",
      "loss: 0.12981000542640686\n",
      "loss: 0.11370474100112915\n",
      "loss: 0.16267886757850647\n",
      "loss: 0.4490116834640503\n",
      "loss: 0.1489756554365158\n",
      "loss: 0.2589612901210785\n",
      "loss: 0.4598953127861023\n",
      "loss: 0.3073824346065521\n",
      "loss: 0.5840304493904114\n",
      "loss: 0.11768271774053574\n",
      "loss: 0.08344961702823639\n",
      "loss: 0.05937507748603821\n",
      "loss: 0.15403328835964203\n",
      "loss: 0.3554816246032715\n",
      "loss: 0.09674305468797684\n",
      "loss: 0.1884276568889618\n",
      "loss: 0.4318092167377472\n",
      "loss: 0.2553372383117676\n",
      "loss: 0.5255014300346375\n",
      "loss: 0.08417380601167679\n",
      "loss: 0.05948064848780632\n",
      "loss: 0.035939667373895645\n",
      "loss: 0.1474798023700714\n",
      "loss: 0.2978023886680603\n",
      "loss: 0.06968064606189728\n",
      "loss: 0.1472363919019699\n",
      "loss: 0.4133304953575134\n",
      "loss: 0.2214318960905075\n",
      "loss: 0.4800785779953003\n",
      "loss: 0.06407914310693741\n",
      "loss: 0.0452042892575264\n",
      "loss: 0.023902151733636856\n",
      "loss: 0.14210361242294312\n",
      "loss: 0.25889456272125244\n",
      "loss: 0.053668513894081116\n",
      "loss: 0.12067809700965881\n",
      "loss: 0.4005811810493469\n",
      "loss: 0.19763687252998352\n",
      "loss: 0.44318562746047974\n",
      "loss: 0.05089905112981796\n",
      "loss: 0.03588208556175232\n",
      "loss: 0.016957074403762817\n",
      "loss: 0.1375080794095993\n",
      "loss: 0.23094461858272552\n",
      "loss: 0.04329948127269745\n",
      "loss: 0.10231970250606537\n",
      "loss: 0.39153963327407837\n",
      "loss: 0.18003994226455688\n",
      "loss: 0.4122943580150604\n",
      "loss: 0.04169492423534393\n",
      "loss: 0.02939220890402794\n",
      "loss: 0.012607923708856106\n",
      "loss: 0.13348078727722168\n",
      "loss: 0.20992235839366913\n",
      "loss: 0.03613598272204399\n",
      "loss: 0.08896245807409286\n",
      "loss: 0.3850407600402832\n",
      "loss: 0.1665143519639969\n",
      "loss: 0.38586515188217163\n",
      "loss: 0.034963928163051605\n",
      "loss: 0.02465648204088211\n",
      "loss: 0.009714188054203987\n",
      "loss: 0.12989158928394318\n",
      "loss: 0.19355039298534393\n",
      "loss: 0.030940616503357887\n",
      "loss: 0.07885625213384628\n",
      "loss: 0.38036200404167175\n",
      "loss: 0.15580560266971588\n",
      "loss: 0.36288943886756897\n",
      "loss: 0.029864413663744926\n",
      "loss: 0.021074146032333374\n",
      "loss: 0.007697148248553276\n",
      "loss: 0.12665288150310516\n",
      "loss: 0.18044885993003845\n",
      "loss: 0.027028340846300125\n",
      "loss: 0.07097065448760986\n",
      "loss: 0.37702834606170654\n",
      "loss: 0.14712631702423096\n",
      "loss: 0.34266993403434753\n",
      "loss: 0.02589162066578865\n",
      "loss: 0.01828622817993164\n",
      "loss: 0.006238388363271952\n",
      "loss: 0.1237025111913681\n",
      "loss: 0.16973325610160828\n",
      "loss: 0.023992443457245827\n",
      "loss: 0.06466339528560638\n",
      "loss: 0.37471383810043335\n",
      "loss: 0.13995720446109772\n",
      "loss: 0.32470187544822693\n",
      "loss: 0.022725746035575867\n",
      "loss: 0.0160660557448864\n",
      "loss: 0.005151404533535242\n",
      "loss: 0.1209939569234848\n",
      "loss: 0.1608113944530487\n",
      "loss: 0.021578067913651466\n",
      "loss: 0.05951423570513725\n",
      "loss: 0.3731869161128998\n",
      "loss: 0.13394182920455933\n",
      "loss: 0.308607816696167\n",
      "loss: 0.020155249163508415\n",
      "loss: 0.014264148660004139\n",
      "loss: 0.004321122542023659\n",
      "loss: 0.1184915155172348\n",
      "loss: 0.15327152609825134\n",
      "loss: 0.01961873285472393\n",
      "loss: 0.05523824691772461\n",
      "loss: 0.37227776646614075\n",
      "loss: 0.12882737815380096\n",
      "loss: 0.2940971553325653\n",
      "loss: 0.018035003915429115\n",
      "loss: 0.01277809590101242\n",
      "loss: 0.003673557424917817\n",
      "loss: 0.1161675676703453\n",
      "loss: 0.14681874215602875\n",
      "loss: 0.018001141026616096\n",
      "loss: 0.051635369658470154\n",
      "loss: 0.37185952067375183\n",
      "loss: 0.12442972511053085\n",
      "loss: 0.28094059228897095\n",
      "loss: 0.016262443736195564\n",
      "loss: 0.01153570506721735\n",
      "loss: 0.003159442450851202\n",
      "loss: 0.11399935930967331\n",
      "loss: 0.14123623073101044\n",
      "loss: 0.016645940020680428\n",
      "loss: 0.048561565577983856\n",
      "loss: 0.3718346059322357\n",
      "loss: 0.12061145901679993\n",
      "loss: 0.268954336643219\n",
      "loss: 0.014763318002223969\n",
      "loss: 0.010484752245247364\n",
      "loss: 0.002744862111285329\n",
      "loss: 0.11196864396333694\n",
      "loss: 0.13636085391044617\n",
      "loss: 0.015495953150093555\n",
      "loss: 0.04591052606701851\n",
      "loss: 0.37212711572647095\n",
      "loss: 0.11726737022399902\n",
      "loss: 0.2579883635044098\n",
      "loss: 0.013482452370226383\n",
      "loss: 0.00958654098212719\n",
      "loss: 0.002406091894954443\n",
      "loss: 0.11006013303995132\n",
      "loss: 0.1320677548646927\n",
      "loss: 0.014509408734738827\n",
      "loss: 0.04360221326351166\n",
      "loss: 0.372676819562912\n",
      "loss: 0.1143169179558754\n",
      "loss: 0.2479185312986374\n",
      "loss: 0.012378216721117496\n",
      "loss: 0.008811858482658863\n",
      "loss: 0.0021259121131151915\n",
      "loss: 0.10826099663972855\n",
      "loss: 0.1282598376274109\n",
      "loss: 0.013654767535626888\n",
      "loss: 0.041575491428375244\n",
      "loss: 0.37343618273735046\n",
      "loss: 0.11169581115245819\n",
      "loss: 0.23864085972309113\n",
      "loss: 0.011418616399168968\n",
      "loss: 0.008138364180922508\n",
      "loss: 0.0018917919369414449\n",
      "loss: 0.1065603718161583\n",
      "loss: 0.12485993653535843\n",
      "loss: 0.012907902710139751\n",
      "loss: 0.03978239372372627\n",
      "loss: 0.3743658661842346\n",
      "loss: 0.10935327410697937\n",
      "loss: 0.2300674319267273\n",
      "loss: 0.010578840039670467\n",
      "loss: 0.007548618130385876\n",
      "loss: 0.0016942659858614206\n",
      "loss: 0.1049492210149765\n",
      "loss: 0.12180671095848083\n",
      "loss: 0.01225016824901104\n",
      "loss: 0.038185566663742065\n",
      "loss: 0.3754343092441559\n",
      "loss: 0.10724814981222153\n",
      "loss: 0.22212325036525726\n",
      "loss: 0.009839143604040146\n",
      "loss: 0.00702878599986434\n",
      "loss: 0.0015261487569659948\n",
      "loss: 0.10341925173997879\n",
      "loss: 0.11905016750097275\n",
      "loss: 0.011666857637465\n",
      "loss: 0.03675483167171478\n",
      "loss: 0.37661564350128174\n",
      "loss: 0.1053471639752388\n",
      "loss: 0.21474388241767883\n",
      "loss: 0.009183845482766628\n",
      "loss: 0.0065680197440087795\n",
      "loss: 0.0013819941086694598\n",
      "loss: 0.1019638180732727\n",
      "loss: 0.11654937267303467\n",
      "loss: 0.011146447621285915\n",
      "loss: 0.03546563535928726\n",
      "loss: 0.3778878152370453\n",
      "loss: 0.10362225770950317\n",
      "loss: 0.20787332952022552\n",
      "loss: 0.00860015582293272\n",
      "loss: 0.006157241761684418\n",
      "loss: 0.0012575542787089944\n",
      "loss: 0.10057687759399414\n",
      "loss: 0.11427061259746552\n",
      "loss: 0.01067938283085823\n",
      "loss: 0.03429844230413437\n",
      "loss: 0.37923291325569153\n",
      "loss: 0.10205081105232239\n",
      "loss: 0.20146311819553375\n",
      "loss: 0.008077793754637241\n",
      "loss: 0.005789436399936676\n",
      "loss: 0.0011494206264615059\n",
      "loss: 0.09925293177366257\n",
      "loss: 0.11218586564064026\n",
      "loss: 0.01025800034403801\n",
      "loss: 0.03323673829436302\n",
      "loss: 0.3806360960006714\n",
      "loss: 0.10061358660459518\n",
      "loss: 0.19547058641910553\n",
      "loss: 0.0076081957668066025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.005458503030240536\n",
      "loss: 0.0010548430727794766\n",
      "loss: 0.09798747301101685\n",
      "loss: 0.11027141660451889\n",
      "loss: 0.009875982999801636\n",
      "loss: 0.03226707875728607\n",
      "loss: 0.38208428025245667\n",
      "loss: 0.09929433465003967\n",
      "loss: 0.40263864398002625\n",
      "loss: 0.44328439235687256\n",
      "loss: 0.32388731837272644\n",
      "loss: 0.1619294136762619\n",
      "loss: 0.25976473093032837\n",
      "loss: 1.2630318403244019\n",
      "loss: 0.39827761054039\n",
      "loss: 0.31231406331062317\n",
      "loss: 0.775634765625\n",
      "loss: 0.30138251185417175\n",
      "loss: 0.6959303021430969\n",
      "loss: 0.35988113284111023\n",
      "loss: 0.2505464553833008\n",
      "loss: 0.08513089269399643\n",
      "loss: 0.37655919790267944\n",
      "loss: 0.4725293815135956\n",
      "loss: 0.12475452572107315\n",
      "loss: 0.15595954656600952\n",
      "loss: 0.49071890115737915\n",
      "loss: 0.19355863332748413\n",
      "loss: 0.3721972405910492\n",
      "loss: 0.04444209486246109\n",
      "loss: 0.03368881344795227\n",
      "loss: 0.008573884144425392\n",
      "loss: 0.21310283243656158\n",
      "loss: 0.22606858611106873\n",
      "loss: 0.035458534955978394\n",
      "loss: 0.07184332609176636\n",
      "loss: 0.42218613624572754\n",
      "loss: 0.13110709190368652\n",
      "loss: 0.2908296287059784\n",
      "loss: 0.0198347270488739\n",
      "loss: 0.014796165749430656\n",
      "loss: 0.00311471801251173\n",
      "loss: 0.15832550823688507\n",
      "loss: 0.1687796264886856\n",
      "loss: 0.02005757950246334\n",
      "loss: 0.04911859706044197\n",
      "loss: 0.40264764428138733\n",
      "loss: 0.10794157534837723\n",
      "loss: 0.24166235327720642\n",
      "loss: 0.011850500479340553\n",
      "loss: 0.00875701941549778\n",
      "loss: 0.0016048905672505498\n",
      "loss: 0.12985703349113464\n",
      "loss: 0.14326603710651398\n",
      "loss: 0.014194379560649395\n",
      "loss: 0.03878982737660408\n",
      "loss: 0.397812157869339\n",
      "loss: 0.09605301916599274\n",
      "loss: 0.2075888216495514\n",
      "loss: 0.008131093345582485\n",
      "loss: 0.005979978013783693\n",
      "loss: 0.0009827673202380538\n",
      "loss: 0.11231109499931335\n",
      "loss: 0.1288689374923706\n",
      "loss: 0.011197681538760662\n",
      "loss: 0.03293263167142868\n",
      "loss: 0.3989240527153015\n",
      "loss: 0.08891548961400986\n",
      "loss: 0.8496484756469727\n",
      "loss: 0.44976139068603516\n",
      "loss: 0.2683178186416626\n",
      "loss: 0.2662329375743866\n",
      "loss: 0.10285822302103043\n",
      "loss: 1.136207103729248\n",
      "loss: 0.26954448223114014\n",
      "loss: 0.3045886754989624\n",
      "loss: 0.5845403075218201\n",
      "loss: 0.2700379192829132\n",
      "loss: 0.37661486864089966\n",
      "loss: 0.03524593263864517\n",
      "loss: 0.024029381573200226\n",
      "loss: 0.009306447580456734\n",
      "loss: 0.10131166130304337\n",
      "loss: 0.3260587453842163\n",
      "loss: 0.044863730669021606\n",
      "loss: 0.09808076918125153\n",
      "loss: 0.45877644419670105\n",
      "loss: 0.1548956334590912\n",
      "loss: 0.2902117967605591\n",
      "loss: 0.01632484421133995\n",
      "loss: 0.011320470832288265\n",
      "loss: 0.003005127888172865\n",
      "loss: 0.09724724292755127\n",
      "loss: 0.20672044157981873\n",
      "loss: 0.022256838157773018\n",
      "loss: 0.05850442871451378\n",
      "loss: 0.42249447107315063\n",
      "loss: 0.11887025088071823\n",
      "loss: 0.23964405059814453\n",
      "loss: 0.010134980082511902\n",
      "loss: 0.007113244850188494\n",
      "loss: 0.0015040019061416388\n",
      "loss: 0.09244192391633987\n",
      "loss: 0.16227279603481293\n",
      "loss: 0.014887332916259766\n",
      "loss: 0.04306317865848541\n",
      "loss: 0.4101918339729309\n",
      "loss: 0.10188006609678268\n",
      "loss: 0.2050441950559616\n",
      "loss: 0.0071589285507798195\n",
      "loss: 0.005073521286249161\n",
      "loss: 0.0009138586465269327\n",
      "loss: 0.08788740634918213\n",
      "loss: 0.13958774507045746\n",
      "loss: 0.011409995146095753\n",
      "loss: 0.03509362414479256\n",
      "loss: 0.40731510519981384\n",
      "loss: 0.09224313497543335\n",
      "loss: 0.17983666062355042\n",
      "loss: 0.005453409161418676\n",
      "loss: 0.003895648755133152\n",
      "loss: 0.0006213926244527102\n",
      "loss: 0.08377823233604431\n",
      "loss: 0.12604494392871857\n",
      "loss: 0.00943026877939701\n",
      "loss: 0.030303725972771645\n",
      "loss: 0.40871474146842957\n",
      "loss: 0.08615133911371231\n",
      "loss: 0.1607629507780075\n",
      "loss: 0.0043695527128875256\n",
      "loss: 0.003141743829473853\n",
      "loss: 0.0004551254096440971\n",
      "loss: 0.08013329654932022\n",
      "loss: 0.11714299023151398\n",
      "loss: 0.008166126906871796\n",
      "loss: 0.027132388204336166\n",
      "loss: 0.4121306240558624\n",
      "loss: 0.08200676739215851\n",
      "loss: 0.1459307074546814\n",
      "loss: 0.0036313822492957115\n",
      "loss: 0.0026250334922224283\n",
      "loss: 0.000351610011421144\n",
      "loss: 0.07692082226276398\n",
      "loss: 0.11088915914297104\n",
      "loss: 0.007293591741472483\n",
      "loss: 0.024886460974812508\n",
      "loss: 0.4164482057094574\n",
      "loss: 0.07902853935956955\n",
      "loss: 0.13414642214775085\n",
      "loss: 0.00310281990095973\n",
      "loss: 0.002252909354865551\n",
      "loss: 0.00028268518508411944\n",
      "loss: 0.07409503310918808\n",
      "loss: 0.10627264529466629\n",
      "loss: 0.006656759884208441\n",
      "loss: 0.023215308785438538\n",
      "loss: 0.4210822284221649\n",
      "loss: 0.07679440826177597\n",
      "loss: 0.12461773306131363\n",
      "loss: 0.002709838328883052\n",
      "loss: 0.001974743092432618\n",
      "loss: 0.0002344525564694777\n",
      "loss: 0.07160862535238266\n",
      "loss: 0.10273060202598572\n",
      "loss: 0.006171815097332001\n",
      "loss: 0.021923823282122612\n",
      "loss: 0.425716757774353\n",
      "loss: 0.07505910098552704\n",
      "loss: 1.0071039199829102\n",
      "loss: 0.4862111210823059\n",
      "loss: 0.30097001791000366\n",
      "loss: 0.14405657351016998\n",
      "loss: 0.2797046899795532\n",
      "loss: 0.5078346133232117\n",
      "loss: 0.1269824057817459\n",
      "loss: 0.16887424886226654\n",
      "loss: 0.44418221712112427\n",
      "loss: 0.19238433241844177\n",
      "loss: 0.45600834488868713\n",
      "loss: 0.05237675458192825\n",
      "loss: 0.03723331168293953\n",
      "loss: 0.011383227072656155\n",
      "loss: 0.183663010597229\n",
      "loss: 0.23505941033363342\n",
      "loss: 0.03623851016163826\n",
      "loss: 0.0759088546037674\n",
      "loss: 0.3950745463371277\n",
      "loss: 0.13048604130744934\n",
      "loss: 0.33468788862228394\n",
      "loss: 0.02241787128150463\n",
      "loss: 0.01606036350131035\n",
      "loss: 0.003818760858848691\n",
      "loss: 0.14494238793849945\n",
      "loss: 0.17336758971214294\n",
      "loss: 0.020503127947449684\n",
      "loss: 0.05121362954378128\n",
      "loss: 0.3841289281845093\n",
      "loss: 0.10769659280776978\n",
      "loss: 0.2682870328426361\n",
      "loss: 0.013068825006484985\n",
      "loss: 0.009396214038133621\n",
      "loss: 0.0018801471451297402\n",
      "loss: 0.1228574737906456\n",
      "loss: 0.1462360918521881\n",
      "loss: 0.0145069295540452\n",
      "loss: 0.040118928998708725\n",
      "loss: 0.3843034505844116\n",
      "loss: 0.09603037685155869\n",
      "loss: 0.22505083680152893\n",
      "loss: 0.008816609159111977\n",
      "loss: 0.006359970197081566\n",
      "loss: 0.0011168395867571235\n",
      "loss: 0.10841444134712219\n",
      "loss: 0.13102847337722778\n",
      "loss: 0.011438029818236828\n",
      "loss: 0.03387383371591568\n",
      "loss: 0.3887070417404175\n",
      "loss: 0.08902376890182495\n",
      "loss: 0.1945870965719223\n",
      "loss: 0.006489065941423178\n",
      "loss: 0.004696113523095846\n",
      "loss: 0.0007430691039189696\n",
      "loss: 0.09819359332323074\n",
      "loss: 0.12130708247423172\n",
      "loss: 0.009596651419997215\n",
      "loss: 0.029877925291657448\n",
      "loss: 0.3947649896144867\n",
      "loss: 0.08437865227460861\n",
      "loss: 0.17206743359565735\n",
      "loss: 0.005063217133283615\n",
      "loss: 0.00367541192099452\n",
      "loss: 0.0005337828188203275\n",
      "loss: 0.09057264029979706\n",
      "loss: 0.11454454809427261\n",
      "loss: 0.008375514298677444\n",
      "loss: 0.02709968388080597\n",
      "loss: 0.4013383388519287\n",
      "loss: 0.08107864856719971\n",
      "loss: 0.15484921634197235\n",
      "loss: 0.004120362922549248\n",
      "loss: 0.002999448450282216\n",
      "loss: 0.00040533411083742976\n",
      "loss: 0.08467403054237366\n",
      "loss: 0.10955143719911575\n",
      "loss: 0.007508501410484314\n",
      "loss: 0.025052806362509727\n",
      "loss: 0.40788304805755615\n",
      "loss: 0.07860952615737915\n",
      "loss: 0.14134271442890167\n",
      "loss: 0.0034613842144608498\n",
      "loss: 0.002526193158701062\n",
      "loss: 0.00032096292125061154\n",
      "loss: 0.07997845858335495\n",
      "loss: 0.10569889098405838\n",
      "loss: 0.006861450150609016\n",
      "loss: 0.023479493334889412\n",
      "loss: 0.41413694620132446\n",
      "loss: 0.0766865462064743\n",
      "loss: 0.13052910566329956\n",
      "loss: 0.0029810352716594934\n",
      "loss: 0.002180567942559719\n",
      "loss: 0.0002626525529194623\n",
      "loss: 0.07615851610898972\n",
      "loss: 0.10262425988912582\n",
      "loss: 0.0063605098985135555\n",
      "loss: 0.022230815142393112\n",
      "loss: 0.41997963190078735\n",
      "loss: 0.07514050602912903\n",
      "loss: 0.12172458320856094\n",
      "loss: 0.0026190574280917645\n",
      "loss: 0.0019196805078536272\n",
      "loss: 0.00022074035950936377\n",
      "loss: 0.07299627363681793\n",
      "loss: 0.10010604560375214\n",
      "loss: 0.005961509887129068\n",
      "loss: 0.021215079352259636\n",
      "loss: 0.4253666400909424\n",
      "loss: 0.07386603206396103\n",
      "loss: 0.11445465683937073\n",
      "loss: 0.0023390566930174828\n",
      "loss: 0.0017174917738884687\n",
      "loss: 0.00018956074200104922\n",
      "loss: 0.07034139335155487\n",
      "loss: 0.09800101816654205\n",
      "loss: 0.005636513698846102\n",
      "loss: 0.02037263661623001\n",
      "loss: 0.43029430508613586\n",
      "loss: 0.07279419153928757\n",
      "loss: 0.10837923735380173\n",
      "loss: 0.0021176093723624945\n",
      "loss: 0.0015573701821267605\n",
      "loss: 0.00016583387332502753\n",
      "loss: 0.06808662414550781\n",
      "loss: 0.09621267020702362\n",
      "loss: 0.0053670527413487434\n",
      "loss: 0.019663289189338684\n",
      "loss: 0.4347797632217407\n",
      "loss: 0.0718783587217331\n",
      "loss: 0.1032494530081749\n",
      "loss: 0.0019392685499042273\n",
      "loss: 0.0014281929470598698\n",
      "loss: 0.00014729391841683537\n",
      "loss: 0.06615280359983444\n",
      "loss: 0.09467397630214691\n",
      "loss: 0.005140320863574743\n",
      "loss: 0.01905832625925541\n",
      "loss: 0.43885084986686707\n",
      "loss: 0.0710858404636383\n",
      "loss: 0.09887932240962982\n",
      "loss: 0.0017935017822310328\n",
      "loss: 0.0013224282301962376\n",
      "loss: 0.00013250990014057606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.06448060274124146\n",
      "loss: 0.09333623200654984\n",
      "loss: 0.004947423469275236\n",
      "loss: 0.01853727363049984\n",
      "loss: 0.4425397217273712\n",
      "loss: 0.07039319723844528\n",
      "loss: 0.3612975776195526\n",
      "loss: 0.08294819295406342\n",
      "loss: 0.061976294964551926\n",
      "loss: 0.02830314077436924\n",
      "loss: 0.14816629886627197\n",
      "loss: 0.5841052532196045\n",
      "loss: 0.11568178236484528\n",
      "loss: 0.171481192111969\n",
      "loss: 0.5808043479919434\n",
      "loss: 0.2157561480998993\n",
      "loss: 0.2735663950443268\n",
      "loss: 0.0225283894687891\n",
      "loss: 0.016663212329149246\n",
      "loss: 0.004497194662690163\n",
      "loss: 0.12481240183115005\n",
      "loss: 0.2588919401168823\n",
      "loss: 0.033869948238134384\n",
      "loss: 0.0758170336484909\n",
      "loss: 0.478016197681427\n",
      "loss: 0.14116539061069489\n",
      "loss: 0.23104546964168549\n",
      "loss: 0.012285588309168816\n",
      "loss: 0.009006044827401638\n",
      "loss: 0.0018950167577713728\n",
      "loss: 0.11046531051397324\n",
      "loss: 0.18260858952999115\n",
      "loss: 0.018975524231791496\n",
      "loss: 0.04994438961148262\n",
      "loss: 0.44132164120674133\n",
      "loss: 0.11291074752807617\n",
      "loss: 0.200922429561615\n",
      "loss: 0.008180429227650166\n",
      "loss: 0.005975780542939901\n",
      "loss: 0.0010685666929930449\n",
      "loss: 0.10024328529834747\n",
      "loss: 0.14974108338356018\n",
      "loss: 0.01333898026496172\n",
      "loss: 0.038544487208127975\n",
      "loss: 0.42634567618370056\n",
      "loss: 0.09854017943143845\n",
      "loss: 0.17813873291015625\n",
      "loss: 0.006024290807545185\n",
      "loss: 0.004397151060402393\n",
      "loss: 0.000697498326189816\n",
      "loss: 0.09251312166452408\n",
      "loss: 0.1318228393793106\n",
      "loss: 0.01049450971186161\n",
      "loss: 0.032280683517456055\n",
      "loss: 0.4208267033100128\n",
      "loss: 0.09005829691886902\n",
      "loss: 0.1603802591562271\n",
      "loss: 0.004722762852907181\n",
      "loss: 0.003448823932558298\n",
      "loss: 0.0004980612429790199\n",
      "loss: 0.08645393699407578\n",
      "loss: 0.12071522325277328\n",
      "loss: 0.008813782595098019\n",
      "loss: 0.02836924046278\n",
      "loss: 0.41996893286705017\n",
      "loss: 0.08456823229789734\n",
      "loss: 0.1462533324956894\n",
      "loss: 0.003865909995511174\n",
      "loss: 0.0028263297863304615\n",
      "loss: 0.0003782033745665103\n",
      "loss: 0.08158327639102936\n",
      "loss: 0.11323349922895432\n",
      "loss: 0.007715648505836725\n",
      "loss: 0.025711802765727043\n",
      "loss: 0.4215262234210968\n",
      "loss: 0.08077740669250488\n",
      "loss: 0.13483236730098724\n",
      "loss: 0.0032670162618160248\n",
      "loss: 0.0023918121587485075\n",
      "loss: 0.00030033328221179545\n",
      "loss: 0.07759217917919159\n",
      "loss: 0.10788573324680328\n",
      "loss: 0.006946434732526541\n",
      "loss: 0.023794716224074364\n",
      "loss: 0.4243292808532715\n",
      "loss: 0.07802790403366089\n",
      "loss: 0.12547290325164795\n",
      "loss: 0.002829557517543435\n",
      "loss: 0.002074603922665119\n",
      "loss: 0.00024679367197677493\n",
      "loss: 0.07427124679088593\n",
      "loss: 0.10388661175966263\n",
      "loss: 0.006379105616360903\n",
      "loss: 0.02234801836311817\n",
      "loss: 0.4277311861515045\n",
      "loss: 0.07595361024141312\n",
      "loss: 0.11771242320537567\n",
      "loss: 0.0024988853838294744\n",
      "loss: 0.001834882772527635\n",
      "loss: 0.00020833993039559573\n",
      "loss: 0.07147308439016342\n",
      "loss: 0.100786954164505\n",
      "loss: 0.00594406109303236\n",
      "loss: 0.02121775969862938\n",
      "loss: 0.43136101961135864\n",
      "loss: 0.07433711737394333\n",
      "loss: 0.1112111285328865\n",
      "loss: 0.002242215909063816\n",
      "loss: 0.0016486519016325474\n",
      "loss: 0.00017978377582039684\n",
      "loss: 0.06909076869487762\n",
      "loss: 0.09831386804580688\n",
      "loss: 0.005600129719823599\n",
      "loss: 0.020310042425990105\n",
      "loss: 0.4350045323371887\n",
      "loss: 0.07304269820451736\n",
      "loss: 0.10571498423814774\n",
      "loss: 0.0020385286770761013\n",
      "loss: 0.00150083820335567\n",
      "loss: 0.00015796477964613587\n",
      "loss: 0.06704448163509369\n",
      "loss: 0.09629306197166443\n",
      "loss: 0.005321510601788759\n",
      "loss: 0.019564999267458916\n",
      "loss: 0.43853864073753357\n",
      "loss: 0.07198218256235123\n",
      "loss: 0.10103051364421844\n",
      "loss: 0.001873936620540917\n",
      "loss: 0.0013812779216095805\n",
      "loss: 0.0001408557000104338\n",
      "loss: 0.06527344137430191\n",
      "loss: 0.0946095660328865\n",
      "loss: 0.005091433878988028\n",
      "loss: 0.018942540511488914\n",
      "loss: 0.44189563393592834\n",
      "loss: 0.07109659165143967\n",
      "loss: 0.09700927138328552\n",
      "loss: 0.001738926861435175\n",
      "loss: 0.0012830975465476513\n",
      "loss: 0.00012726400746032596\n",
      "loss: 0.0637306421995163\n",
      "loss: 0.09318449348211288\n",
      "loss: 0.0048984261229634285\n",
      "loss: 0.018414990976452827\n",
      "loss: 0.4450419843196869\n",
      "loss: 0.07034510374069214\n",
      "loss: 0.0935349091887474\n",
      "loss: 0.001626681420020759\n",
      "loss: 0.0012014569947496057\n",
      "loss: 0.00011623580940067768\n",
      "loss: 0.06237919256091118\n",
      "loss: 0.09196184575557709\n",
      "loss: 0.004734380636364222\n",
      "loss: 0.017962543293833733\n",
      "loss: 0.4479641616344452\n",
      "loss: 0.06969885528087616\n",
      "loss: 0.09051567316055298\n",
      "loss: 0.0015324169071391225\n",
      "loss: 0.0011327719548717141\n",
      "loss: 0.00010723450395744294\n",
      "loss: 0.06118923798203468\n",
      "loss: 0.09090177714824677\n",
      "loss: 0.004593535326421261\n",
      "loss: 0.017570655792951584\n",
      "loss: 0.4506617784500122\n",
      "loss: 0.06913714110851288\n",
      "loss: 0.08787842839956284\n",
      "loss: 0.0014524870784953237\n",
      "loss: 0.0010745335603132844\n",
      "loss: 9.966393554350361e-05\n",
      "loss: 0.06013709679245949\n",
      "loss: 0.0899742990732193\n",
      "loss: 0.004471449181437492\n",
      "loss: 0.017228707671165466\n",
      "loss: 0.45314210653305054\n",
      "loss: 0.0686444342136383\n",
      "loss: 0.08556407690048218\n",
      "loss: 0.0013840831816196442\n",
      "loss: 0.0010246518068015575\n",
      "loss: 9.334523201687261e-05\n",
      "loss: 0.05920317396521568\n",
      "loss: 0.08915688842535019\n",
      "loss: 0.004364823456853628\n",
      "loss: 0.01692809723317623\n",
      "loss: 0.4554155766963959\n",
      "loss: 0.06820888072252274\n",
      "loss: 0.08352439105510712\n",
      "loss: 0.0013251736527308822\n",
      "loss: 0.000981633667834103\n",
      "loss: 8.803993841866031e-05\n",
      "loss: 0.05837147310376167\n",
      "loss: 0.0884319543838501\n",
      "loss: 0.004271198529750109\n",
      "loss: 0.016662485897541046\n",
      "loss: 0.45749586820602417\n",
      "loss: 0.06782180815935135\n",
      "loss: 0.08172012865543365\n",
      "loss: 0.0012740857200697064\n",
      "loss: 0.000944285246077925\n",
      "loss: 8.344998786924407e-05\n",
      "loss: 0.05762847512960434\n",
      "loss: 0.08778595179319382\n",
      "loss: 0.004188595339655876\n",
      "loss: 0.01642657443881035\n",
      "loss: 0.4593968391418457\n",
      "loss: 0.06747575849294662\n",
      "loss: 0.5166084170341492\n",
      "loss: 0.32635751366615295\n",
      "loss: 0.2406589537858963\n",
      "loss: 0.08533769100904465\n",
      "loss: 0.3509202003479004\n",
      "loss: 0.6432857513427734\n",
      "loss: 0.17796024680137634\n",
      "loss: 0.193389892578125\n",
      "loss: 0.5850932598114014\n",
      "loss: 0.225624680519104\n",
      "loss: 0.3042985498905182\n",
      "loss: 0.036499232053756714\n",
      "loss: 0.028593512251973152\n",
      "loss: 0.0070234439335763454\n",
      "loss: 0.19943338632583618\n",
      "loss: 0.2520095109939575\n",
      "loss: 0.039458248764276505\n",
      "loss: 0.07783614099025726\n",
      "loss: 0.4684865474700928\n",
      "loss: 0.14191901683807373\n",
      "loss: 0.25245460867881775\n",
      "loss: 0.01693403720855713\n",
      "loss: 0.012884958647191525\n",
      "loss: 0.002600471954792738\n",
      "loss: 0.15061531960964203\n",
      "loss: 0.1781439334154129\n",
      "loss: 0.02091832086443901\n",
      "loss: 0.050825364887714386\n",
      "loss: 0.43153342604637146\n",
      "loss: 0.11298929154872894\n",
      "loss: 0.2171243280172348\n",
      "loss: 0.010414703749120235\n",
      "loss: 0.007799324579536915\n",
      "loss: 0.0013734588865190744\n",
      "loss: 0.12488783150911331\n",
      "loss: 0.14742550253868103\n",
      "loss: 0.014382297173142433\n",
      "loss: 0.039262957870960236\n",
      "loss: 0.4176715314388275\n",
      "loss: 0.09869056195020676\n",
      "loss: 0.19080975651741028\n",
      "loss: 0.007300076074898243\n",
      "loss: 0.005419908557087183\n",
      "loss: 0.0008592123631387949\n",
      "loss: 0.10881024599075317\n",
      "loss: 0.13077831268310547\n",
      "loss: 0.011177187785506248\n",
      "loss: 0.032942548394203186\n",
      "loss: 0.4133216440677643\n",
      "loss: 0.09033755213022232\n",
      "loss: 0.17050795257091522\n",
      "loss: 0.0055303629487752914\n",
      "loss: 0.004086907021701336\n",
      "loss: 0.0005948524340055883\n",
      "loss: 0.09778796881437302\n",
      "loss: 0.12041646242141724\n",
      "loss: 0.009309817105531693\n",
      "loss: 0.02898477390408516\n",
      "loss: 0.413483589887619\n",
      "loss: 0.08494211733341217\n",
      "loss: 0.15448568761348724\n",
      "loss: 0.004413914401084185\n",
      "loss: 0.003254159353673458\n",
      "loss: 0.0004409927933011204\n",
      "loss: 0.08976725488901138\n",
      "loss: 0.113375224173069\n",
      "loss: 0.008098764345049858\n",
      "loss: 0.026279989629983902\n",
      "loss: 0.41589605808258057\n",
      "loss: 0.08120696246623993\n",
      "loss: 0.14161764085292816\n",
      "loss: 0.0036580630112439394\n",
      "loss: 0.002694179769605398\n",
      "loss: 0.0003435009566601366\n",
      "loss: 0.08367959409952164\n",
      "loss: 0.10828517377376556\n",
      "loss: 0.007253845222294331\n",
      "loss: 0.024314912036061287\n",
      "loss: 0.4194128215312958\n",
      "loss: 0.07848335057497025\n",
      "loss: 0.13113155961036682\n",
      "loss: 0.0031195012852549553\n",
      "loss: 0.0022969378624111414\n",
      "loss: 0.00027779623633250594\n",
      "loss: 0.0789116621017456\n",
      "loss: 0.1044306755065918\n",
      "loss: 0.00663209892809391\n",
      "loss: 0.022821297869086266\n",
      "loss: 0.42341187596321106\n",
      "loss: 0.07641397416591644\n",
      "loss: 0.1224793866276741\n",
      "loss: 0.0027204169891774654\n",
      "loss: 0.0020035894121974707\n",
      "loss: 0.00023135238734539598\n",
      "loss: 0.07508493214845657\n",
      "loss: 0.10140473395586014\n",
      "loss: 0.006156042218208313\n",
      "loss: 0.02164640463888645\n",
      "loss: 0.42754533886909485\n",
      "loss: 0.07478857785463333\n",
      "loss: 0.11526224762201309\n",
      "loss: 0.002415651688352227\n",
      "loss: 0.0017799473134800792\n",
      "loss: 0.00019737044931389391\n",
      "loss: 0.07195322960615158\n",
      "loss: 0.09896019101142883\n",
      "loss: 0.00578014412894845\n",
      "loss: 0.020697224885225296\n",
      "loss: 0.43161702156066895\n",
      "loss: 0.07347650825977325\n",
      "loss: 0.10918426513671875\n",
      "loss: 0.0021769837476313114\n",
      "loss: 0.001605189056135714\n",
      "loss: 0.00017167611804325134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.06934921443462372\n",
      "loss: 0.09693985432386398\n",
      "loss: 0.005476003047078848\n",
      "loss: 0.01991407200694084\n",
      "loss: 0.43551814556121826\n",
      "loss: 0.07239323109388351\n",
      "loss: 0.10402178764343262\n",
      "loss: 0.001986389048397541\n",
      "loss: 0.0014656191924586892\n",
      "loss: 0.0001518245553597808\n",
      "loss: 0.06715564429759979\n",
      "loss: 0.0952390804886818\n",
      "loss: 0.0052249799482524395\n",
      "loss: 0.01925688050687313\n",
      "loss: 0.4391924738883972\n",
      "loss: 0.0714821144938469\n",
      "loss: 0.09960390627384186\n",
      "loss: 0.001831479137763381\n",
      "loss: 0.001352330087684095\n",
      "loss: 0.00013608665904030204\n",
      "loss: 0.06528718024492264\n",
      "loss: 0.09378606826066971\n",
      "loss: 0.0050146933645009995\n",
      "loss: 0.018698012456297874\n",
      "loss: 0.4426147937774658\n",
      "loss: 0.07070423662662506\n",
      "loss: 0.09579768031835556\n",
      "loss: 0.0017038786318153143\n",
      "loss: 0.001258986652828753\n",
      "loss: 0.00012350844917818904\n",
      "loss: 0.06368090212345123\n",
      "loss: 0.09252993017435074\n",
      "loss: 0.004836075007915497\n",
      "loss: 0.01821727119386196\n",
      "loss: 0.445779025554657\n",
      "loss: 0.07003148645162582\n",
      "loss: 0.092498280107975\n",
      "loss: 0.0015974279958754778\n",
      "loss: 0.0011811673175543547\n",
      "loss: 0.00011319562327116728\n",
      "loss: 0.06228898465633392\n",
      "loss: 0.09143327176570892\n",
      "loss: 0.004682878963649273\n",
      "loss: 0.01780010387301445\n",
      "loss: 0.44868984818458557\n",
      "loss: 0.06944403052330017\n",
      "loss: 0.08962290734052658\n",
      "loss: 0.0015077029820531607\n",
      "loss: 0.001115526887588203\n",
      "loss: 0.00010473084694240242\n",
      "loss: 0.06107461452484131\n",
      "loss: 0.09046836942434311\n",
      "loss: 0.004550363402813673\n",
      "loss: 0.017435390502214432\n",
      "loss: 0.45135873556137085\n",
      "loss: 0.06892664730548859\n",
      "loss: 0.0871049240231514\n",
      "loss: 0.0014313565334305167\n",
      "loss: 0.0010596761712804437\n",
      "loss: 9.763717389432713e-05\n",
      "loss: 0.06000898778438568\n",
      "loss: 0.08961424231529236\n",
      "loss: 0.00443474855273962\n",
      "loss: 0.017114466056227684\n",
      "loss: 0.4538003206253052\n",
      "loss: 0.0684681236743927\n",
      "loss: 0.08489026874303818\n",
      "loss: 0.0013659384567290545\n",
      "loss: 0.001011823769658804\n",
      "loss: 9.161653724731877e-05\n",
      "loss: 0.059068989008665085\n",
      "loss: 0.08885392546653748\n",
      "loss: 0.004333394579589367\n",
      "loss: 0.016830559819936752\n",
      "loss: 0.45603054761886597\n",
      "loss: 0.06805938482284546\n",
      "loss: 0.08293452858924866\n",
      "loss: 0.0013094769092276692\n",
      "loss: 0.0009704767726361752\n",
      "loss: 8.654969133203849e-05\n",
      "loss: 0.05823632329702377\n",
      "loss: 0.08817432075738907\n",
      "loss: 0.004243902862071991\n",
      "loss: 0.01657842844724655\n",
      "loss: 0.4580657482147217\n",
      "loss: 0.06769322603940964\n",
      "loss: 0.08120127022266388\n",
      "loss: 0.0012604189105331898\n",
      "loss: 0.0009345605503767729\n",
      "loss: 8.219818118959665e-05\n",
      "loss: 0.057495761662721634\n",
      "loss: 0.08756452798843384\n",
      "loss: 0.0041647134348750114\n",
      "loss: 0.016353623941540718\n",
      "loss: 0.4599224030971527\n",
      "loss: 0.06736430525779724\n",
      "loss: 0.07966044545173645\n",
      "loss: 0.001217569806613028\n",
      "loss: 0.0009031796944327652\n",
      "loss: 7.844279025448486e-05\n",
      "loss: 0.05683492496609688\n",
      "loss: 0.08701597154140472\n",
      "loss: 0.004094328265637159\n",
      "loss: 0.01615249365568161\n",
      "loss: 0.46161550283432007\n",
      "loss: 0.06706780940294266\n",
      "loss: 0.0782865360379219\n",
      "loss: 0.001179973827674985\n",
      "loss: 0.0008756178431212902\n",
      "loss: 7.516428013332188e-05\n",
      "loss: 0.05624343827366829\n",
      "loss: 0.0865212008357048\n",
      "loss: 0.004031368996948004\n",
      "loss: 0.01597193442285061\n",
      "loss: 0.46315979957580566\n",
      "loss: 0.06680001318454742\n",
      "loss: 0.07705839723348618\n",
      "loss: 0.0011468547163531184\n",
      "loss: 0.0008513974607922137\n",
      "loss: 7.230304618133232e-05\n",
      "loss: 0.05571254715323448\n",
      "loss: 0.08607415854930878\n",
      "loss: 0.003975115716457367\n",
      "loss: 0.015809696167707443\n",
      "loss: 0.4645683765411377\n",
      "loss: 0.06655757874250412\n",
      "loss: 0.07595779001712799\n",
      "loss: 0.001117555657401681\n",
      "loss: 0.0008298620814457536\n",
      "loss: 6.979947647778317e-05\n",
      "loss: 0.055235035717487335\n",
      "loss: 0.08566926419734955\n",
      "loss: 0.003924670163542032\n",
      "loss: 0.015663469210267067\n",
      "loss: 0.46585291624069214\n",
      "loss: 0.06633749604225159\n",
      "loss: 0.8517780303955078\n",
      "loss: 0.2617064416408539\n",
      "loss: 0.13988681137561798\n",
      "loss: 0.09569843858480453\n",
      "loss: 0.07818147540092468\n",
      "loss: 1.1407750844955444\n",
      "loss: 0.10743123292922974\n",
      "loss: 0.14513519406318665\n",
      "loss: 0.5206553936004639\n",
      "loss: 0.1474197655916214\n",
      "loss: 0.6780053973197937\n",
      "loss: 0.15451964735984802\n",
      "loss: 0.09387720376253128\n",
      "loss: 0.025989003479480743\n",
      "loss: 0.19268262386322021\n",
      "loss: 0.48278599977493286\n",
      "loss: 0.059194762259721756\n",
      "loss: 0.09207535535097122\n",
      "loss: 0.4384841024875641\n",
      "loss: 0.1185956671833992\n",
      "loss: 0.3067411780357361\n",
      "loss: 0.015315149910748005\n",
      "loss: 0.0108462143689394\n",
      "loss: 0.0019682333804666996\n",
      "loss: 0.11793056130409241\n",
      "loss: 0.2034033238887787\n",
      "loss: 0.01743745245039463\n",
      "loss: 0.043036483228206635\n",
      "loss: 0.3991517424583435\n",
      "loss: 0.08438318967819214\n",
      "loss: 0.22120550274848938\n",
      "loss: 0.006747487932443619\n",
      "loss: 0.00484290299937129\n",
      "loss: 0.0007034629816189408\n",
      "loss: 0.09096666425466537\n",
      "loss: 0.15597745776176453\n",
      "loss: 0.010788431391119957\n",
      "loss: 0.031122544780373573\n",
      "loss: 0.40006503462791443\n",
      "loss: 0.07322828471660614\n",
      "loss: 0.1749461591243744\n",
      "loss: 0.004089181311428547\n",
      "loss: 0.002964355982840061\n",
      "loss: 0.0003713462792802602\n",
      "loss: 0.07696285843849182\n",
      "loss: 0.13693183660507202\n",
      "loss: 0.008251763880252838\n",
      "loss: 0.025871863588690758\n",
      "loss: 0.4090239107608795\n",
      "loss: 0.06791335344314575\n",
      "loss: 0.14640064537525177\n",
      "loss: 0.0028859255835413933\n",
      "loss: 0.002109247026965022\n",
      "loss: 0.0002371353912167251\n",
      "loss: 0.06844042241573334\n",
      "loss: 0.12669314444065094\n",
      "loss: 0.006932270247489214\n",
      "loss: 0.022913198918104172\n",
      "loss: 0.41916200518608093\n",
      "loss: 0.06483133882284164\n",
      "loss: 1.1878669261932373\n",
      "loss: 0.29233554005622864\n",
      "loss: 0.14459070563316345\n",
      "loss: 0.07260311394929886\n",
      "loss: 0.11407418549060822\n",
      "loss: 0.7072978019714355\n",
      "loss: 0.07515837252140045\n",
      "loss: 0.11559713631868362\n",
      "loss: 0.42815399169921875\n",
      "loss: 0.1277875155210495\n",
      "loss: 0.3545377850532532\n",
      "loss: 0.016613395884633064\n",
      "loss: 0.011172967962920666\n",
      "loss: 0.0024246140383183956\n",
      "loss: 0.0995146855711937\n",
      "loss: 0.22246897220611572\n",
      "loss: 0.018675239756703377\n",
      "loss: 0.04689788445830345\n",
      "loss: 0.3911643624305725\n",
      "loss: 0.08671215921640396\n",
      "loss: 0.2392299622297287\n",
      "loss: 0.0071052005514502525\n",
      "loss: 0.004973659757524729\n",
      "loss: 0.0007860768819227815\n",
      "loss: 0.08456842601299286\n",
      "loss: 0.16228365898132324\n",
      "loss: 0.01117531955242157\n",
      "loss: 0.03251346945762634\n",
      "loss: 0.394862562417984\n",
      "loss: 0.07421758025884628\n",
      "loss: 0.18337391316890717\n",
      "loss: 0.004247793462127447\n",
      "loss: 0.0030392652843147516\n",
      "loss: 0.00039859607932157815\n",
      "loss: 0.07448402792215347\n",
      "loss: 0.13984648883342743\n",
      "loss: 0.008437125012278557\n",
      "loss: 0.026540517807006836\n",
      "loss: 0.40558722615242004\n",
      "loss: 0.06844335794448853\n",
      "loss: 0.1508953720331192\n",
      "loss: 0.0029716494027525187\n",
      "loss: 0.0021563158370554447\n",
      "loss: 0.0002491188351996243\n",
      "loss: 0.06747276335954666\n",
      "loss: 0.1282779425382614\n",
      "loss: 0.0070411511696875095\n",
      "loss: 0.023289920762181282\n",
      "loss: 0.41682034730911255\n",
      "loss: 0.0651572048664093\n",
      "loss: 0.1301189810037613\n",
      "loss: 0.002281584544107318\n",
      "loss: 0.0016716974787414074\n",
      "loss: 0.00017602801381144673\n",
      "loss: 0.062417447566986084\n",
      "loss: 0.12120614945888519\n",
      "loss: 0.006197184324264526\n",
      "loss: 0.0212419293820858\n",
      "loss: 0.4268907308578491\n",
      "loss: 0.063026562333107\n",
      "loss: 0.11596419662237167\n",
      "loss: 0.0018630085978657007\n",
      "loss: 0.0013744139578193426\n",
      "loss: 0.00013483480142895132\n",
      "loss: 0.058661796152591705\n",
      "loss: 0.1164078637957573\n",
      "loss: 0.005633216816931963\n",
      "loss: 0.019831808283925056\n",
      "loss: 0.4354623854160309\n",
      "loss: 0.06152132898569107\n",
      "loss: 0.10587453097105026\n",
      "loss: 0.001588831190019846\n",
      "loss: 0.0011781238717958331\n",
      "loss: 0.00010932089207926765\n",
      "loss: 0.05580436810851097\n",
      "loss: 0.11292616277933121\n",
      "loss: 0.0052315108478069305\n",
      "loss: 0.018804477527737617\n",
      "loss: 0.4426109790802002\n",
      "loss: 0.06039602309465408\n",
      "loss: 0.09843447804450989\n",
      "loss: 0.001399243832565844\n",
      "loss: 0.00104147766251117\n",
      "loss: 9.245107503375039e-05\n",
      "loss: 0.05358877405524254\n",
      "loss: 0.11028625816106796\n",
      "loss: 0.004932987503707409\n",
      "loss: 0.018026933073997498\n",
      "loss: 0.44852733612060547\n",
      "loss: 0.05952303111553192\n",
      "loss: 0.09280320256948471\n",
      "loss: 0.0012626867974177003\n",
      "loss: 0.0009426147444173694\n",
      "loss: 8.076755329966545e-05\n",
      "loss: 0.05184464529156685\n",
      "loss: 0.10822546482086182\n",
      "loss: 0.004704616963863373\n",
      "loss: 0.017423078417778015\n",
      "loss: 0.45341408252716064\n",
      "loss: 0.05882911756634712\n",
      "loss: 0.6560807824134827\n",
      "loss: 0.2537561357021332\n",
      "loss: 0.14579057693481445\n",
      "loss: 0.03323310241103172\n",
      "loss: 0.24505704641342163\n",
      "loss: 0.6058017611503601\n",
      "loss: 0.07833952456712723\n",
      "loss: 0.10356930643320084\n",
      "loss: 0.492610901594162\n",
      "loss: 0.1284870058298111\n",
      "loss: 0.26856619119644165\n",
      "loss: 0.01458833273500204\n",
      "loss: 0.010726556181907654\n",
      "loss: 0.001812729169614613\n",
      "loss: 0.12535351514816284\n",
      "loss: 0.2099064737558365\n",
      "loss: 0.01828155480325222\n",
      "loss: 0.04376061633229256\n",
      "loss: 0.41883909702301025\n",
      "loss: 0.08651210367679596\n",
      "loss: 0.20582719147205353\n",
      "loss: 0.006427695509046316\n",
      "loss: 0.004695334937423468\n",
      "loss: 0.0006544943316839635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0929546132683754\n",
      "loss: 0.1569674015045166\n",
      "loss: 0.010874896310269833\n",
      "loss: 0.031064746901392937\n",
      "loss: 0.4098990261554718\n",
      "loss: 0.07385370880365372\n",
      "loss: 0.1673930287361145\n",
      "loss: 0.00393029535189271\n",
      "loss: 0.002875285455957055\n",
      "loss: 0.0003508944937493652\n",
      "loss: 0.07747746258974075\n",
      "loss: 0.1368284970521927\n",
      "loss: 0.008219251409173012\n",
      "loss: 0.025698775425553322\n",
      "loss: 0.41458141803741455\n",
      "loss: 0.0680646151304245\n",
      "loss: 0.14219363033771515\n",
      "loss: 0.0027958459686487913\n",
      "loss: 0.0020537590608000755\n",
      "loss: 0.0002268810203531757\n",
      "loss: 0.06846250593662262\n",
      "loss: 0.12632590532302856\n",
      "loss: 0.0068799350410699844\n",
      "loss: 0.022740989923477173\n",
      "loss: 0.4225561320781708\n",
      "loss: 0.06481257826089859\n",
      "loss: 0.12490370124578476\n",
      "loss: 0.0021737581118941307\n",
      "loss: 0.001603935263119638\n",
      "loss: 0.00016392620455008\n",
      "loss: 0.06260811537504196\n",
      "loss: 0.11984965950250626\n",
      "loss: 0.0060757421888411045\n",
      "loss: 0.020859040319919586\n",
      "loss: 0.43074673414230347\n",
      "loss: 0.06273002177476883\n",
      "loss: 0.1125975027680397\n",
      "loss: 0.0017920687096193433\n",
      "loss: 0.0013276804238557816\n",
      "loss: 0.0001275620743399486\n",
      "loss: 0.05853457376360893\n",
      "loss: 0.11542118340730667\n",
      "loss: 0.005540371872484684\n",
      "loss: 0.019552964717149734\n",
      "loss: 0.4381762444972992\n",
      "loss: 0.06127269193530083\n",
      "loss: 0.1035710871219635\n",
      "loss: 0.0015397595707327127\n",
      "loss: 0.0011446467833593488\n",
      "loss: 0.00010467123502166942\n",
      "loss: 0.05556358024477959\n",
      "loss: 0.11218532919883728\n",
      "loss: 0.005159612745046616\n",
      "loss: 0.018594838678836823\n",
      "loss: 0.44459837675094604\n",
      "loss: 0.06019020453095436\n",
      "loss: 0.09678559750318527\n",
      "loss: 0.0013639091048389673\n",
      "loss: 0.0010168952867388725\n",
      "loss: 8.929174509830773e-05\n",
      "loss: 0.05332273989915848\n",
      "loss: 0.10971684753894806\n",
      "loss: 0.00487662386149168\n",
      "loss: 0.01786569505929947\n",
      "loss: 0.45003098249435425\n",
      "loss: 0.059353381395339966\n",
      "loss: 0.09158075600862503\n",
      "loss: 0.0012365473667159677\n",
      "loss: 0.0009241200750693679\n",
      "loss: 7.850239489926025e-05\n",
      "loss: 0.0515906848013401\n",
      "loss: 0.1077791079878807\n",
      "loss: 0.00466000335291028\n",
      "loss: 0.017296748235821724\n",
      "loss: 0.4545820951461792\n",
      "loss: 0.058689482510089874\n",
      "loss: 0.0875219851732254\n",
      "loss: 0.0011414841283112764\n",
      "loss: 0.000854678510222584\n",
      "loss: 7.063399971229956e-05\n",
      "loss: 0.05022688955068588\n",
      "loss: 0.10622955858707428\n",
      "loss: 0.0044904290698468685\n",
      "loss: 0.016844987869262695\n",
      "loss: 0.4583791196346283\n",
      "loss: 0.058153435587882996\n",
      "loss: 0.08431366831064224\n",
      "loss: 0.0010688650654628873\n",
      "loss: 0.000801646092440933\n",
      "loss: 6.47327397018671e-05\n",
      "loss: 0.049137942492961884\n",
      "loss: 0.10497384518384933\n",
      "loss: 0.0043556042946875095\n",
      "loss: 0.016481652855873108\n",
      "loss: 0.4615435004234314\n",
      "loss: 0.05771574005484581\n",
      "loss: 0.08174930512905121\n",
      "loss: 0.0010124800028279424\n",
      "loss: 0.0007603077683597803\n",
      "loss: 6.026211121934466e-05\n",
      "loss: 0.04825882986187935\n",
      "loss: 0.1039469912648201\n",
      "loss: 0.004247194621711969\n",
      "loss: 0.01618647761642933\n",
      "loss: 0.46418124437332153\n",
      "loss: 0.05735523998737335\n",
      "loss: 0.07968058437108994\n",
      "loss: 0.0009679112699814141\n",
      "loss: 0.0007276201504282653\n",
      "loss: 5.680483809555881e-05\n",
      "loss: 0.04754318296909332\n",
      "loss: 0.10310149937868118\n",
      "loss: 0.004159266594797373\n",
      "loss: 0.01594492420554161\n",
      "loss: 0.46638214588165283\n",
      "loss: 0.05705671384930611\n",
      "loss: 0.07799896597862244\n",
      "loss: 0.0009324128041043878\n",
      "loss: 0.000701494631357491\n",
      "loss: 5.406287527875975e-05\n",
      "loss: 0.04695660248398781\n",
      "loss: 0.1024019718170166\n",
      "loss: 0.004087326116859913\n",
      "loss: 0.01574629731476307\n",
      "loss: 0.4682215452194214\n",
      "loss: 0.05680861696600914\n",
      "loss: 1.0151257514953613\n",
      "loss: 0.31793349981307983\n",
      "loss: 0.1638796478509903\n",
      "loss: 0.04197864234447479\n",
      "loss: 0.2270001620054245\n",
      "loss: 0.5234240889549255\n",
      "loss: 0.06799041479825974\n",
      "loss: 0.09806196391582489\n",
      "loss: 0.4357856512069702\n",
      "loss: 0.1211932972073555\n",
      "loss: 0.3241143524646759\n",
      "loss: 0.01747208647429943\n",
      "loss: 0.012349250726401806\n",
      "loss: 0.00226318440400064\n",
      "loss: 0.1256529986858368\n",
      "loss: 0.2060592770576477\n",
      "loss: 0.018233781680464745\n",
      "loss: 0.0440794937312603\n",
      "loss: 0.39536774158477783\n",
      "loss: 0.08499935269355774\n",
      "loss: 0.22948624193668365\n",
      "loss: 0.007277020253241062\n",
      "loss: 0.00521713076159358\n",
      "loss: 0.0007675254601053894\n",
      "loss: 0.09418293088674545\n",
      "loss: 0.15712955594062805\n",
      "loss: 0.011084428988397121\n",
      "loss: 0.03162442147731781\n",
      "loss: 0.3970763385295868\n",
      "loss: 0.07356567680835724\n",
      "loss: 0.1797313541173935\n",
      "loss: 0.004304959904402494\n",
      "loss: 0.003117707557976246\n",
      "loss: 0.00039519727579317987\n",
      "loss: 0.07866007834672928\n",
      "loss: 0.1377248913049698\n",
      "loss: 0.008414163254201412\n",
      "loss: 0.026192795485258102\n",
      "loss: 0.4067372679710388\n",
      "loss: 0.06815936416387558\n",
      "loss: 0.14943864941596985\n",
      "loss: 0.002996578812599182\n",
      "loss: 0.0021882737055420876\n",
      "loss: 0.0002485226432327181\n",
      "loss: 0.06946651637554169\n",
      "loss: 0.12730763852596283\n",
      "loss: 0.007037129253149033\n",
      "loss: 0.023141436278820038\n",
      "loss: 0.41740086674690247\n",
      "loss: 0.06502730399370193\n",
      "loss: 0.1295281946659088\n",
      "loss: 0.0022945483215153217\n",
      "loss: 0.0016868624370545149\n",
      "loss: 0.00017608761845622212\n",
      "loss: 0.06343287229537964\n",
      "loss: 0.12074185162782669\n",
      "loss: 0.006199043709784746\n",
      "loss: 0.021177150309085846\n",
      "loss: 0.4271669089794159\n",
      "loss: 0.0629672110080719\n",
      "loss: 0.11573731899261475\n",
      "loss: 0.0018705327529460192\n",
      "loss: 0.0013825313653796911\n",
      "loss: 0.00013501362991519272\n",
      "loss: 0.05920513719320297\n",
      "loss: 0.11618342995643616\n",
      "loss: 0.005636753514409065\n",
      "loss: 0.0198046937584877\n",
      "loss: 0.4355769157409668\n",
      "loss: 0.06149603798985481\n",
      "loss: 0.10580495744943619\n",
      "loss: 0.0015936668496578932\n",
      "loss: 0.001182897831313312\n",
      "loss: 0.00010955933976219967\n",
      "loss: 0.056107569485902786\n",
      "loss: 0.11282013356685638\n",
      "loss: 0.005235165823251009\n",
      "loss: 0.01879476010799408\n",
      "loss: 0.44264158606529236\n",
      "loss: 0.06038728356361389\n",
      "loss: 0.0984325110912323\n",
      "loss: 0.0014024670235812664\n",
      "loss: 0.0010444610379636288\n",
      "loss: 9.26299107959494e-05\n",
      "loss: 0.053764618933200836\n",
      "loss: 0.11023993790149689\n",
      "loss: 0.004936401732265949\n",
      "loss: 0.018025415018200874\n",
      "loss: 0.44851601123809814\n",
      "loss: 0.05952233448624611\n",
      "loss: 0.09282884001731873\n",
      "loss: 0.0012650142889469862\n",
      "loss: 0.0009445835603401065\n",
      "loss: 8.088677714113146e-05\n",
      "loss: 0.05195049196481705\n",
      "loss: 0.10820932686328888\n",
      "loss: 0.004707551561295986\n",
      "loss: 0.01742508076131344\n",
      "loss: 0.45338377356529236\n",
      "loss: 0.058832213282585144\n",
      "loss: 0.0884881466627121\n",
      "loss: 0.0011630262015387416\n",
      "loss: 0.0008703083731234074\n",
      "loss: 7.242226274684072e-05\n",
      "loss: 0.0505208782851696\n",
      "loss: 0.1065831184387207\n",
      "loss: 0.004528628662228584\n",
      "loss: 0.01694883033633232\n",
      "loss: 0.45741572976112366\n",
      "loss: 0.058273158967494965\n",
      "loss: 0.08507430553436279\n",
      "loss: 0.0010855126893147826\n",
      "loss: 0.0008136959513649344\n",
      "loss: 6.610373384319246e-05\n",
      "loss: 0.04937894642353058\n",
      "loss: 0.10526517778635025\n",
      "loss: 0.00438643479719758\n",
      "loss: 0.01656612567603588\n",
      "loss: 0.46075960993766785\n",
      "loss: 0.05781576782464981\n",
      "loss: 0.08235587924718857\n",
      "loss: 0.0010254870867356658\n",
      "loss: 0.0007697921828366816\n",
      "loss: 6.133506394689903e-05\n",
      "loss: 0.048457201570272446\n",
      "loss: 0.10418754816055298\n",
      "loss: 0.0042722756043076515\n",
      "loss: 0.01625559851527214\n",
      "loss: 0.4635379910469055\n",
      "loss: 0.05743888393044472\n",
      "loss: 0.08016940206289291\n",
      "loss: 0.0009782329434528947\n",
      "loss: 0.0007351954700425267\n",
      "loss: 5.757974577136338e-05\n",
      "loss: 0.04770690202713013\n",
      "loss: 0.103300541639328\n",
      "loss: 0.004179796669632196\n",
      "loss: 0.016001733019948006\n",
      "loss: 0.4658505320549011\n",
      "loss: 0.05712650716304779\n",
      "loss: 0.07839612662792206\n",
      "loss: 0.0009406459284946322\n",
      "loss: 0.0007075786124914885\n",
      "loss: 5.465895446832292e-05\n",
      "loss: 0.04709210991859436\n",
      "loss: 0.10256727039813995\n",
      "loss: 0.004104143474251032\n",
      "loss: 0.015793105587363243\n",
      "loss: 0.46778011322021484\n",
      "loss: 0.05686691403388977\n",
      "loss: 0.07694824784994125\n",
      "loss: 0.0009104580385610461\n",
      "loss: 0.0006853902013972402\n",
      "loss: 5.2334245992824435e-05\n",
      "loss: 0.046585723757743835\n",
      "loss: 0.10195866972208023\n",
      "loss: 0.004042081534862518\n",
      "loss: 0.01562084536999464\n",
      "loss: 0.46939241886138916\n",
      "loss: 0.056650467216968536\n",
      "loss: 0.07575948536396027\n",
      "loss: 0.0008859385270625353\n",
      "loss: 0.0006673774332739413\n",
      "loss: 5.048640741733834e-05\n",
      "loss: 0.04616679251194\n",
      "loss: 0.10145183652639389\n",
      "loss: 0.0039907945320010185\n",
      "loss: 0.015478034503757954\n",
      "loss: 0.470741868019104\n",
      "loss: 0.056469447910785675\n",
      "loss: 0.07477875053882599\n",
      "loss: 0.0008659535087645054\n",
      "loss: 0.0006526453653350472\n",
      "loss: 4.8996218538377434e-05\n",
      "loss: 0.04581885784864426\n",
      "loss: 0.10102873295545578\n",
      "loss: 0.00394824706017971\n",
      "loss: 0.015359212644398212\n",
      "loss: 0.4718729555606842\n",
      "loss: 0.05631790682673454\n",
      "loss: 0.07396647334098816\n",
      "loss: 0.0008494884823448956\n",
      "loss: 0.0006405378226190805\n",
      "loss: 4.774445915245451e-05\n",
      "loss: 0.04552917927503586\n",
      "loss: 0.1006748229265213\n",
      "loss: 0.003913001623004675\n",
      "loss: 0.015260133892297745\n",
      "loss: 0.4728224575519562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.05619078874588013\n",
      "loss: 0.07329164445400238\n",
      "loss: 0.0008359468192793429\n",
      "loss: 0.0006305178394541144\n",
      "loss: 4.673113289754838e-05\n",
      "loss: 0.045287419110536575\n",
      "loss: 0.1003783717751503\n",
      "loss: 0.0038835618179291487\n",
      "loss: 0.015177344903349876\n",
      "loss: 0.47361990809440613\n",
      "loss: 0.05608392506837845\n",
      "loss: 0.40620556473731995\n",
      "loss: 0.18471813201904297\n",
      "loss: 0.12023966014385223\n",
      "loss: 0.035522207617759705\n",
      "loss: 0.18830028176307678\n",
      "loss: 0.862417995929718\n",
      "loss: 0.10193253308534622\n",
      "loss: 0.12252654880285263\n",
      "loss: 0.5672518014907837\n",
      "loss: 0.14258797466754913\n",
      "loss: 0.22288407385349274\n",
      "loss: 0.011379248462617397\n",
      "loss: 0.00854635052382946\n",
      "loss: 0.0014237163122743368\n",
      "loss: 0.11096614599227905\n",
      "loss: 0.22400213778018951\n",
      "loss: 0.01864001713693142\n",
      "loss: 0.0448746420443058\n",
      "loss: 0.44841858744621277\n",
      "loss: 0.08950719982385635\n",
      "loss: 0.1828162968158722\n",
      "loss: 0.005418110638856888\n",
      "loss: 0.004002045374363661\n",
      "loss: 0.0005396272172220051\n",
      "loss: 0.08652795851230621\n",
      "loss: 0.15947766602039337\n",
      "loss: 0.010665105655789375\n",
      "loss: 0.030809370800852776\n",
      "loss: 0.42601925134658813\n",
      "loss: 0.07459962368011475\n",
      "loss: 0.1540953814983368\n",
      "loss: 0.003466707421466708\n",
      "loss: 0.0025533821899443865\n",
      "loss: 0.00030164499185048044\n",
      "loss: 0.07387251406908035\n",
      "loss: 0.1367645263671875\n",
      "loss: 0.007984180934727192\n",
      "loss: 0.02523978054523468\n",
      "loss: 0.4245414137840271\n",
      "loss: 0.06809619814157486\n",
      "loss: 0.13381534814834595\n",
      "loss: 0.0025405941996723413\n",
      "loss: 0.0018744741100817919\n",
      "loss: 0.00020130514167249203\n",
      "loss: 0.06616184860467911\n",
      "loss: 0.12554055452346802\n",
      "loss: 0.006679801270365715\n",
      "loss: 0.02229541726410389\n",
      "loss: 0.42919689416885376\n",
      "loss: 0.06459866464138031\n",
      "loss: 0.11928021907806396\n",
      "loss: 0.002017087070271373\n",
      "loss: 0.0014927794691175222\n",
      "loss: 0.0001490227150497958\n",
      "loss: 0.06101658195257187\n",
      "loss: 0.11891093105077744\n",
      "loss: 0.005914441309869289\n",
      "loss: 0.02047441154718399\n",
      "loss: 0.4354292154312134\n",
      "loss: 0.0624457448720932\n",
      "loss: 0.10863727331161499\n",
      "loss: 0.001688653603196144\n",
      "loss: 0.0012537348084151745\n",
      "loss: 0.00011814338358817622\n",
      "loss: 0.05737392604351044\n",
      "loss: 0.11452389508485794\n",
      "loss: 0.005412117578089237\n",
      "loss: 0.01923372969031334\n",
      "loss: 0.4416256546974182\n",
      "loss: 0.06098737567663193\n",
      "loss: 0.10067752748727798\n",
      "loss: 0.001467887545004487\n",
      "loss: 0.0010929714189842343\n",
      "loss: 9.835250239120796e-05\n",
      "loss: 0.05468617007136345\n",
      "loss: 0.11139161139726639\n",
      "loss: 0.005057645961642265\n",
      "loss: 0.018333761021494865\n",
      "loss: 0.4472270905971527\n",
      "loss: 0.059929877519607544\n",
      "loss: 0.09460996091365814\n",
      "loss: 0.001312043284997344\n",
      "loss: 0.0009793664794415236\n",
      "loss: 8.488061575917527e-05\n",
      "loss: 0.05264259874820709\n",
      "loss: 0.10903807729482651\n",
      "loss: 0.004795348271727562\n",
      "loss: 0.01765315979719162\n",
      "loss: 0.4520871639251709\n",
      "loss: 0.05912602320313454\n",
      "loss: 0.08990797400474548\n",
      "loss: 0.0011979361297562718\n",
      "loss: 0.0008960803388617933\n",
      "loss: 7.528349669883028e-05\n",
      "loss: 0.0510537251830101\n",
      "loss: 0.1072082668542862\n",
      "loss: 0.004594852682203054\n",
      "loss: 0.017123622819781303\n",
      "loss: 0.4562218189239502\n",
      "loss: 0.05849501118063927\n",
      "loss: 0.086213119328022\n",
      "loss: 0.0011121255811303854\n",
      "loss: 0.000833322003018111\n",
      "loss: 6.82496465742588e-05\n",
      "loss: 0.04979722201824188\n",
      "loss: 0.10575281828641891\n",
      "loss: 0.00443786196410656\n",
      "loss: 0.016703637316823006\n",
      "loss: 0.45970577001571655\n",
      "loss: 0.057989008724689484\n",
      "loss: 0.08327514678239822\n",
      "loss: 0.0010461317142471671\n",
      "loss: 0.0007850031834095716\n",
      "loss: 6.294448394328356e-05\n",
      "loss: 0.048790473490953445\n",
      "loss: 0.10457704961299896\n",
      "loss: 0.004313101060688496\n",
      "loss: 0.01636580191552639\n",
      "loss: 0.4626278877258301\n",
      "loss: 0.05757739394903183\n",
      "loss: 0.08091609179973602\n",
      "loss: 0.0009945806814357638\n",
      "loss: 0.0007471848512068391\n",
      "loss: 5.889112435397692e-05\n",
      "loss: 0.04797563701868057\n",
      "loss: 0.10361677408218384\n",
      "loss: 0.0042124176397919655\n",
      "loss: 0.016091253608465195\n",
      "loss: 0.4650748074054718\n",
      "loss: 0.057239290326833725\n",
      "loss: 0.07900595664978027\n",
      "loss: 0.0009536520810797811\n",
      "loss: 0.0007171817705966532\n",
      "loss: 5.5672287999186665e-05\n",
      "loss: 0.04731093719601631\n",
      "loss: 0.10282664746046066\n",
      "loss: 0.004130776971578598\n",
      "loss: 0.015866437926888466\n",
      "loss: 0.4671226143836975\n",
      "loss: 0.0569595992565155\n",
      "loss: 0.07744874060153961\n",
      "loss: 0.0009208983974531293\n",
      "loss: 0.0006930844974704087\n",
      "loss: 5.3168758313404396e-05\n",
      "loss: 0.04676508903503418\n",
      "loss: 0.10217300057411194\n",
      "loss: 0.004063805565237999\n",
      "loss: 0.015681268647313118\n",
      "loss: 0.46883732080459595\n",
      "loss: 0.05672717094421387\n",
      "loss: 0.07617196440696716\n",
      "loss: 0.0008944696164689958\n",
      "loss: 0.0006736401119269431\n",
      "loss: 5.11420912516769e-05\n",
      "loss: 0.046314679086208344\n",
      "loss: 0.1016298308968544\n",
      "loss: 0.004008688032627106\n",
      "loss: 0.015528037212789059\n",
      "loss: 0.47027432918548584\n",
      "loss: 0.05653327330946922\n",
      "loss: 0.07511988282203674\n",
      "loss: 0.0008728736429475248\n",
      "loss: 0.0006577747408300638\n",
      "loss: 4.95326858072076e-05\n",
      "loss: 0.04594147950410843\n",
      "loss: 0.10117724537849426\n",
      "loss: 0.003963207360357046\n",
      "loss: 0.015400795266032219\n",
      "loss: 0.471479594707489\n",
      "loss: 0.05637131631374359\n",
      "loss: 0.07424961030483246\n",
      "loss: 0.0008552154176868498\n",
      "loss: 0.0006447724299505353\n",
      "loss: 4.816171349375509e-05\n",
      "loss: 0.045631177723407745\n",
      "loss: 0.10079901665449142\n",
      "loss: 0.003925328608602285\n",
      "loss: 0.015294753015041351\n",
      "loss: 0.47249123454093933\n",
      "loss: 0.05623536556959152\n",
      "loss: 0.0735272541642189\n",
      "loss: 0.0008406595443375409\n",
      "loss: 0.0006340367253869772\n",
      "loss: 4.708877895609476e-05\n",
      "loss: 0.04537242650985718\n",
      "loss: 0.10048242658376694\n",
      "loss: 0.0038939134683459997\n",
      "loss: 0.015206332318484783\n",
      "loss: 0.4733412265777588\n",
      "loss: 0.05612143874168396\n",
      "loss: 0.07292600721120834\n",
      "loss: 0.0008286689990200102\n",
      "loss: 0.0006251500453799963\n",
      "loss: 4.619466562871821e-05\n",
      "loss: 0.04515621066093445\n",
      "loss: 0.1002170592546463\n",
      "loss: 0.0038676452822983265\n",
      "loss: 0.01513238251209259\n",
      "loss: 0.4740561842918396\n",
      "loss: 0.05602586641907692\n",
      "loss: 0.07242431491613388\n",
      "loss: 0.0008187068160623312\n",
      "loss: 0.000617814133875072\n",
      "loss: 4.547937714960426e-05\n",
      "loss: 0.04497526213526726\n",
      "loss: 0.09999419003725052\n",
      "loss: 0.0038456858601421118\n",
      "loss: 0.015070418827235699\n",
      "loss: 0.47465771436691284\n",
      "loss: 0.05594530329108238\n",
      "loss: 0.07200479507446289\n",
      "loss: 0.0008103553554974496\n",
      "loss: 0.0006116710719652474\n",
      "loss: 4.4883305235998705e-05\n",
      "loss: 0.04482352361083031\n",
      "loss: 0.09980692714452744\n",
      "loss: 0.0038272570818662643\n",
      "loss: 0.015018381178379059\n",
      "loss: 0.47516438364982605\n",
      "loss: 0.05587754398584366\n",
      "loss: 0.07165351510047913\n",
      "loss: 0.0008034356287680566\n",
      "loss: 0.0006065419875085354\n",
      "loss: 4.434683796716854e-05\n",
      "loss: 0.044696178287267685\n",
      "loss: 0.09964953362941742\n",
      "loss: 0.003811820177361369\n",
      "loss: 0.014974696561694145\n",
      "loss: 0.47559094429016113\n",
      "loss: 0.05582062900066376\n",
      "loss: 0.0713590756058693\n",
      "loss: 0.0007976493798196316\n",
      "loss: 0.0006023074965924025\n",
      "loss: 4.392958726384677e-05\n",
      "loss: 0.04458916187286377\n",
      "loss: 0.09951698780059814\n",
      "loss: 0.0037987767718732357\n",
      "loss: 0.014937911182641983\n",
      "loss: 0.47595036029815674\n",
      "loss: 0.05577247962355614\n",
      "loss: 0.0711117535829544\n",
      "loss: 0.0007928175618872046\n",
      "loss: 0.0005987290642224252\n",
      "loss: 4.35719448432792e-05\n",
      "loss: 0.044499173760414124\n",
      "loss: 0.09940532594919205\n",
      "loss: 0.003787947352975607\n",
      "loss: 0.014907055534422398\n",
      "loss: 0.4762537479400635\n",
      "loss: 0.05573201924562454\n",
      "loss: 0.7091250419616699\n",
      "loss: 0.13772188127040863\n",
      "loss: 0.07670462876558304\n",
      "loss: 0.026213353499770164\n",
      "loss: 0.10507778823375702\n",
      "loss: 0.8565049171447754\n",
      "loss: 0.051164884120225906\n",
      "loss: 0.07883542031049728\n",
      "loss: 0.4575924277305603\n",
      "loss: 0.09286735951900482\n",
      "loss: 0.5803871154785156\n",
      "loss: 0.19240933656692505\n",
      "loss: 0.10355853289365768\n",
      "loss: 0.03947313129901886\n",
      "loss: 0.0993235632777214\n",
      "loss: 1.2407448291778564\n",
      "loss: 0.06553585827350616\n",
      "loss: 0.08890115469694138\n",
      "loss: 0.523960530757904\n",
      "loss: 0.10003174096345901\n",
      "loss: 0.20440073311328888\n",
      "loss: 0.005925354082137346\n",
      "loss: 0.004367816727608442\n",
      "loss: 0.0005902601405978203\n",
      "loss: 0.07317248731851578\n",
      "loss: 0.227251797914505\n",
      "loss: 0.012371216900646687\n",
      "loss: 0.03288230672478676\n",
      "loss: 0.42740094661712646\n",
      "loss: 0.0653177872300148\n",
      "loss: 0.1608908474445343\n",
      "loss: 0.002894294448196888\n",
      "loss: 0.002137022092938423\n",
      "loss: 0.00022986192198004574\n",
      "loss: 0.05934983864426613\n",
      "loss: 0.16701702773571014\n",
      "loss: 0.007779921405017376\n",
      "loss: 0.024042384698987007\n",
      "loss: 0.42026761174201965\n",
      "loss: 0.056946538388729095\n",
      "loss: 0.13319160044193268\n",
      "loss: 0.0019189638551324606\n",
      "loss: 0.0014277155278250575\n",
      "loss: 0.000135490539832972\n",
      "loss: 0.051957521587610245\n",
      "loss: 0.14800746738910675\n",
      "loss: 0.0062392279505729675\n",
      "loss: 0.020657489076256752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.42706623673439026\n",
      "loss: 0.05357148125767708\n",
      "loss: 0.11552601307630539\n",
      "loss: 0.0014625152107328176\n",
      "loss: 0.0010962532833218575\n",
      "loss: 9.572964336257428e-05\n",
      "loss: 0.047505054622888565\n",
      "loss: 0.13898469507694244\n",
      "loss: 0.0054780407808721066\n",
      "loss: 0.018873050808906555\n",
      "loss: 0.4355977475643158\n",
      "loss: 0.05179863050580025\n",
      "loss: 0.4144725203514099\n",
      "loss: 0.0810120701789856\n",
      "loss: 0.05194929987192154\n",
      "loss: 0.019163310527801514\n",
      "loss: 0.08998420834541321\n",
      "loss: 0.9519919157028198\n",
      "loss: 0.05240727961063385\n",
      "loss: 0.08051237463951111\n",
      "loss: 0.4963313937187195\n",
      "loss: 0.09544351696968079\n",
      "loss: 0.21322040259838104\n",
      "loss: 0.005859281402081251\n",
      "loss: 0.004265810828655958\n",
      "loss: 0.0005851311143487692\n",
      "loss: 0.07143544405698776\n",
      "loss: 0.22226303815841675\n",
      "loss: 0.012015355750918388\n",
      "loss: 0.03240196406841278\n",
      "loss: 0.42120909690856934\n",
      "loss: 0.06462664157152176\n",
      "loss: 0.16343723237514496\n",
      "loss: 0.00289955479092896\n",
      "loss: 0.0021315866615623236\n",
      "loss: 0.00023093505296856165\n",
      "loss: 0.05891800299286842\n",
      "loss: 0.1662321388721466\n",
      "loss: 0.00772832240909338\n",
      "loss: 0.023974372074007988\n",
      "loss: 0.41813167929649353\n",
      "loss: 0.05678211897611618\n",
      "loss: 0.13412843644618988\n",
      "loss: 0.0019252343336120248\n",
      "loss: 0.0014299836475402117\n",
      "loss: 0.00013620588288176805\n",
      "loss: 0.05184954032301903\n",
      "loss: 0.14787442982196808\n",
      "loss: 0.006232450250536203\n",
      "loss: 0.02065645530819893\n",
      "loss: 0.4261983335018158\n",
      "loss: 0.05353381112217903\n",
      "loss: 0.1159437820315361\n",
      "loss: 0.0014668130315840244\n",
      "loss: 0.0010986997513100505\n",
      "loss: 9.608730033505708e-05\n",
      "loss: 0.047489430755376816\n",
      "loss: 0.13899463415145874\n",
      "loss: 0.005480258259922266\n",
      "loss: 0.018883801996707916\n",
      "loss: 0.4351970851421356\n",
      "loss: 0.05179580673575401\n",
      "loss: 0.10411511361598969\n",
      "loss: 0.0012111245887354016\n",
      "loss: 0.0009130830294452608\n",
      "loss: 7.516428013332188e-05\n",
      "loss: 0.044615212827920914\n",
      "loss: 0.13373222947120667\n",
      "loss: 0.005028351675719023\n",
      "loss: 0.01777753233909607\n",
      "loss: 0.44291380047798157\n",
      "loss: 0.050706781446933746\n",
      "loss: 0.0961068868637085\n",
      "loss: 0.0010536498157307506\n",
      "loss: 0.0007981265662238002\n",
      "loss: 6.282526737777516e-05\n",
      "loss: 0.04263016954064369\n",
      "loss: 0.13023751974105835\n",
      "loss: 0.004729410167783499\n",
      "loss: 0.017026066780090332\n",
      "loss: 0.44906342029571533\n",
      "loss: 0.049958113580942154\n",
      "loss: 0.09050921350717545\n",
      "loss: 0.0009501320309937\n",
      "loss: 0.000722251832485199\n",
      "loss: 5.49569922441151e-05\n",
      "loss: 0.041213806718587875\n",
      "loss: 0.1277661919593811\n",
      "loss: 0.004520545247942209\n",
      "loss: 0.01649092510342598\n",
      "loss: 0.4538421630859375\n",
      "loss: 0.04941639304161072\n",
      "loss: 0.08649773895740509\n",
      "loss: 0.0008790182764641941\n",
      "loss: 0.0006699421210214496\n",
      "loss: 4.971151065547019e-05\n",
      "loss: 0.04017961025238037\n",
      "loss: 0.12595610320568085\n",
      "loss: 0.004369612783193588\n",
      "loss: 0.016099007800221443\n",
      "loss: 0.45752450823783875\n",
      "loss: 0.0490131750702858\n",
      "loss: 0.08356722444295883\n",
      "loss: 0.0008286093361675739\n",
      "loss: 0.0006327246082946658\n",
      "loss: 4.6075452701188624e-05\n",
      "loss: 0.03941187262535095\n",
      "loss: 0.12460288405418396\n",
      "loss: 0.004258388187736273\n",
      "loss: 0.01580660790205002\n",
      "loss: 0.4603571593761444\n",
      "loss: 0.04870811477303505\n",
      "loss: 0.7612159252166748\n",
      "loss: 0.12510572373867035\n",
      "loss: 0.0691569447517395\n",
      "loss: 0.019360974431037903\n",
      "loss: 0.12154842168092728\n",
      "loss: 0.6646018624305725\n",
      "loss: 0.04397335648536682\n",
      "loss: 0.07165677845478058\n",
      "loss: 0.42756909132003784\n",
      "loss: 0.08829072117805481\n",
      "loss: 0.2666620910167694\n",
      "loss: 0.007928186096251011\n",
      "loss: 0.005598931107670069\n",
      "loss: 0.000799975823611021\n",
      "loss: 0.08012645691633224\n",
      "loss: 0.21555966138839722\n",
      "loss: 0.012354380451142788\n",
      "loss: 0.03282693028450012\n",
      "loss: 0.39592158794403076\n",
      "loss: 0.0638439804315567\n",
      "loss: 0.18606536090373993\n",
      "loss: 0.003505585715174675\n",
      "loss: 0.0025410724338144064\n",
      "loss: 0.00028834922704845667\n",
      "loss: 0.06285041570663452\n",
      "loss: 0.1673625111579895\n",
      "loss: 0.0080961212515831\n",
      "loss: 0.024718981236219406\n",
      "loss: 0.4052305817604065\n",
      "loss: 0.057000044733285904\n",
      "loss: 0.14578616619110107\n",
      "loss: 0.0021900658030062914\n",
      "loss: 0.001614323235116899\n",
      "loss: 0.00015963398618623614\n",
      "loss: 0.05407525226473808\n",
      "loss: 0.14987657964229584\n",
      "loss: 0.00650208443403244\n",
      "loss: 0.021267928183078766\n",
      "loss: 0.4185936450958252\n",
      "loss: 0.053924135863780975\n",
      "loss: 0.12272714078426361\n",
      "loss: 0.0016070397105067968\n",
      "loss: 0.0011983538279309869\n",
      "loss: 0.00010783061588881537\n",
      "loss: 0.04890644922852516\n",
      "loss: 0.14079095423221588\n",
      "loss: 0.005671220365911722\n",
      "loss: 0.01934237964451313\n",
      "loss: 0.4302724003791809\n",
      "loss: 0.05216272175312042\n",
      "loss: 0.10840228199958801\n",
      "loss: 0.0012943176552653313\n",
      "loss: 0.000972982554230839\n",
      "loss: 8.184052421711385e-05\n",
      "loss: 0.04557871073484421\n",
      "loss: 0.1351601630449295\n",
      "loss: 0.005164285656064749\n",
      "loss: 0.018115056678652763\n",
      "loss: 0.43951350450515747\n",
      "loss: 0.05100856348872185\n",
      "loss: 0.09897617995738983\n",
      "loss: 0.001106755225919187\n",
      "loss: 0.000836722319945693\n",
      "loss: 6.693825707770884e-05\n",
      "loss: 0.043311744928359985\n",
      "loss: 0.13133279979228973\n",
      "loss: 0.0048273904249072075\n",
      "loss: 0.017274854704737663\n",
      "loss: 0.4466150104999542\n",
      "loss: 0.0501953661441803\n",
      "loss: 0.09250815212726593\n",
      "loss: 0.0009858101839199662\n",
      "loss: 0.0007483182125724852\n",
      "loss: 5.7639354054117575e-05\n",
      "loss: 0.04170846939086914\n",
      "loss: 0.128597691655159\n",
      "loss: 0.004591978620737791\n",
      "loss: 0.016675636172294617\n",
      "loss: 0.4520310163497925\n",
      "loss: 0.04959983751177788\n",
      "loss: 0.08793061971664429\n",
      "loss: 0.0009038359276019037\n",
      "loss: 0.0006881935405544937\n",
      "loss: 5.155934559297748e-05\n",
      "loss: 0.0405450165271759\n",
      "loss: 0.12658685445785522\n",
      "loss: 0.004422535188496113\n",
      "loss: 0.016237059608101845\n",
      "loss: 0.4561620354652405\n",
      "loss: 0.049154285341501236\n",
      "loss: 0.0846157819032669\n",
      "loss: 0.0008463864214718342\n",
      "loss: 0.0006458460120484233\n",
      "loss: 4.732720844913274e-05\n",
      "loss: 0.03968533128499985\n",
      "loss: 0.1250830888748169\n",
      "loss: 0.004297776613384485\n",
      "loss: 0.015910828486084938\n",
      "loss: 0.4593214690685272\n",
      "loss: 0.04881669580936432\n",
      "loss: 0.08217370510101318\n",
      "loss: 0.0008050462929531932\n",
      "loss: 0.0006153092253953218\n",
      "loss: 4.434683796716854e-05\n",
      "loss: 0.0390416719019413\n",
      "loss: 0.12394551187753677\n",
      "loss: 0.004204816184937954\n",
      "loss: 0.015665223821997643\n",
      "loss: 0.4617452621459961\n",
      "loss: 0.04855893552303314\n",
      "loss: 0.08035082370042801\n",
      "loss: 0.0007747432100586593\n",
      "loss: 0.0005928843165747821\n",
      "loss: 4.22009798057843e-05\n",
      "loss: 0.03855514153838158\n",
      "loss: 0.1230788305401802\n",
      "loss: 0.004134667571634054\n",
      "loss: 0.015478821471333504\n",
      "loss: 0.46361055970191956\n",
      "loss: 0.04836098477244377\n",
      "loss: 0.07897680252790451\n",
      "loss: 0.0007521954248659313\n",
      "loss: 0.0005761852371506393\n",
      "loss: 4.059158527525142e-05\n",
      "loss: 0.0381847620010376\n",
      "loss: 0.12241511791944504\n",
      "loss: 0.004081401042640209\n",
      "loss: 0.015336454845964909\n",
      "loss: 0.465049684047699\n",
      "loss: 0.04820854216814041\n",
      "loss: 0.07793323695659637\n",
      "loss: 0.000735255132894963\n",
      "loss: 0.0005636014975607395\n",
      "loss: 3.9399445086019114e-05\n",
      "loss: 0.03790131211280823\n",
      "loss: 0.12190414220094681\n",
      "loss: 0.004040525294840336\n",
      "loss: 0.015227272175252438\n",
      "loss: 0.4661622643470764\n",
      "loss: 0.04809083044528961\n",
      "loss: 0.07713591307401657\n",
      "loss: 0.0007223710999824107\n",
      "loss: 0.000554059399291873\n",
      "loss: 3.850534267257899e-05\n",
      "loss: 0.037683483213186264\n",
      "loss: 0.12151011824607849\n",
      "loss: 0.004009286407381296\n",
      "loss: 0.015143213793635368\n",
      "loss: 0.467024028301239\n",
      "loss: 0.047999586910009384\n",
      "loss: 1.1625086069107056\n",
      "loss: 0.18968304991722107\n",
      "loss: 0.09188120812177658\n",
      "loss: 0.028610317036509514\n",
      "loss: 0.1161576583981514\n",
      "loss: 0.6975275874137878\n",
      "loss: 0.045526620000600815\n",
      "loss: 0.07401306927204132\n",
      "loss: 0.41194775700569153\n",
      "loss: 0.08841320127248764\n",
      "loss: 0.2895393967628479\n",
      "loss: 0.008690278977155685\n",
      "loss: 0.006040301639586687\n",
      "loss: 0.0008990035858005285\n",
      "loss: 0.08069728314876556\n",
      "loss: 0.21794094145298004\n",
      "loss: 0.012640820816159248\n",
      "loss: 0.033458344638347626\n",
      "loss: 0.38946017622947693\n",
      "loss: 0.06402398645877838\n",
      "loss: 0.19406060874462128\n",
      "loss: 0.003703709226101637\n",
      "loss: 0.0026681222952902317\n",
      "loss: 0.0003093959530815482\n",
      "loss: 0.06350406259298325\n",
      "loss: 0.1686497926712036\n",
      "loss: 0.008240765891969204\n",
      "loss: 0.025055618956685066\n",
      "loss: 0.40181252360343933\n",
      "loss: 0.05718008801341057\n",
      "loss: 0.14951270818710327\n",
      "loss: 0.0022719064727425575\n",
      "loss: 0.0016697869868949056\n",
      "loss: 0.00016732423682697117\n",
      "loss: 0.0545685738325119\n",
      "loss: 0.15073908865451813\n",
      "loss: 0.006590278819203377\n",
      "loss: 0.021479710936546326\n",
      "loss: 0.41653987765312195\n",
      "loss: 0.0540742464363575\n",
      "loss: 0.12477701157331467\n",
      "loss: 0.0016489503905177116\n",
      "loss: 0.001227655215188861\n",
      "loss: 0.00011146689939778298\n",
      "loss: 0.049261223524808884\n",
      "loss: 0.14139819145202637\n",
      "loss: 0.005729187745600939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.01948489248752594\n",
      "loss: 0.4289267659187317\n",
      "loss: 0.05228003114461899\n",
      "loss: 0.10965625941753387\n",
      "loss: 0.001318608527071774\n",
      "loss: 0.00099028495606035\n",
      "loss: 8.380764484172687e-05\n",
      "loss: 0.045833706855773926\n",
      "loss: 0.13559740781784058\n",
      "loss: 0.0052040088921785355\n",
      "loss: 0.018214600160717964\n",
      "loss: 0.43857836723327637\n",
      "loss: 0.05109819769859314\n",
      "loss: 0.09979889541864395\n",
      "loss: 0.0011220310116186738\n",
      "loss: 0.0008477584924548864\n",
      "loss: 6.813043000875041e-05\n",
      "loss: 0.043496809899806976\n",
      "loss: 0.13165244460105896\n",
      "loss: 0.004855241160839796\n",
      "loss: 0.017346234992146492\n",
      "loss: 0.4459393322467804\n",
      "loss: 0.05026349425315857\n",
      "loss: 0.09307438880205154\n",
      "loss: 0.0009959529852494597\n",
      "loss: 0.0007557147764600813\n",
      "loss: 5.841425809194334e-05\n",
      "loss: 0.041844386607408524\n",
      "loss: 0.12883448600769043\n",
      "loss: 0.00461227772757411\n",
      "loss: 0.016727760434150696\n",
      "loss: 0.4515306055545807\n",
      "loss: 0.04965157434344292\n",
      "loss: 0.0883333683013916\n",
      "loss: 0.0009108756785281003\n",
      "loss: 0.000693323090672493\n",
      "loss: 5.2036208217032254e-05\n",
      "loss: 0.040645889937877655\n",
      "loss: 0.12676425278186798\n",
      "loss: 0.00443738279864192\n",
      "loss: 0.01627577282488346\n",
      "loss: 0.4557855725288391\n",
      "loss: 0.04919366538524628\n",
      "loss: 0.08490896224975586\n",
      "loss: 0.0008513974607922137\n",
      "loss: 0.000649543886538595\n",
      "loss: 4.768485450767912e-05\n",
      "loss: 0.03976093605160713\n",
      "loss: 0.12521690130233765\n",
      "loss: 0.004308850970119238\n",
      "loss: 0.015939775854349136\n",
      "loss: 0.4590349495410919\n",
      "loss: 0.04884686321020126\n",
      "loss: 0.08239070326089859\n",
      "loss: 0.0008086850866675377\n",
      "loss: 0.00061799306422472\n",
      "loss: 4.464487574296072e-05\n",
      "loss: 0.039098817855119705\n",
      "loss: 0.12404732406139374\n",
      "loss: 0.004213016480207443\n",
      "loss: 0.015687141567468643\n",
      "loss: 0.46152591705322266\n",
      "loss: 0.048581961542367935\n",
      "loss: 0.08051347732543945\n",
      "loss: 0.0007774274563416839\n",
      "loss: 0.0005948524340055883\n",
      "loss: 4.2379801016068086e-05\n",
      "loss: 0.03859856724739075\n",
      "loss: 0.12315662950277328\n",
      "loss: 0.004140832461416721\n",
      "loss: 0.015495529398322105\n",
      "loss: 0.46344199776649475\n",
      "loss: 0.04837881401181221\n",
      "loss: 0.07909969240427017\n",
      "loss: 0.000754163833335042\n",
      "loss: 0.0005776762263849378\n",
      "loss: 4.0770406485535204e-05\n",
      "loss: 0.03821795433759689\n",
      "loss: 0.12247446924448013\n",
      "loss: 0.004086128901690245\n",
      "loss: 0.015349225141108036\n",
      "loss: 0.4649197459220886\n",
      "loss: 0.04822230339050293\n",
      "loss: 0.07802674174308777\n",
      "loss: 0.0007367462967522442\n",
      "loss: 0.000564734626095742\n",
      "loss: 3.951866165152751e-05\n",
      "loss: 0.037926819175481796\n",
      "loss: 0.1219501942396164\n",
      "loss: 0.004044235683977604\n",
      "loss: 0.015237136743962765\n",
      "loss: 0.46606194972991943\n",
      "loss: 0.04810140281915665\n",
      "loss: 0.07720750570297241\n",
      "loss: 0.0007235640659928322\n",
      "loss: 0.0005548943299800158\n",
      "loss: 3.862455560010858e-05\n",
      "loss: 0.03770304471254349\n",
      "loss: 0.12154559046030045\n",
      "loss: 0.00401215860620141\n",
      "loss: 0.01515071839094162\n",
      "loss: 0.46694615483283997\n",
      "loss: 0.0480077788233757\n",
      "loss: 0.07657920569181442\n",
      "loss: 0.0007134836632758379\n",
      "loss: 0.000547439674846828\n",
      "loss: 3.790927439695224e-05\n",
      "loss: 0.03753061965107918\n",
      "loss: 0.12123299390077591\n",
      "loss: 0.003987323492765427\n",
      "loss: 0.015084153972566128\n",
      "loss: 0.46763208508491516\n",
      "loss: 0.04793542996048927\n",
      "loss: 0.07609548419713974\n",
      "loss: 0.0007057891925796866\n",
      "loss: 0.0005417144857347012\n",
      "loss: 3.7372810766100883e-05\n",
      "loss: 0.03739745542407036\n",
      "loss: 0.12099092453718185\n",
      "loss: 0.003968353848904371\n",
      "loss: 0.015032600611448288\n",
      "loss: 0.46816426515579224\n",
      "loss: 0.04787902906537056\n",
      "loss: 0.07572219520807266\n",
      "loss: 0.0006998245371505618\n",
      "loss: 0.0005373013555072248\n",
      "loss: 3.695556370075792e-05\n",
      "loss: 0.03729431331157684\n",
      "loss: 0.12080319225788116\n",
      "loss: 0.003953513223677874\n",
      "loss: 0.01499266643077135\n",
      "loss: 0.46857795119285583\n",
      "loss: 0.047835446894168854\n",
      "loss: 0.07543349266052246\n",
      "loss: 0.0006952317780815065\n",
      "loss: 0.0005339020863175392\n",
      "loss: 3.6657529562944546e-05\n",
      "loss: 0.037214379757642746\n",
      "loss: 0.12065738439559937\n",
      "loss: 0.003942083567380905\n",
      "loss: 0.01496162824332714\n",
      "loss: 0.46889930963516235\n",
      "loss: 0.047801434993743896\n",
      "loss: 0.07520978152751923\n",
      "loss: 0.0006917126593180001\n",
      "loss: 0.0005312780849635601\n",
      "loss: 3.641910006990656e-05\n",
      "loss: 0.03715239465236664\n",
      "loss: 0.12054421752691269\n",
      "loss: 0.003933167550712824\n",
      "loss: 0.014937669038772583\n",
      "loss: 0.4691494107246399\n",
      "loss: 0.04777504876255989\n",
      "loss: 0.47235605120658875\n",
      "loss: 0.11743868887424469\n",
      "loss: 0.06984546780586243\n",
      "loss: 0.018058672547340393\n",
      "loss: 0.12914146482944489\n",
      "loss: 0.7964571714401245\n",
      "loss: 0.05034102872014046\n",
      "loss: 0.07552225887775421\n",
      "loss: 0.4758613407611847\n",
      "loss: 0.09245290607213974\n",
      "loss: 0.22650745511054993\n",
      "loss: 0.006804857403039932\n",
      "loss: 0.00495952321216464\n",
      "loss: 0.000665349536575377\n",
      "loss: 0.07891791313886642\n",
      "loss: 0.21754270792007446\n",
      "loss: 0.012216741219162941\n",
      "loss: 0.032382261008024216\n",
      "loss: 0.41218751668930054\n",
      "loss: 0.06426027417182922\n",
      "loss: 0.17127595841884613\n",
      "loss: 0.0031721785198897123\n",
      "loss: 0.0023282431066036224\n",
      "loss: 0.00025436535361222923\n",
      "loss: 0.06162338703870773\n",
      "loss: 0.16625270247459412\n",
      "loss: 0.007899829186499119\n",
      "loss: 0.02424797974526882\n",
      "loss: 0.4130156934261322\n",
      "loss: 0.056865714490413666\n",
      "loss: 0.13876678049564362\n",
      "loss: 0.0020445610862225294\n",
      "loss: 0.0015158811584115028\n",
      "loss: 0.00014622088929172605\n",
      "loss: 0.05315300449728966\n",
      "loss: 0.148610457777977\n",
      "loss: 0.006355411373078823\n",
      "loss: 0.020912418141961098\n",
      "loss: 0.42297595739364624\n",
      "loss: 0.05370354652404785\n",
      "loss: 0.11883467435836792\n",
      "loss: 0.0015303274849429727\n",
      "loss: 0.0011446467833593488\n",
      "loss: 0.00010127342102350667\n",
      "loss: 0.04823818802833557\n",
      "loss: 0.13974854350090027\n",
      "loss: 0.00556728383526206\n",
      "loss: 0.019085116684436798\n",
      "loss: 0.43302467465400696\n",
      "loss: 0.05195990949869156\n",
      "loss: 0.10600978136062622\n",
      "loss: 0.0012490202207118273\n",
      "loss: 0.0009405862656421959\n",
      "loss: 7.814474520273507e-05\n",
      "loss: 0.04509466513991356\n",
      "loss: 0.1343608945608139\n",
      "loss: 0.005090475548058748\n",
      "loss: 0.017929168418049812\n",
      "loss: 0.4413788318634033\n",
      "loss: 0.0508437380194664\n",
      "loss: 0.09740051627159119\n",
      "loss: 0.0010779347503557801\n",
      "loss: 0.0008158434648066759\n",
      "loss: 6.46731277811341e-05\n",
      "loss: 0.042958203703165054\n",
      "loss: 0.1307310312986374\n",
      "loss: 0.004774386528879404\n",
      "loss: 0.017139388248324394\n",
      "loss: 0.4479440152645111\n",
      "loss: 0.05006676912307739\n",
      "loss: 0.0914207324385643\n",
      "loss: 0.0009664793615229428\n",
      "loss: 0.0007342410972341895\n",
      "loss: 5.620875890599564e-05\n",
      "loss: 0.04144768789410591\n",
      "loss: 0.12814560532569885\n",
      "loss: 0.004553417209535837\n",
      "loss: 0.016575580462813377\n",
      "loss: 0.4530082046985626\n",
      "loss: 0.04950093850493431\n",
      "loss: 0.08715552091598511\n",
      "loss: 0.000890472496394068\n",
      "loss: 0.0006783520802855492\n",
      "loss: 5.054601570009254e-05\n",
      "loss: 0.04035075381398201\n",
      "loss: 0.12624603509902954\n",
      "loss: 0.004394038114696741\n",
      "loss: 0.016162671148777008\n",
      "loss: 0.4568948745727539\n",
      "loss: 0.04907846823334694\n",
      "loss: 0.08405061811208725\n",
      "loss: 0.0008367819827981293\n",
      "loss: 0.0006388081819750369\n",
      "loss: 4.667152461479418e-05\n",
      "loss: 0.03953935205936432\n",
      "loss: 0.12482462078332901\n",
      "loss: 0.004276645369827747\n",
      "loss: 0.01585487090051174\n",
      "loss: 0.4598776698112488\n",
      "loss: 0.048758432269096375\n",
      "loss: 0.081754669547081\n",
      "loss: 0.0007980669615790248\n",
      "loss: 0.0006101800827309489\n",
      "loss: 4.386998261907138e-05\n",
      "loss: 0.038931235671043396\n",
      "loss: 0.1237487867474556\n",
      "loss: 0.004188834689557552\n",
      "loss: 0.01562272198498249\n",
      "loss: 0.4621707797050476\n",
      "loss: 0.048514075577259064\n",
      "loss: 0.08003643900156021\n",
      "loss: 0.0007695535896345973\n",
      "loss: 0.0005890673492103815\n",
      "loss: 4.184333738521673e-05\n",
      "loss: 0.03847102075815201\n",
      "loss: 0.1229281798005104\n",
      "loss: 0.004122457932680845\n",
      "loss: 0.015446435660123825\n",
      "loss: 0.46393781900405884\n",
      "loss: 0.04832645133137703\n",
      "loss: 0.07873868942260742\n",
      "loss: 0.0007483182125724852\n",
      "loss: 0.000573322584386915\n",
      "loss: 4.035315942019224e-05\n",
      "loss: 0.03812042623758316\n",
      "loss: 0.12229905277490616\n",
      "loss: 0.004072124604135752\n",
      "loss: 0.015311699360609055\n",
      "loss: 0.46530210971832275\n",
      "loss: 0.04818183183670044\n",
      "loss: 0.07775167375802994\n",
      "loss: 0.0007322726887650788\n",
      "loss: 0.0005614545079879463\n",
      "loss: 3.922062387573533e-05\n",
      "loss: 0.037851911038160324\n",
      "loss: 0.1218147948384285\n",
      "loss: 0.004033523611724377\n",
      "loss: 0.015208148397505283\n",
      "loss: 0.46635761857032776\n",
      "loss: 0.04807019233703613\n",
      "loss: 0.07699691504240036\n",
      "loss: 0.0007201641565188766\n",
      "loss: 0.0005523895961232483\n",
      "loss: 3.83861297450494e-05\n",
      "loss: 0.037645358592271805\n",
      "loss: 0.1214410662651062\n",
      "loss: 0.004003840498626232\n",
      "loss: 0.015128388069570065\n",
      "loss: 0.46717551350593567\n",
      "loss: 0.04798370599746704\n",
      "loss: 0.07641718536615372\n",
      "loss: 0.0007108591962605715\n",
      "loss: 0.000545531278476119\n",
      "loss: 3.7730453186668456e-05\n",
      "loss: 0.03748606517910957\n",
      "loss: 0.1211521178483963\n",
      "loss: 0.003981040325015783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.015066848136484623\n",
      "loss: 0.4678098261356354\n",
      "loss: 0.04791654646396637\n",
      "loss: 0.0759705901145935\n",
      "loss: 0.0007037611794658005\n",
      "loss: 0.0005402235547080636\n",
      "loss: 3.7253597838571295e-05\n",
      "loss: 0.037362925708293915\n",
      "loss: 0.120928093791008\n",
      "loss: 0.003963327035307884\n",
      "loss: 0.015019168145954609\n",
      "loss: 0.4683023691177368\n",
      "loss: 0.0478643961250782\n",
      "loss: 0.07562562823295593\n",
      "loss: 0.0006982737104408443\n",
      "loss: 0.0005361682851798832\n",
      "loss: 3.6836347135249525e-05\n",
      "loss: 0.03726758435368538\n",
      "loss: 0.12075429409742355\n",
      "loss: 0.0039496831595897675\n",
      "loss: 0.014982320368289948\n",
      "loss: 0.4686847925186157\n",
      "loss: 0.0478239431977272\n",
      "loss: 0.07535868138074875\n",
      "loss: 0.0006940984749235213\n",
      "loss: 0.000533007550984621\n",
      "loss: 3.659792128019035e-05\n",
      "loss: 0.0371936559677124\n",
      "loss: 0.12061946094036102\n",
      "loss: 0.003939151298254728\n",
      "loss: 0.014953641220927238\n",
      "loss: 0.46898266673088074\n",
      "loss: 0.04779255390167236\n",
      "loss: 0.07515181601047516\n",
      "loss: 0.0006908179493620992\n",
      "loss: 0.0005305624799802899\n",
      "loss: 3.635949542513117e-05\n",
      "loss: 0.037136249244213104\n",
      "loss: 0.12051476538181305\n",
      "loss: 0.003930953331291676\n",
      "loss: 0.01493137702345848\n",
      "loss: 0.46921437978744507\n",
      "loss: 0.04776817187666893\n",
      "loss: 0.07499125599861145\n",
      "loss: 0.00068825320340693\n",
      "loss: 0.0005287137464620173\n",
      "loss: 3.6180674214847386e-05\n",
      "loss: 0.037091709673404694\n",
      "loss: 0.12043306976556778\n",
      "loss: 0.003924550488591194\n",
      "loss: 0.0149140739813447\n",
      "loss: 0.469394326210022\n",
      "loss: 0.04774903878569603\n",
      "loss: 0.07486649602651596\n",
      "loss: 0.0006863445742055774\n",
      "loss: 0.0005272228154353797\n",
      "loss: 3.60614612873178e-05\n",
      "loss: 0.03705700859427452\n",
      "loss: 0.12036973983049393\n",
      "loss: 0.003919524140655994\n",
      "loss: 0.014900581911206245\n",
      "loss: 0.4695345163345337\n",
      "loss: 0.04773428663611412\n",
      "loss: 0.07476963102817535\n",
      "loss: 0.0006847937474958599\n",
      "loss: 0.0005260897451080382\n",
      "loss: 3.594224835978821e-05\n",
      "loss: 0.037030041217803955\n",
      "loss: 0.12032052874565125\n",
      "loss: 0.003915754146873951\n",
      "loss: 0.014890236780047417\n",
      "loss: 0.469643771648407\n",
      "loss: 0.04772278293967247\n",
      "loss: 0.07469435036182404\n",
      "loss: 0.0006836008396930993\n",
      "loss: 0.00052519520977512\n",
      "loss: 3.588264007703401e-05\n",
      "loss: 0.03700907155871391\n",
      "loss: 0.1202821433544159\n",
      "loss: 0.003912642598152161\n",
      "loss: 0.014882008545100689\n",
      "loss: 0.4697284400463104\n",
      "loss: 0.04771384224295616\n",
      "loss: 0.07463584095239639\n",
      "loss: 0.0006827061879448593\n",
      "loss: 0.0005245392094366252\n",
      "loss: 3.5823031794279814e-05\n",
      "loss: 0.036992743611335754\n",
      "loss: 0.12025236338376999\n",
      "loss: 0.003910368774086237\n",
      "loss: 0.014875777997076511\n",
      "loss: 0.4697948098182678\n",
      "loss: 0.04770690202713013\n",
      "{'learning_rate': 0.05, 'epoches': 25}\n",
      "tensor(0.1607, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.15]\n",
    "epochs = [1, 5, 10, 15, 20, 25]\n",
    "best = {'learning_rate' : learning_rates[0], 'epoches' : epochs[0]}\n",
    "b_loss = 100000\n",
    "for le in learning_rates:\n",
    "    for ep in epochs:\n",
    "        model = NeuralNetwork2()\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=le)\n",
    "        for t in range(ep):\n",
    "            train_loop(data, targets, model, loss_fn, optimizer, log=False)\n",
    "        cur_loss = loss_fn(model(data), targets)\n",
    "        if cur_loss <= b_loss:\n",
    "            b_loss = cur_loss\n",
    "            best['learning_rate'] = le\n",
    "            best['epoches'] = ep\n",
    "\n",
    "print(best)\n",
    "print(b_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4048], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = torch.tensor([3.0,2.0])\n",
    "dt = dt.to(device)\n",
    "model(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание (факультативное).\n",
    "Прочитать параграф 2.6 в учебнике С. Николенко и др. (стр. 81–92) и воспроизвести рассмотренные там нейронные сети на TensorFlow и Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
